tags:
  - GCP
  - IA
status: 
Parent: "Area-Prog"
categories: "[Data Science](/data%20science/data-science/)"
public_note: "true"
# APACHE SPARK
``

- hadoop
- [apache](/backend/apache/)
- [cloud](/cloud/cloud/)
- [Data Science](/data%20science/data-science/)
- IA
- Big Data
- [Stream Processing](/backend/stream-processing/)
- Apache Spark vs. Hadoop Diferencias Clave y Cu谩ndo Usar Cada Uno

## Fundamentos de Apache Spark
Apache Spark es un motor de procesamiento distribuido de c贸digo abierto dise帽ado para el procesamiento r谩pido de grandes vol煤menes de datos, tanto batch como en tiempo real. Su arquitectura permite escalabilidad horizontal y procesamiento en memoria, lo que lo hace m谩s r谩pido que los sistemas tradicionales basados en disco como Hadoop MapReduce.

### Caracter铆sticas Principales
- Procesamiento en memoria: reduce el tiempo de lectura/escritura en disco.
- Soporte para m煤ltiples lenguajes: Scala, Java, Python y R.
- Compatibilidad con hadoop y otros sistemas de almacenamiento como S3, HDFS y bases de datos NoSQL.
- Capacidad para procesamiento batch y stream en un mismo framework.
- Extensiones para an谩lisis avanzado: [Data Science](/data%20science/data-science/) y IA.
- Optimizaci贸n autom谩tica de consultas mediante Catalyst Optimizer y Tungsten Engine.

### Componentes Clave
- **Spark Core:** motor central que maneja la planificaci贸n, distribuci贸n y ejecuci贸n de tareas.
- **Spark SQL:** m贸dulo para trabajar con datos estructurados y consultas SQL.
- **Spark Streaming:** permite el procesamiento de datos en tiempo real.
- **MLlib:** librer铆a de aprendizaje autom谩tico.
- **GraphX:** para procesamiento y an谩lisis de grafos.
- **SparkR y PySpark:** interfaces para R y Python respectivamente.

## Arquitectura de Apache Spark
Apache Spark sigue una arquitectura maestro-esclavo basada en cluster:

- **Driver:** componente principal que coordina la ejecuci贸n de aplicaciones, mantiene el DAG (Directed Acyclic Graph) de tareas y comunica con los ejecutores.
- **Cluster Manager:** gestiona los recursos del cluster. Puede ser:
	- Spark Standalone
	- YARN
	- Mesos
	- Kubernetes
- **Executors:** nodos que ejecutan tareas en paralelo y almacenan datos en memoria o disco.
- **Task:** unidad de trabajo m铆nima que se ejecuta en un executor.
- **RDD (Resilient Distributed Dataset):** estructura de datos distribuida y tolerante a fallos.
- **DataFrame y Dataset:** abstracciones de datos estructurados optimizadas.

## Procesamiento y Optimizaci贸n
- **Transformaciones:** operaciones que crean nuevos RDD sin ejecutar tareas inmediatamente (lazy evaluation).
- **Acciones:** operaciones que devuelven un resultado o escriben datos, desencadenando la ejecuci贸n.
- **Particionamiento:** clave para la eficiencia; permite distribuir datos y tareas de manera 贸ptima.
- **Caching y Persistencia:** almacenamiento temporal de RDD/DataFrame en memoria para acelerar consultas repetidas.

## Integraci贸n y Ecosistema
- Compatible con hadoop para almacenamiento y procesamiento.
- Se integra con [cloud](/cloud/cloud/) para despliegues escalables (AWS EMR, Azure Databricks, GCP Dataproc).
- Se conecta con bases de datos y sistemas de mensajer铆a: Kafka, Cassandra, HBase, MongoDB.

## Casos de Uso
- Procesamiento de grandes vol煤menes de datos batch.
- An谩lisis en tiempo real de logs y m茅tricas de aplicaciones.
- Machine Learning y predicciones sobre grandes datasets.
- Procesamiento de grafos para redes sociales, recomendaciones y detecci贸n de fraudes.
- ETL distribuido y optimizado para Big Data.

## Ejemplo de C贸digo: PySpark
### Creaci贸n de SparkSession
{% raw %}
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
	.appName("EjemploSpark") \
	.getOrCreate()
```
{% endraw %}`

### Creaci贸n y Transformaci贸n de DataFrame

{% raw %}
```python
data = [("Eduardo", 30), ("Ana", 25), ("Luis", 35)]
columns = ["Nombre", "Edad"]

df = spark.createDataFrame(data, columns)
df.show()

df_filtered = df.filter(df.Edad > 28)
df_filtered.show()
```
{% endraw %}

### Lectura y Escritura de Datos

{% raw %}
```python
# Leer desde CSV
df_csv = spark.read.csv("datos.csv", header=True, inferSchema=True)

# Escribir en parquet
df_csv.write.parquet("salida.parquet")
```
{% endraw %}

## Optimizaci贸n y Buenas Pr谩cticas

- Usar **DataFrames/Datasets** en lugar de RDD cuando sea posible.
- Aplicar **persistencia** solo cuando sea necesario.
- Ajustar el **n煤mero de particiones** seg煤n tama帽o de datos y recursos.
- Monitorear y depurar con Spark UI para identificar cuellos de botella.
- Evitar **shuffles innecesarios** para reducir tiempo de ejecuci贸n.

## Recursos y Referencias

- [Documentaci贸n oficial de Apache Spark](https://spark.apache.org/docs/latest/)
- Apache Spark vs. Hadoop Diferencias Clave y Cu谩ndo Usar Cada Uno

# APACHE SPARK AVANZADO
## Optimizaci贸n Avanzada
### Catalyst Optimizer
- Motor de optimizaci贸n de consultas de Spark SQL.
- Aplica reglas de reescritura l贸gica para mejorar el plan de ejecuci贸n.
- Optimiza joins, filtros y proyecciones de manera autom谩tica.

### Tungsten Engine
- Optimizaci贸n a nivel de memoria y CPU.
- Usa c贸digo generado din谩micamente para operaciones en columnas.
- Reduce el overhead de Java Virtual Machine (JVM) en procesos masivos.

### Broadcast Variables y Accumulators
- **Broadcast Variables:** permiten enviar una copia de datos a todos los nodos sin replicarla en cada tarea.
- **Accumulators:** variables compartidas para sumar valores de manera segura en tareas distribuidas.

### Shuffle Optimization
- Evitar wide transformations innecesarias (reduceByKey vs groupByKey).
- Ajustar n煤mero de particiones para minimizar movimientos de datos.
- Usar **salting** para manejar keys muy grandes o skewed.

## Tolerancia a Fallos
- Spark re-ejecuta tareas fallidas bas谩ndose en el DAG.
- **Checkpointing:** guardado de estados intermedios en RDD o Structured Streaming para recuperaci贸n.
- Persistencia selectiva para asegurar consistencia y eficiencia.

## Structured Streaming
- **Micro-batch Processing:** procesamiento en peque帽os lotes de datos.
- **Continuous Processing:** baja latencia para flujos de datos casi en tiempo real.
- **Watermarking:** manejo de eventos tard铆os y ventanas de tiempo.
- **Output Modes:**
	- Append: solo nuevos registros.
	- Update: registros modificados.
	- Complete: resultados completos.

## Integraciones Avanzadas
- **Kafka:** ingesti贸n de streams en tiempo real.
- **Delta Lake:** almacenamiento con transacciones ACID, manejo de versiones y upserts.
- **MLlib:** pipelines de machine learning distribuido.
- **Bases de datos y almacenamiento:** Cassandra, HBase, S3, JDBC.

## Seguridad y Despliegue
- Autenticaci贸n: Kerberos y LDAP.
- Autorizaci贸n: ACLs y roles.
- Encriptaci贸n de datos en tr谩nsito y reposo.
- Despliegue en cloud: AWS EMR, Azure Databricks, GCP Dataproc.

## Buenas Pr谩cticas para Producci贸n
- Ajustar recursos: n煤mero de ejecutores, cores por executor, memoria.
- Evitar anti-patrones: shuffles innecesarios, caching excesivo.
- Monitoreo: Spark UI, Ganglia, Prometheus.
- Logging estructurado y alertas tempranas.

## Casos de Uso Avanzados
- Recomendadores en tiempo real.
- An谩lisis de grafos complejos con GraphX.
- Entrenamiento distribuido de modelos ML y deep learning.
- ETL masivo y optimizado para Big Data.
- Procesamiento de logs y m茅tricas en streaming.

## Ejemplo de C贸digo: Broadcast y Accumulator en PySpark
### Broadcast
{% raw %}
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("BroadcastExample").getOrCreate()
data = [("Ana", 25), ("Luis", 35), ("Eduardo", 30)]
df = spark.createDataFrame(data, ["Nombre", "Edad"])

factor = 2
broadcast_factor = spark.sparkContext.broadcast(factor)

df_rdd = df.rdd.map(lambda row: (row.Nombre, row.Edad * broadcast_factor.value))
df_rdd.collect()
```
{% endraw %}`

### Accumulator

{% raw %}
```python
accum = spark.sparkContext.accumulator(0)

def increment(x):
	global accum
	if x > 30:
		accum += 1
	df_rdd.collect()

print("Cantidad de mayores de 30:", accum.value)
```
{% endraw %}

## Ejemplo de C贸digo: Structured Streaming con Kafka

{% raw %}
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import expr

spark = SparkSession.builder.appName("StreamingKafkaExample").getOrCreate()

df_stream = spark.readStream \
	.format("kafka") \
	.option("kafka.bootstrap.servers", "localhost:9092") \
	.option("subscribe", "topic_test") \
	.load()

df_parsed = df_stream.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")

query = df_parsed.writeStream \
	.outputMode("append") \
	.format("console") \
	.start()

query.awaitTermination()
```
{% endraw %}

## Recursos y Referencias Avanzadas

- [Documentaci贸n oficial de Apache Spark](https://spark.apache.org/docs/latest/)
- Apache Spark vs. Hadoop Diferencias Clave y Cu谩ndo Usar Cada Uno
- [Delta Lake](https://delta.io/)
- MLlib
- Kafka
# APACHE SPARK Hadoop
Objetivo: Al finalizar el m贸dulo el alumno ser谩 capaz de importar datos a Apache Hadoop Cluster y procesarlos con Spark, Hive, Flume, Sqoop, Impala, y otras herramientas del ecosistema de Hadoop.

- Parte 1
	-   Introduction to Hadoop and the Hadoop Ecosystem
	-   Hadoop Architecture and HDFS
	-   Importing Relational Data with Apache Sqoop
- Parte 2
	-   Introduction to Impala and Hive
	-   Modeling and Managing Data with Impala and Hive
	-   Data Formats
	-   Data Partitioning
- Parte 3
	-   Capturing Data with Apache Flume
	-   Spark Basics
	-   Working with RDDs in Spark
	-   Writing and Deploying Spark Applications
- Parte 4
	-   Parallel Programming with Spark
	-   Spark Caching and Persistence
	-   Common Patterns in Spark Data Processing
	-   Preview: Spark SQL
	-   Conclusion

## Parte 1: Introduction to Hadoop and the Hadoop Ecosystem
Apache Hadoop es un framework de c贸digo abierto dise帽ado para el almacenamiento y procesamiento distribuido de grandes vol煤menes de datos (Big Data) en clusters de servidores. Su dise帽o permite tolerancia a fallos, escalabilidad horizontal y procesamiento paralelo masivo.

### Componentes del Ecosistema Hadoop
- **Hadoop Distributed File System (HDFS):** sistema de archivos distribuido que almacena datos en bloques replicados en varios nodos del cluster para garantizar tolerancia a fallos.
- **MapReduce:** modelo de programaci贸n para procesamiento distribuido de datos en paralelo.
- **YARN (Yet Another Resource Negotiator):** gestor de recursos del cluster que coordina la ejecuci贸n de aplicaciones.
- **Hive:** motor de SQL sobre Hadoop para consultas de datos estructurados.
- **Pig:** lenguaje de alto nivel para procesamiento de datos grandes, basado en scripts.
- **HBase:** base de datos NoSQL distribuida para acceso en tiempo real a datos masivos.
- **Sqoop:** herramienta para importar/exportar datos entre Hadoop y bases de datos relacionales.
- **Flume:** sistema de ingesti贸n de datos en streaming hacia Hadoop.
- **Impala:** motor SQL en tiempo real para Hadoop, optimizado para consultas r谩pidas.

### Beneficios del Ecosistema
- Escalabilidad: f谩cil de ampliar agregando nodos al cluster.
- Resiliencia: replicaci贸n de datos y recuperaci贸n ante fallos de nodos.
- Flexibilidad: soporte para diferentes tipos de datos (estructurados, semiestructurados y no estructurados).
- Integraci贸n: interoperabilidad entre componentes para ETL, an谩lisis y procesamiento en tiempo real.

## Parte 1: Hadoop Architecture and HDFS
Hadoop se basa en una arquitectura maestro-esclavo, dise帽ada para procesar datos en paralelo y distribuir la carga de manera eficiente.

### Componentes Clave
- **NameNode:** nodo maestro que mantiene el metadata del sistema de archivos y coordina el acceso a los datos.
- **DataNode:** nodos esclavos que almacenan bloques de datos y ejecutan operaciones de lectura/escritura.
- **Secondary NameNode:** nodo auxiliar que realiza checkpoints del NameNode para evitar p茅rdida de metadata.
- **HDFS:** sistema de archivos distribuido que divide los archivos en bloques (por defecto 128 MB) y los replica en varios DataNodes (replicaci贸n por defecto 3).

### Funcionalidades Principales de HDFS
- **Tolerancia a fallos:** mediante replicaci贸n de bloques en m煤ltiples nodos.
- **Acceso paralelo:** permite que m煤ltiples tareas accedan a diferentes bloques al mismo tiempo.
- **Almacenamiento escalable:** adecuado para datos masivos de petabytes.
- **Optimizaci贸n para lectura secuencial:** ideal para grandes vol煤menes de datos en procesamiento batch.

## Parte 1: Importing Relational Data with Apache Sqoop
Apache Sqoop es una herramienta dise帽ada para transferir datos entre sistemas de almacenamiento relacionales (RDBMS) y Hadoop.

### Funcionalidades Principales
- Importaci贸n de datos desde bases de datos relacionales a HDFS, Hive o HBase.
- Exportaci贸n de datos desde Hadoop hacia bases de datos relacionales.
- Soporte para m煤ltiples RDBMS: MySQL, PostgreSQL, Oracle, SQL Server, entre otros.
- Integraci贸n con Hadoop para generar autom谩ticamente ficheros CSV o Avro a partir de tablas relacionales.

### Ejemplo de Uso B谩sico
{% raw %}
```bash
# Importar una tabla de MySQL a HDFS
sqoop import \
--connect jdbc:mysql://localhost:3306/empresa \
--username usuario \
--password clave \
--table empleados \
--target-dir /hadoop/empleados \
--m 1
```
{% endraw %}`

- `--m 1` indica n煤mero de mappers (tareas paralelas) para la importaci贸n.
- Los datos importados pueden ser procesados posteriormente con Spark, Hive o Impala.

### Beneficios de Usar Sqoop

- Automatiza la transferencia de grandes vol煤menes de datos.
- Mantiene la integridad de los datos durante la importaci贸n/exportaci贸n.
- Reduce el tiempo de desarrollo en procesos ETL hacia Hadoop.
## Parte 2: Introduction to Impala and Hive
### Apache Hive
Hive es un sistema de data warehouse sobre Hadoop que permite consultar y analizar datos estructurados usando un lenguaje similar a SQL llamado HiveQL. Est谩 optimizado para procesamiento batch de grandes vol煤menes de datos almacenados en HDFS.

- Permite transformar consultas SQL en trabajos MapReduce, Tez o Spark.
- Integra con HDFS y soporta m煤ltiples formatos de archivo.
- Facilita la creaci贸n de tablas, vistas y particiones para organizar datos.

### Apache Impala
Impala es un motor de consultas SQL de baja latencia para Hadoop, dise帽ado para consultas interactivas en tiempo real.

- Proporciona acceso r谩pido a datos almacenados en HDFS o en Hive Metastore.
- Evita la sobrecarga de MapReduce, permitiendo ejecuci贸n de queries en segundos.
- Compatible con HiveQL, lo que permite interoperabilidad entre Hive e Impala.

### Diferencias Clave
- Hive: optimizado para procesamiento batch; ideal para an谩lisis complejos y ETL.
- Impala: optimizado para consultas interactivas; ideal para dashboards y reporting en tiempo real.

## Parte 2: Modeling and Managing Data with Impala and Hive
### Tablas y Esquemas
- **Tablas Internas (Managed Tables):** Hive/Impala gestiona el ciclo de vida de los datos; eliminarlas borra los archivos subyacentes.
- **Tablas Externas:** los datos existen fuera del metastore; eliminar la tabla no elimina los datos.
- **Vistas:** consultas guardadas que simplifican consultas complejas o recurrentes.

### Gesti贸n de Datos
- Creaci贸n y modificaci贸n de tablas con HiveQL.
- Uso de comandos DDL (CREATE, ALTER, DROP) para definir estructuras.
- Integraci贸n con Spark para procesamiento distribuido de tablas.

### Optimizaci贸n
- Definir tipos de datos correctos para reducir espacio y mejorar performance.
- Utilizar particiones y buckets para segmentar datos y acelerar consultas.

## Parte 2: Data Formats
Hadoop y sus motores de procesamiento soportan m煤ltiples formatos de datos, cada uno con ventajas espec铆ficas:

- **Text/CSV:** simple y legible, pero poco eficiente en almacenamiento y lectura.
- **Avro:** soporta esquemas evolutivos, eficiente para serializaci贸n de datos.
- **Parquet:** columna orientada, ideal para analytics y consultas con filtros, alta compresi贸n.
- **ORC:** optimizado para Hive, almacenamiento columna orientado, alta eficiencia en lectura y compresi贸n.

### Consideraciones
- Elegir formato columna orientado (Parquet, ORC) para queries anal铆ticas.
- Usar Avro para interoperabilidad entre sistemas y evoluci贸n de esquemas.
- CSV/Text 煤til para interoperabilidad con sistemas externos o exportaci贸n de datos.

## Parte 2: Data Partitioning
La partici贸n de datos es clave para mejorar la eficiencia en procesamiento y consultas sobre grandes vol煤menes de datos.

### Conceptos Clave
- **Particiones:** divisi贸n l贸gica de tablas basadas en columnas (ej. fecha, regi贸n). Cada partici贸n se almacena como subdirectorio en HDFS.
- **Buckets:** subdivisi贸n dentro de una partici贸n que distribuye filas seg煤n un hash de columnas; 煤til para joins y sampling.
- **Ventajas:**
	- Reduce la cantidad de datos escaneados en consultas.
	- Mejora el rendimiento de joins y agregaciones.
	- Facilita la organizaci贸n y mantenimiento de datos hist贸ricos.

### Ejemplo de Creaci贸n de Tabla Particionada en Hive
{% raw %}
```sql
CREATE TABLE ventas (
	id INT,
	producto STRING,
	monto DOUBLE
)
PARTITIONED BY (anio INT, mes INT)
STORED AS PARQUET;
```
{% endraw %}`

### Ejemplo de Consulta Particionada

{% raw %}
```sql
SELECT producto, SUM(monto)
FROM ventas
WHERE anio = 2025 AND mes = 12
GROUP BY producto;
```
{% endraw %}

* Solo se escanean los datos correspondientes a la partici贸n seleccionada, mejorando el rendimiento de la consulta.




## Parte 3: Capturing Data with Apache Flume
Apache Flume es un sistema distribuido dise帽ado para la ingesti贸n eficiente de grandes vol煤menes de datos de manera confiable hacia Hadoop.

### Componentes Principales
- **Source:** origen de los datos (logs de aplicaciones, sensores, sistemas de mensajer铆a).
- **Channel:** almacenamiento temporal de eventos mientras esperan ser enviados al destino.
- **Sink:** destino final de los datos, como HDFS, HBase o Kafka.

### Funcionalidades
- Captura de datos en tiempo real desde m煤ltiples fuentes.
- Tolerancia a fallos mediante canales persistentes.
- Escalabilidad horizontal mediante agentes Flume adicionales.
- Integraci贸n con Spark para procesamiento en tiempo real o batch.

### Ejemplo de Configuraci贸n B谩sica
{% raw %}
```properties
# Flume agent configuration
agent.sources = source1
agent.channels = channel1
agent.sinks = sink1

agent.sources.source1.type = exec
agent.sources.source1.command = tail -F /var/log/app.log
agent.sources.source1.channels = channel1

agent.channels.channel1.type = memory
agent.sinks.sink1.type = hdfs
agent.sinks.sink1.hdfs.path = hdfs://namenode:8020/logs/
agent.sinks.sink1.channel = channel1
```
{% endraw %}`

## Parte 3: Spark Basics

Apache Spark es un motor de procesamiento distribuido dise帽ado para procesamiento en memoria de grandes vol煤menes de datos.

### Conceptos Fundamentales

* **Driver Program:** coordina la ejecuci贸n de la aplicaci贸n Spark.
* **Cluster Manager:** administra los recursos del cluster (YARN, Standalone, Mesos, Kubernetes).
* **Executors:** nodos donde se ejecutan las tareas en paralelo.
* **Task:** unidad m铆nima de trabajo ejecutada en un executor.
* **RDD (Resilient Distributed Dataset):** colecci贸n distribuida de datos tolerante a fallos.
* **DataFrame/Dataset:** abstracciones de datos estructurados que facilitan consultas y optimizaciones.

### Ventajas

* Procesamiento en memoria para alta velocidad.
* APIs unificadas para batch y stream.
* Soporte para m煤ltiples lenguajes: Scala, Java, Python y R.
* Optimizaci贸n autom谩tica con Catalyst y Tungsten.

## Parte 3: Working with RDDs in Spark

Los RDDs son la base de Spark y permiten el procesamiento distribuido y tolerante a fallos de los datos.

### Operaciones Principales

* **Transformations:** operaciones que crean nuevos RDDs (p. ej., map, filter, flatMap, reduceByKey). Se ejecutan de manera perezosa (lazy evaluation).
* **Actions:** operaciones que devuelven resultados o escriben datos (p. ej., collect, count, saveAsTextFile).

### Persistencia y Particionamiento

* **Caching/Persisting:** almacena RDD en memoria o disco para acelerar consultas repetidas.
* **Particionamiento:** distribuir datos entre nodos para paralelismo y minimizar shuffles.

### Ejemplo B谩sico en PySpark

{% raw %}
```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
rdd_filtered = rdd.filter(lambda x: x % 2 == 0)
rdd_sum = rdd_filtered.reduce(lambda a, b: a + b)
print(rdd_sum)  # Output: 6
```
{% endraw %}

## Parte 3: Writing and Deploying Spark Applications

### Estructura de una Aplicaci贸n Spark

* Definir **SparkSession** o **SparkContext**.
* Leer y transformar datos usando RDDs, DataFrames o Datasets.
* Aplicar transformaciones y acciones seg煤n la l贸gica de negocio.
* Guardar resultados en HDFS, bases de datos o sistemas de almacenamiento compatibles.

### Ejemplo de Aplicaci贸n PySpark

{% raw %}
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("EjemploAppSpark").getOrCreate()

df = spark.read.csv("hdfs://namenode:8020/data/ventas.csv", header=True, inferSchema=True)
df_filtered = df.filter(df.monto > 100)
df_agg = df_filtered.groupBy("producto").sum("monto")
df_agg.write.parquet("hdfs://namenode:8020/output/ventas_agg.parquet")

spark.stop()
```
{% endraw %}

### Despliegue

* **Modo Local:** para desarrollo y pruebas en m谩quina local.
* **Modo Cluster:** enviar la aplicaci贸n a un cluster Hadoop/Spark para ejecuci贸n distribuida.
* **Comando spark-submit:**

{% raw %}
```bash
spark-submit \
--master yarn \
--deploy-mode cluster \
--num-executors 4 \
--executor-memory 4G \
--executor-cores 2 \
mi_aplicacion.py
```
{% endraw %}

* Par谩metros importantes:

  * `--master`: tipo de cluster manager.
  * `--deploy-mode`: modo de despliegue (client o cluster).
  * `--num-executors`, `--executor-memory`, `--executor-cores`: configuraci贸n de recursos para optimizaci贸n del rendimiento.





## Parte 4: Parallel Programming with Spark
Spark permite el procesamiento paralelo de grandes vol煤menes de datos mediante la distribuci贸n de tareas entre m煤ltiples nodos de un cluster.

### Conceptos Clave
- **Task:** unidad m铆nima de trabajo que se ejecuta en un executor.
- **Stage:** conjunto de tareas que se pueden ejecutar en paralelo sin necesidad de shuffle.
- **Shuffle:** redistribuci贸n de datos entre nodos durante operaciones que requieren agregaci贸n o joins, puede ser costosa en t茅rminos de rendimiento.
- **Partitioning:** divisi贸n de RDD/DataFrame en particiones que se distribuyen entre nodos para paralelismo.
- **Lazy Evaluation:** las transformaciones no se ejecutan inmediatamente, se construye un DAG (Directed Acyclic Graph) de operaciones que se ejecuta al llamar a una acci贸n.

### Buenas Pr谩cticas
- Minimizar shuffles usando operaciones como `reduceByKey` en lugar de `groupByKey`.
- Alinear el n煤mero de particiones con la cantidad de cores disponibles en el cluster.
- Usar `mapPartitions` para optimizar procesamiento cuando se necesita inicializar recursos costosos por partici贸n.

## Parte 4: Spark Caching and Persistence
Caching y persistence permiten almacenar RDD o DataFrames en memoria o disco para acelerar operaciones repetidas.

### Tipos de Persistencia
- **MEMORY_ONLY:** almacena en memoria RAM, m谩s r谩pido pero depende de la memoria disponible.
- **MEMORY_AND_DISK:** almacena en memoria y si se llena se escribe en disco.
- **DISK_ONLY:** almacena solo en disco, 煤til para datasets muy grandes.
- **MEMORY_ONLY_SER / MEMORY_AND_DISK_SER:** versiones serializadas que ocupan menos espacio en memoria.

### Ejemplo en PySpark
{% raw %}
```python
rdd = spark.sparkContext.textFile("hdfs://namenode:8020/logs/app.log")
rdd_filtered = rdd.filter(lambda line: "ERROR" in line)
rdd_filtered.persist(storageLevel=StorageLevel.MEMORY_AND_DISK)
count_errors = rdd_filtered.count()
```
{% endraw %}`

### Beneficios

* Reducci贸n del tiempo de ejecuci贸n en pipelines iterativos.
* Mejora del rendimiento de algoritmos de ML o procesamiento de grafos.
* Control del uso de memoria seg煤n tama帽o del dataset y recursos disponibles.

## Parte 4: Common Patterns in Spark Data Processing

Algunos patrones comunes que facilitan el desarrollo de aplicaciones Spark:

* **Map-Reduce:** transformaci贸n seguida de agregaci贸n.
* **Filtering:** eliminaci贸n de registros irrelevantes antes de an谩lisis pesado.
* **Joining:** combinaci贸n de datasets basados en claves; optimizar usando broadcast joins si un dataset es peque帽o.
* **Aggregation:** sumarizaci贸n de datos por columnas o grupos.
* **Windowing:** c谩lculos sobre ventanas de tiempo o grupos de filas para an谩lisis temporales o secuenciales.

### Ejemplo de Patr贸n Map-Reduce

{% raw %}
```python
rdd = spark.sparkContext.parallelize([("a",1), ("b",1), ("a",2)])
rdd_agg = rdd.reduceByKey(lambda x, y: x + y)
rdd_agg.collect()  # Output: [('a',3), ('b',1)]
```
{% endraw %}

## Parte 4: Preview: Spark SQL

Spark SQL permite consultar datos estructurados usando SQL y DataFrames, integrando optimizaciones autom谩ticas del motor Catalyst.

* Permite ejecutar consultas sobre RDD, DataFrames o tablas Hive.
* Ofrece funciones integradas para agregaciones, joins y operaciones complejas.
* Soporta integraci贸n con BI tools y visualizaci贸n de datos.
* Ejemplo b谩sico:

{% raw %}
```python
df.createOrReplaceTempView("ventas")
result = spark.sql("SELECT producto, SUM(monto) as total FROM ventas GROUP BY producto")
result.show()
```
{% endraw %}

## Parte 4: Conclusion

* Apache Spark proporciona un entorno unificado para procesamiento batch, streaming, SQL y machine learning.
* Aprovecha la distribuci贸n de datos y el paralelismo para manejar grandes vol煤menes de informaci贸n.
* La combinaci贸n de RDDs, DataFrames, caching, persistencia y patrones de programaci贸n paralela permite desarrollar aplicaciones eficientes y escalables.
* Spark SQL y la integraci贸n con el ecosistema Hadoop (Hive, HDFS, Sqoop, Flume, Impala) ampl铆an las capacidades de an谩lisis y procesamiento de datos estructurados y semiestructurados.

# DESARROLLADOR CLOUDERA PARA APACHE SPARK II
Objetivo: Al finalizar el m贸dulo el alumno ser谩 capaz de: simplificar el desarrollo con Kite SDK, definir y usar Data Sets, importar datos relacionales con Apache Sqoop, capturar datos con Apache Flume y desarrollar componentes personalizados, manejar Workflows con Apache Oozie, procesar pipeline de datos con Apache Crunch, leer y analizar formatos de datos customizados en Apache Hive, responder queries interactivas con Impala, transformar Data Streams con Morphlines, autorizar b煤squeda completa en los datos guardados con HDFS y presentar resultados a los usuarios.

## Parte 1: Arquitectura de aplicaciones
La arquitectura de aplicaciones en el ecosistema Cloudera est谩 dise帽ada para facilitar el procesamiento de datos masivos de manera distribuida y escalable, integrando herramientas de ingesti贸n, almacenamiento, an谩lisis y visualizaci贸n.

### Componentes Principales
- **Apache Spark:** motor central para procesamiento batch y streaming.
- **HDFS:** almacenamiento distribuido que garantiza tolerancia a fallos y escalabilidad.
- **Hive y Impala:** motores para consultas SQL sobre datos estructurados.
- **Sqoop y Flume:** herramientas de ingesti贸n de datos relacionales y streaming.
- **Kite SDK:** simplifica el desarrollo y manejo de datasets en Hadoop.
- **Oozie:** coordinador de workflows para tareas programadas y pipelines complejos.

### Patrones de Arquitectura
- **Lambda Architecture:** combina procesamiento batch (Spark/Hive) con procesamiento en tiempo real (Spark Streaming/Flume).
- **Data Lake:** almacenamiento centralizado de datos crudos y procesados en HDFS, accesible por m煤ltiples herramientas.
- **Pipelines de datos:** definici贸n de flujos ETL desde ingesti贸n, transformaci贸n, almacenamiento y consumo por aplicaciones.

## Parte 1: Simplificando el desarrollo con Kite SDK
Kite SDK es un framework que simplifica la interacci贸n con datasets en Hadoop, proporcionando APIs de alto nivel para lectura, escritura y validaci贸n de datos.

### Funcionalidades Clave
- Manejo de **Data Sets** con esquemas definidos.
- Operaciones de lectura/escritura en HDFS sin necesidad de manejar directamente el bajo nivel del sistema.
- Validaci贸n autom谩tica de datos contra el esquema definido.
- Integraci贸n con herramientas de desarrollo y testing para pipelines de datos.

### Ejemplo de Uso B谩sico (Java)
{% raw %}
```java
Dataset<Record> dataset = Datasets.create("ventas", Record.class);
Record record = new Record();
record.put("producto", "Laptop");
record.put("monto", 1200.0);
dataset.write(record);
```
{% endraw %}`

## Parte 1: Definiendo y usando Data Sets

Los Data Sets son abstracciones de colecciones de datos con esquema, optimizadas para Hadoop y Spark.

### Ventajas

- Estandarizaci贸n de datos y consistencia de esquemas.
- Integraci贸n con Spark para procesamiento distribuido.
- Facilita la serializaci贸n y deserializaci贸n de datos.
- Permite validaciones autom谩ticas al escribir datos.

### Operaciones Comunes

- **Creaci贸n:** definir nombre y tipo de esquema.
- **Lectura:** acceder a los datos mediante API de alto nivel.
- **Escritura:** persistir datos en HDFS o en formatos column-oriented (Avro, Parquet).
- **Transformaci贸n:** map, filter y agregaciones usando APIs de Spark integradas.

## Parte 1: Importaci贸n de datos relacionales con Apache Sqoop

Apache Sqoop permite la transferencia eficiente de datos entre bases de datos relacionales (RDBMS) y Hadoop.

### Funcionalidades Principales

- Importar tablas de bases de datos relacionales a HDFS, Hive o HBase.
- Exportar datos desde Hadoop hacia bases de datos.
- Soporte para m煤ltiples RDBMS: MySQL, PostgreSQL, Oracle, SQL Server.
- Integraci贸n con Spark y Hive para procesamiento posterior.

### Ejemplo de Importaci贸n B谩sica

{% raw %}
```bash
sqoop import \
--connect jdbc:mysql://localhost:3306/empresa \
--username usuario \
--password clave \
--table empleados \
--target-dir /user/hadoop/empleados \
--m 1
```
{% endraw %}

- `--m 1` indica el n煤mero de mappers (tareas paralelas) para la importaci贸n.
- Los datos importados pueden ser procesados con Spark, Hive o Impala.

## Parte 2: Captura de datos con Apache Flume
Apache Flume es un sistema distribuido dise帽ado para recolectar, agregar y mover grandes vol煤menes de datos de manera confiable hacia HDFS u otros sistemas de almacenamiento.

### Componentes Principales
- **Source:** origen de los datos, como logs de aplicaciones, sensores o sistemas de mensajer铆a.
- **Channel:** almacenamiento temporal de eventos mientras esperan ser enviados al destino.
- **Sink:** destino final de los datos, como HDFS, HBase o Kafka.

### Funcionalidades
- Captura de datos en tiempo real desde m煤ltiples fuentes.
- Tolerancia a fallos mediante canales persistentes.
- Escalabilidad horizontal mediante m煤ltiples agentes Flume.
- Integraci贸n con Spark para procesamiento posterior.

### Ejemplo de Configuraci贸n B谩sica
{% raw %}
```properties
agent.sources = source1
agent.channels = channel1
agent.sinks = sink1

agent.sources.source1.type = exec
agent.sources.source1.command = tail -F /var/log/app.log
agent.sources.source1.channels = channel1

agent.channels.channel1.type = memory
agent.sinks.sink1.type = hdfs
agent.sinks.sink1.hdfs.path = hdfs://namenode:8020/logs/
agent.sinks.sink1.channel = channel1
```
{% endraw %}`

## Parte 2: Desarrollo de componentes Flume customizados

Flume permite extender su funcionalidad mediante la creaci贸n de componentes personalizados: Sources, Channels o Sinks.

### Ejemplos de Customizaci贸n

* **Custom Source:** leer datos de APIs o sistemas propietarios.
* **Custom Sink:** enviar datos a sistemas de almacenamiento no soportados por defecto.
* **Custom Channel:** implementar pol铆ticas de almacenamiento temporal espec铆ficas.

### Ventajas

* Flexibilidad para adaptarse a arquitecturas de ingesti贸n complejas.
* Posibilidad de aplicar l贸gica de filtrado o transformaci贸n antes del env铆o.
* Integraci贸n transparente con pipelines de datos existentes.

## Parte 2: Manejo de Workflows con Apache Oozie

Apache Oozie es un coordinador de workflows que permite automatizar y programar la ejecuci贸n de tareas en Hadoop y Spark.

### Funcionalidades Clave

* Definici贸n de workflows con DAG de tareas.
* Soporte para tareas Hadoop, Spark, Hive, Pig y shell scripts.
* Programaci贸n de trabajos mediante triggers de tiempo o eventos.
* Manejo de dependencias y reintentos autom谩ticos.

### Ejemplo de Workflow B谩sico (XML)

{% raw %}
```xml
<workflow-app name="ejemplo_workflow" xmlns="uri:oozie:workflow:0.5">
    <start to="spark-node"/>
    <action name="spark-node">
        <spark xmlns="uri:oozie:spark-action:1.0">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <master>yarn-cluster</master>
            <mode>cluster</mode>
            <name>EjemploSparkApp</name>
            <class>com.ejemplo.SparkApp</class>
            <jar>hdfs://user/jars/ejemplo-spark.jar</jar>
        </spark>
        <ok to="end"/>
        <error to="fail"/>
    </action>
    <kill name="fail">
        <message>Workflow fall贸</message>
    </kill>
    <end name="end"/>
</workflow-app>
```
{% endraw %}

## Parte 2: Procesamiento de pipeline de datos con Apache Crunch

Apache Crunch es un framework para construir pipelines de datos escalables sobre Hadoop y Spark, ofreciendo APIs de alto nivel para procesamiento batch.

### Conceptos Clave

* **PCollections:** colecciones distribuidas de datos, equivalentes a RDDs o DataFrames.
* **Pipelines:** secuencias de transformaciones y acciones sobre PCollections.
* **Operations:** map, filter, groupByKey, join y combinaciones complejas.

### Ventajas

* Simplifica la creaci贸n de pipelines de datos sin manejar directamente MapReduce.
* Permite expresividad mediante APIs funcionales.
* Integraci贸n con Hive y otros componentes del ecosistema Hadoop para entrada/salida de datos.

### Ejemplo de Pipeline B谩sico en Java

{% raw %}
```java
Pipeline pipeline = new MRPipeline(EjemploCrunch.class, getConf());
PCollection<String> lines = pipeline.readTextFile("hdfs://user/data/input.txt");
PCollection<String> errors = lines.filter(new FilterFn<String>() {
    public boolean accept(String line) { return line.contains("ERROR"); }
});
errors.write(To.textFile("hdfs://user/data/errors_output.txt"));
pipeline.run();
```
{% endraw %}

* El pipeline lee datos desde HDFS, filtra registros de error y los escribe en un directorio de salida.

## Parte 3: Leer y analizar formatos de datos customizados en Apache Hive
Apache Hive permite trabajar con datos estructurados y semi-estructurados, incluyendo formatos customizados mediante **SerDes (Serializer/Deserializer)**.

### Conceptos Clave
- **SerDe:** mecanismo que convierte datos desde su representaci贸n en archivos a estructuras internas de Hive y viceversa.
- **Formatos Customizados:** datos no est谩ndar como JSON, XML, Avro o formatos propios de la organizaci贸n.
- **Tabla Externa:** permite leer datos existentes en HDFS sin moverlos ni copiarlos.

### Ejemplo de Tabla con Formato JSON
{% raw %}
```sql
CREATE EXTERNAL TABLE logs_json (
    user_id STRING,
    action STRING,
    timestamp STRING
)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
STORED AS TEXTFILE
LOCATION '/user/hive/logs_json';
```
{% endraw %}`

* Hive utiliza el SerDe para interpretar los archivos JSON y permitir consultas SQL sobre ellos.

### Ventajas

* Flexibilidad para trabajar con datos de diferentes formatos.
* Integraci贸n con pipelines de ingesti贸n de Flume o Sqoop.
* Capacidad de an谩lisis usando HiveQL, Spark SQL o Impala.

## Parte 3: Responder queries interactivas con Impala

Impala permite ejecutar consultas SQL de baja latencia sobre datos almacenados en HDFS o Hive Metastore.

### Caracter铆sticas

* Consulta r谩pida, ideal para dashboards o reporting.
* Compatible con HiveQL, lo que facilita interoperabilidad con Hive.
* Optimizaci贸n interna para reducir I/O y acelerar joins y agregaciones.

### Ejemplo de Query Interactiva

{% raw %}
```sql
SELECT producto, SUM(monto) as total_ventas
FROM ventas
WHERE fecha BETWEEN '2025-01-01' AND '2025-12-31'
GROUP BY producto
ORDER BY total_ventas DESC;
```
{% endraw %}

* Solo escanea las particiones necesarias si la tabla est谩 particionada, mejorando rendimiento.

### Buenas Pr谩cticas

* Particionar y bucketing para mejorar eficiencia.
* Mantener estad铆sticas de tablas actualizadas para optimizador de Impala.
* Usar views para consultas complejas reutilizables.

## Parte 3: Transformaci贸n de Data Streams con Morphlines

Morphlines es un framework ligero que permite procesar, limpiar y transformar datos en tiempo real antes de almacenarlos en HDFS o bases de datos.

### Funcionalidades

* Parsing de formatos complejos como JSON, XML, CSV, Avro.
* Limpieza y normalizaci贸n de datos (trimming, conversi贸n de tipos, filtrado).
* Enriquecimiento de datos mediante transformaciones y reglas definidas.
* Integraci贸n con Flume para ingesti贸n continua hacia Hadoop.

### Ejemplo B谩sico de Configuraci贸n Morphlines

{% raw %}
```json
{
  "commands": [
    { "readJson": {} },
    { "setFields": { "fields": { "timestamp": "${currentTime}" } } },
    { "removeFields": { "fields": ["temp"] } },
    { "writeHDFS": { "path": "/user/hadoop/processed_logs" } }
  ]
}
```
{% endraw %}

## Parte 3: Autorizar B煤squeda completa en los datos guardados con HDFS

HDFS permite implementar mecanismos de autorizaci贸n y seguridad para controlar el acceso a los datos.

### Conceptos Clave

* **Permisos POSIX:** lectura, escritura y ejecuci贸n sobre archivos y directorios.
* **ACLs (Access Control Lists):** control granular de usuarios y grupos.
* **Integraci贸n con Kerberos:** autenticaci贸n segura en entornos corporativos.
* **HDFS Search / Full-Text Indexing:** integraci贸n con herramientas de b煤squeda para localizar datos r谩pidamente.

### Beneficios

* Seguridad y control de acceso centralizado sobre datos cr铆ticos.
* Auditor铆a y trazabilidad de accesos y modificaciones.
* Capacidad de habilitar b煤squedas eficientes sin comprometer la integridad de los datos.



## Parte 4: Presentaci贸n de resultados a los usuarios
La presentaci贸n de resultados en el ecosistema Hadoop/Spark implica transformar los datos procesados en formatos 煤tiles para an谩lisis, dashboards o reportes interactivos.

### Estrategias Comunes
- **Exportaci贸n a Hive/Impala:** permite que los usuarios consulten los resultados mediante SQL.
- **Dashboards y BI Tools:** integraci贸n con herramientas como Tableau, Power BI o Hue para visualizaci贸n de datos.
- **Archivos planos o formatos column-oriented:** CSV, Parquet u ORC para interoperabilidad y an谩lisis adicional.
- **APIs o servicios REST:** exponer resultados procesados para aplicaciones externas o servicios web.

### Buenas Pr谩cticas
- Guardar resultados particionados para mejorar rendimiento en consultas.
- Documentar esquemas y metadatos de los resultados.
- Mantener consistencia y versionado de datos procesados.

## Parte 4: Trabajo de RDDs
Los RDDs (Resilient Distributed Datasets) son la unidad fundamental de procesamiento en Spark.

### Operaciones B谩sicas
- **Transformaciones:** map, filter, flatMap, reduceByKey; se ejecutan de forma perezosa (lazy evaluation).
- **Acciones:** collect, count, take, saveAsTextFile; disparan la ejecuci贸n de las transformaciones.
- **Persistencia:** caching en memoria o disco para optimizar pipelines iterativos.
- **Particionamiento:** distribuci贸n de datos entre nodos para paralelismo y eficiencia en joins/aggregations.

### Ejemplo PySpark
{% raw %}
```python
rdd = spark.sparkContext.parallelize([1,2,3,4,5])
rdd_squared = rdd.map(lambda x: x**2)
print(rdd_squared.collect())  # Output: [1,4,9,16,25]
```
{% endraw %}`

## Parte 4: El Hadoop Distributed File System

HDFS es el sistema de almacenamiento distribuido de Hadoop que garantiza tolerancia a fallos y escalabilidad.

### Conceptos Clave

* **NameNode:** nodo maestro que mantiene el metadata del sistema de archivos.
* **DataNode:** nodos esclavos que almacenan bloques de datos.
* **Replicaci贸n de bloques:** por defecto 3, asegura disponibilidad y resiliencia.
* **Particiones de archivos:** HDFS divide archivos en bloques de tama帽o configurable (default 128 MB).

### Ventajas

* Almacenamiento escalable para grandes vol煤menes de datos.
* Tolerancia a fallos mediante replicaci贸n de bloques.
* Optimizado para lectura secuencial en batch processing.

## Parte 4: Ejecutar Spark en un Cluster

Spark puede ejecutarse en diferentes modos para aprovechar recursos de un cluster distribuido.

### Modos de Ejecuci贸n

* **Local:** para desarrollo y pruebas en una sola m谩quina.
* **Standalone Cluster:** Spark gestiona sus propios recursos.
* **YARN:** Spark se integra con Hadoop YARN para gesti贸n de recursos.
* **Mesos/Kubernetes:** integraci贸n con otros gestores de cluster.

### Uso de spark-submit

{% raw %}
```bash
spark-submit \
--master yarn \
--deploy-mode cluster \
--num-executors 4 \
--executor-memory 4G \
--executor-cores 2 \
mi_aplicacion.py
```
{% endraw %}

* `--master`: tipo de cluster manager.
* `--deploy-mode`: cluster o client.
* `--num-executors`, `--executor-memory`, `--executor-cores`: ajuste de recursos seg煤n tama帽o del dataset y capacidad del cluster.

### Buenas Pr谩cticas

* Ajustar particiones de datos seg煤n n煤mero de cores y nodos disponibles.
* Evitar shuffles innecesarios.
* Monitorear ejecuci贸n con Spark UI para identificar cuellos de botella.

## Parte 5: Programaci贸n paralela con Spark
La programaci贸n paralela en Spark permite distribuir tareas y datos a trav茅s de m煤ltiples nodos de un cluster para un procesamiento eficiente y escalable.

### Conceptos Clave
- **Tasks:** unidades m铆nimas de trabajo ejecutadas en los executors.
- **Stages:** conjuntos de tareas que pueden ejecutarse en paralelo sin necesidad de shuffle.
- **Shuffle:** redistribuci贸n de datos entre nodos para operaciones de join, groupBy o agregaci贸n; costosa en t茅rminos de rendimiento.
- **Lazy Evaluation:** las transformaciones se registran pero no se ejecutan hasta que se llama a una acci贸n, optimizando el flujo de trabajo.
- **Partitioning:** dividir RDD/DataFrame en particiones distribuidas para maximizar paralelismo y minimizar movimiento de datos.

### Buenas Pr谩cticas
- Minimizar shuffles usando `reduceByKey` o broadcast joins.
- Alinear n煤mero de particiones con la cantidad de cores disponibles.
- Usar `mapPartitions` cuando la inicializaci贸n de recursos por elemento sea costosa.

## Parte 5: Caching y Persistence
El caching y persistence permiten almacenar RDDs o DataFrames en memoria o disco para acelerar operaciones iterativas o repetidas.

### Tipos de Persistencia
- **MEMORY_ONLY:** almacena en memoria, r谩pido pero depende de RAM disponible.
- **MEMORY_AND_DISK:** almacena en memoria y si se llena, guarda en disco.
- **DISK_ONLY:** almacena solo en disco; 煤til para datasets muy grandes.
- **MEMORY_ONLY_SER / MEMORY_AND_DISK_SER:** versiones serializadas para reducir uso de memoria.

### Ejemplo PySpark
{% raw %}
```python
from pyspark import StorageLevel

rdd = spark.sparkContext.textFile("hdfs://namenode:8020/logs/app.log")
rdd_errors = rdd.filter(lambda line: "ERROR" in line)
rdd_errors.persist(StorageLevel.MEMORY_AND_DISK)
count_errors = rdd_errors.count()
```
{% endraw %}`

### Beneficios

* Reducci贸n del tiempo de ejecuci贸n en pipelines iterativos.
* Mejor rendimiento en algoritmos de ML o procesamiento de grafos.
* Flexibilidad en la gesti贸n de memoria seg煤n tama帽o del dataset.

## Parte 5: Escritura de aplicaciones Spark

Desarrollar aplicaciones Spark implica estructurar el flujo de procesamiento de manera eficiente y escalable.

### Estructura T铆pica

* **SparkSession/SparkContext:** inicializa la aplicaci贸n.
* **Lectura de datos:** desde HDFS, Hive, Kafka o bases de datos.
* **Transformaciones y Acciones:** RDDs, DataFrames o Datasets.
* **Escritura de resultados:** HDFS, Hive, bases de datos o sistemas externos.
* **Cierre de la aplicaci贸n:** liberar recursos del cluster.

### Ejemplo PySpark

{% raw %}
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("EjemploAppSpark").getOrCreate()

df = spark.read.csv("hdfs://namenode:8020/data/ventas.csv", header=True, inferSchema=True)
df_filtered = df.filter(df.monto > 100)
df_agg = df_filtered.groupBy("producto").sum("monto")
df_agg.write.parquet("hdfs://namenode:8020/output/ventas_agg.parquet")

spark.stop()
```
{% endraw %}

### Despliegue con spark-submit

{% raw %}
```bash
spark-submit \
--master yarn \
--deploy-mode cluster \
--num-executors 4 \
--executor-memory 4G \
--executor-cores 2 \
mi_aplicacion.py
```
{% endraw %}

## Parte 5: Spark, Hadoop y el Enterprise Data Center

La integraci贸n de Spark y Hadoop en un Enterprise Data Center permite aprovechar recursos de infraestructura corporativa para procesamiento y an谩lisis de Big Data.

### Beneficios

* **Escalabilidad:** distribuir procesamiento y almacenamiento entre m煤ltiples nodos f铆sicos.
* **Seguridad:** autenticaci贸n y autorizaci贸n centralizadas mediante Kerberos y ACLs.
* **Integraci贸n:** interoperabilidad con Hive, Impala, Flume, Sqoop, Morphlines y otros componentes del ecosistema.
* **Optimizaci贸n de recursos:** Spark permite paralelismo y caching; Hadoop provee almacenamiento tolerante a fallos.
* **Flexibilidad:** soporte para datos batch, streaming y anal铆tica en tiempo real.

### Buenas Pr谩cticas

* Planificaci贸n de recursos seg煤n carga de trabajo.
* Monitorizaci贸n con Spark UI, Ganglia o Prometheus.
* Uso de pipelines de datos y workflows coordinados para tareas ETL y anal铆ticas.
* Implementaci贸n de pol铆ticas de seguridad y auditor铆a para datos cr铆ticos.

## Parte 6: Spark Streaming
Spark Streaming permite procesar flujos de datos en tiempo real utilizando la misma API de Spark que para batch, facilitando la integraci贸n y reutilizaci贸n de c贸digo.

### Conceptos Clave
- **DStreams (Discretized Streams):** abstracci贸n de flujos de datos como series de RDDs.
- **Micro-batches:** procesamiento de datos en peque帽os lotes para reducir latencia.
- **Integraci贸n con Fuentes de Datos:** Kafka, Flume, sockets TCP, HDFS.
- **Sinks:** salida de datos a HDFS, bases de datos, dashboards o sistemas externos.

### Ejemplo B谩sico en PySpark
{% raw %}
```python
from pyspark.sql import SparkSession
from pyspark.streaming import StreamingContext

spark = SparkSession.builder.appName("StreamingApp").getOrCreate()
ssc = StreamingContext(spark.sparkContext, 5)  # micro-batch cada 5 segundos

lines = ssc.socketTextStream("localhost", 9999)
errors = lines.filter(lambda line: "ERROR" in line)
errors.pprint()

ssc.start()
ssc.awaitTermination()
```
{% endraw %}`

### Buenas Pr谩cticas

* Ajustar tama帽o de micro-batches seg煤n volumen de datos y recursos.
* Usar checkpointing para tolerancia a fallos.
* Aplicar transformaciones eficientes y persistir datos cuando sea necesario.

## Parte 6: Algoritmos usuales en Spark

Spark proporciona soporte nativo para algoritmos comunes en Big Data y Machine Learning, a trav茅s de MLlib y GraphX.

### Ejemplos

* **Filtrado y agregaci贸n:** map, filter, reduceByKey.
* **Machine Learning:** regresi贸n, clasificaci贸n, clustering (KMeans, LogisticRegression, Decision Trees).
* **Procesamiento de grafos:** PageRank, Connected Components, Graph Traversals.
* **Operaciones de joins y aggregations:** para analytics en grandes datasets.

### Buenas Pr谩cticas

* Elegir algoritmos distribuidos y optimizados para Spark.
* Evitar operaciones que generan shuffles innecesarios.
* Persistir datos intermedios cuando se reutilizan en m煤ltiples pasos.

## Parte 6: Mejora de rendimiento Spark

Optimizar aplicaciones Spark es clave para eficiencia y reducci贸n de costos en clusters.

### Estrategias

* **Uso de persistencia/caching:** guardar RDD/DataFrame que se reutiliza.
* **Particionamiento adecuado:** ajustar n煤mero y tama帽o de particiones seg煤n cores y nodos.
* **Minimizar shuffles:** usar reduceByKey en lugar de groupByKey, broadcast joins para datasets peque帽os.
* **Optimizaci贸n de queries SQL:** Spark SQL optimiza consultas con Catalyst y Tungsten.
* **Uso de formatos columna orientados:** Parquet u ORC para acelerar lectura y reducir I/O.

### Monitoreo y Tuning

* Spark UI: identificar cuellos de botella, tiempos de stage y task.
* Ajuste de memoria y recursos: executor-memory, executor-cores, number of executors.
* An谩lisis de DAG para optimizar transformaciones y reducir overhead.

## Parte 6: Conclusion

* Spark es una plataforma unificada para procesamiento batch, streaming, SQL y ML sobre grandes vol煤menes de datos.
* Su integraci贸n con Hadoop y el ecosistema Cloudera permite ingesta, almacenamiento, transformaci贸n y an谩lisis eficientes.
* Las buenas pr谩cticas de programaci贸n paralela, caching, particionamiento y optimizaci贸n de consultas son clave para maximizar rendimiento.
* Con Spark Streaming, pipelines de datos, algoritmos de ML y herramientas de visualizaci贸n, se pueden entregar insights en tiempo real y batch a los usuarios y aplicaciones del Enterprise Data Center.

