---
date: 2025-11-06 15:14
title: Complejidad Logar√≠tmica y Notaci√≥n Big O
tags:
  - CS
  - computer_Science
  - logaritmos
  - big_0
keywords:
source:
status: üåü
Parent: "[[Area-Prog]]"
cssclasses:
  - hide-embedded-header1
categories:
  - Computer Science
public_note: "true"
category: Computer Science
---
# Complejidad Logar√≠tmica y Notaci√≥n Big O
`$= dv.current().file.tags.join(" ")`

- [Computer Science](/uncategorized/computer-science/)
- [mates](/uncategorized/mates/)
## üìà INTRODUCCI√ìN A LA COMPLEJIDAD

La **complejidad algor√≠tmica** mide el crecimiento del tiempo o espacio que necesita un algoritmo en funci√≥n del tama√±o de la entrada (**n**).  
Sirve para comparar la **eficiencia** de distintos algoritmos de forma independiente del hardware o lenguaje de programaci√≥n.

---

## üßÆ NOTACI√ìN BIG O (O GRANDE)

### **Definici√≥n**
- Expresa el **l√≠mite superior** del crecimiento del tiempo de ejecuci√≥n de un algoritmo.  
- Describe el **peor caso** posible.  

**Ejemplo:**
Si un algoritmo tarda aproximadamente `3n¬≤ + 5n + 7` operaciones, se dice que tiene **O(n¬≤)**, ya que el t√©rmino de mayor orden domina para grandes valores de *n*.

### **Interpretaci√≥n**
| Notaci√≥n | Nombre | Ejemplo de algoritmo | Escalado aproximado |
|-----------|---------|----------------------|---------------------|
| **O(1)** | Constante | Acceso a elemento en array | üîπ Independiente de *n* |
| **O(log n)** | Logar√≠tmica | B√∫squeda binaria | üîπ Crece lentamente |
| **O(n)** | Lineal | Recorrer lista completa | üîπ Proporcional a *n* |
| **O(n log n)** | Quasilineal | MergeSort, QuickSort | üîπ Eficiente en grandes datos |
| **O(n¬≤)** | Cuadr√°tica | Doble bucle anidado | üîπ Muy costoso para grandes *n* |
| **O(2‚Åø)** | Exponencial | Fuerza bruta en combinaciones | üîπ Crecimiento explosivo |
| **O(n!)** | Factorial | Permutaciones totales | üîπ Inviable para *n > 10* |

---

## üîπ COMPLEJIDAD LOGAR√çTMICA: O(log n)

### **Concepto**
La complejidad logar√≠tmica aparece cuando el tama√±o del problema **se reduce por un factor constante** en cada paso.

**Ejemplo cl√°sico:**  
En una **b√∫squeda binaria**, cada paso descarta la mitad del espacio de b√∫squeda:
```

n ‚Üí n/2 ‚Üí n/4 ‚Üí n/8 ‚Üí ... ‚Üí 1

```
El n√∫mero de pasos necesarios es `log‚ÇÇ(n)`.

### **Caracter√≠sticas**
- Muy eficiente incluso para entradas grandes.  
- Cada iteraci√≥n reduce el problema **exponencialmente**.  
- Suele aparecer en estructuras de datos **balanceadas** (como BST, heaps, AVL, B-trees).

### **Ejemplo de comparaci√≥n**
| n | O(n) pasos | O(log‚ÇÇ n) pasos |
|---|-------------|----------------|
| 10 | 10 | 3.3 |
| 1000 | 1000 | 10 |
| 1.000.000 | 1.000.000 | 20 |

---

## üî∏ COMPLEJIDAD INVERSA: O(1/log n) o O(‚àön)

Aunque menos comunes, existen algoritmos cuya complejidad decrece o crece sublinealmente.

### **O(‚àön)**
- T√≠pica en algoritmos de b√∫squeda como **Jump Search** o **Block Search**.  
- Divide el conjunto en bloques de tama√±o `‚àön` y reduce el n√∫mero de comparaciones.

### **O(1/log n)**
- Raramente aparece de forma directa; puede aparecer como **eficiencia incremental** en estructuras optimizadas o an√°lisis te√≥ricos.

---

## üß† OTRAS NOTACIONES ASOCIADAS

### **Œò (Theta Grande)**
- Representa el **caso promedio o crecimiento exacto**.
- Ejemplo: un algoritmo que siempre tarda entre `2n` y `4n` ‚Üí **Œò(n)**.

### **Œ© (Omega Grande)**
- Representa el **mejor caso posible** (l√≠mite inferior).
- Ejemplo: b√∫squeda en lista desordenada ‚Üí **Œ©(1)** (si el primer elemento es el correcto).

### **O Peque√±a (o(n)) y Œ© Peque√±a (œâ(n))**
- Describen l√≠mites **estrictos**:
	- **o(n):** crece m√°s lentamente que *n* pero nunca igual.  
	- **œâ(n):** crece m√°s r√°pido que *n* sin llegar al siguiente orden.

Ejemplo:
- `log n = o(n)` ‚Üí log n crece m√°s lento que n.
- `n¬≤ = œâ(n)` ‚Üí n¬≤ crece m√°s r√°pido que n.

---

## ‚öôÔ∏è COMPLEJIDAD COMBINADA

Muchos algoritmos combinan distintas fases con diferentes complejidades:
```

O(n) + O(log n) = O(n)

```
El t√©rmino dominante (de mayor orden) determina la complejidad final.

**Ejemplo:**
- Leer n elementos ‚Üí `O(n)`
- Luego aplicar b√∫squeda binaria en cada uno ‚Üí `O(n log n)`

---

## üìä REPRESENTACI√ìN GR√ÅFICA (Intuitiva)

```

Crecimiento (tiempo)
‚îÇ
‚îÇ                    O(n¬≤)
‚îÇ                /
‚îÇ           /
‚îÇ       /
‚îÇ   /
‚îÇ / O(n)
‚îÇ---- O(log n)
‚îÇ__ O(1)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Tama√±o del input (n)

```

---

## üß© RELACI√ìN CON ESTRUCTURAS DE DATOS

| Estructura | Operaci√≥n | Complejidad | Descripci√≥n |
|-------------|------------|--------------|--------------|
| **Array** | Acceso | O(1) | Posici√≥n directa |
| **Linked List** | B√∫squeda | O(n) | Recorrido secuencial |
| **BST Balanceado** | B√∫squeda | O(log n) | Reducci√≥n del espacio por nivel |
| **Hash Table** | Acceso promedio | O(1) | Dispersi√≥n por clave |
| **Heap** | Inserci√≥n / extracci√≥n | O(log n) | Propiedad de orden parcial |
| **Graph (Dijkstra)** | Camino m√≠nimo | O(E log V) | Logar√≠tmico por cola de prioridad |

---

## üßæ RESUMEN FINAL

| Tipo | Ejemplo | Interpretaci√≥n |
|------|----------|----------------|
| **O(1)** | Hash lookup | Tiempo constante |
| **O(log n)** | B√∫squeda binaria | Mitades sucesivas |
| **O(n)** | Recorrido secuencial | Escalado lineal |
| **O(n log n)** | Ordenamientos √≥ptimos | Divisi√≥n y conquista |
| **O(n¬≤)** | Burbujas, combinaciones | Crecimiento cuadr√°tico |
| **O(2‚Åø)** | Recursi√≥n total | Explosi√≥n combinatoria |
| **O(n!)** | Permutaciones | Inviable |

---

üîç **Conclusi√≥n:**  
La **complejidad logar√≠tmica (O(log n))** es una de las m√°s deseables, ya que ofrece un equilibrio ideal entre velocidad y escalabilidad. Comprender la **notaci√≥n Big O**, junto con **Theta (Œò)** y **Omega (Œ©)**, permite analizar el rendimiento de estructuras y algoritmos con precisi√≥n te√≥rica y pr√°ctica.


# Fundamentos Matem√°ticos para Entender la Complejidad Algor√≠tmica
`$= dv.current().file.tags.join(" ")`

---

## üßÆ 1. CRECIMIENTO DE FUNCIONES

La **complejidad algor√≠tmica** describe c√≥mo crece una funci√≥n de tiempo o espacio a medida que aumenta el tama√±o de la entrada (*n*).  
Por ello, entender **comparaci√≥n de tasas de crecimiento** es esencial.

### **Conceptos Clave**
- **Orden de crecimiento:** c√≥mo una funci√≥n f(n) se comporta conforme *n ‚Üí ‚àû*.  
- **Dominancia:** si `f(n) < g(n)` para grandes *n*, entonces `f(n)` crece m√°s lentamente.  
- **L√≠mites y asint√≥tica:**  
	- Se usan para comparar funciones ignorando constantes y t√©rminos menores.  
	- Ejemplo:  
		`f(n) = 3n¬≤ + 2n + 1 ‚Üí O(n¬≤)` porque el t√©rmino cuadr√°tico domina.

**Matem√°ticamente:**  
\[
\lim_{n \to \infty} \frac{f(n)}{g(n)} =
\begin{cases}
0 & \Rightarrow f(n) = o(g(n)) \\
c \neq 0 & \Rightarrow f(n) = Œò(g(n)) \\
\infty & \Rightarrow f(n) = œâ(g(n))
\end{cases}
\]

---

## üìà 2. LOGARITMOS Y EXPONENCIALES

El **logaritmo** es la funci√≥n inversa de la exponencial, y est√° directamente relacionado con la complejidad **O(log n)**.

### **Definici√≥n**
\[
\log_b(a) = c \quad \text{si y solo si} \quad b^c = a
\]

Ejemplo:  
\[
\log_2(8) = 3 \quad \text{porque} \quad 2^3 = 8
\]

### **Propiedades √∫tiles**
1. **Cambio de base:**  
   \[
   \log_a(b) = \frac{\log_c(b)}{\log_c(a)}
   \]
   ‚Üí La base es irrelevante en Big O.

2. **Multiplicaci√≥n a suma:**  
   \[
   \log(ab) = \log a + \log b
   \]

3. **Potencia a multiplicaci√≥n:**  
   \[
   \log(a^k) = k \log a
   \]

**Implicaci√≥n:**  
Si un algoritmo divide el problema entre 2 en cada paso ‚Üí `O(log‚ÇÇ n)`.  
Si divide entre 10 ‚Üí `O(log‚ÇÅ‚ÇÄ n)`, pero ambas son equivalentes en Big O.

---

## üîÅ 3. SERIES, SUMAS Y PROGRESIONES

Muchos algoritmos implican bucles anidados o recursi√≥n.  
Comprender **series aritm√©ticas y geom√©tricas** es clave para calcular su coste total.

### **Series Aritm√©ticas**
\[
1 + 2 + 3 + ... + n = \frac{n(n + 1)}{2} = O(n¬≤)
\]
‚Üí t√≠pica en bucles dobles.

### **Series Geom√©tricas**
\[
1 + r + r^2 + ... + r^k = \frac{r^{k+1} - 1}{r - 1}
\]
‚Üí si |r| < 1, converge a un valor constante.  
‚Üí se usa en an√°lisis de **divisi√≥n y conquista** (recursiones).

**Ejemplo:**
En *MergeSort*, la serie del coste por nivel es:
\[
O(n) + O(n) + ... + O(n) = O(n \log n)
\]
porque hay `log n` niveles de divisi√≥n.

---

## ‚ôªÔ∏è 4. RECURSIVIDAD Y RELACIONES DE RECURRENCIA

Los algoritmos recursivos se analizan mediante **ecuaciones de recurrencia**, que describen el coste en funci√≥n del tama√±o del subproblema.

### **Ejemplo General**
\[
T(n) = a \cdot T\left(\frac{n}{b}\right) + f(n)
\]
donde:
- *a* = n√∫mero de subproblemas generados.  
- *b* = factor de divisi√≥n.  
- *f(n)* = coste fuera de la recursi√≥n.

### **Teorema Maestro**
Permite resolver muchas recurrencias comunes sin c√°lculo manual.

| Caso | Condici√≥n | Complejidad |
|------|------------|-------------|
| 1Ô∏è‚É£ | \( f(n) = O(n^{\log_b a - Œµ}) \) | \( T(n) = Œò(n^{\log_b a}) \) |
| 2Ô∏è‚É£ | \( f(n) = Œò(n^{\log_b a}) \) | \( T(n) = Œò(n^{\log_b a} \log n) \) |
| 3Ô∏è‚É£ | \( f(n) = Œ©(n^{\log_b a + Œµ}) \) | \( T(n) = Œò(f(n)) \) |

**Ejemplo:**  
MergeSort ‚Üí \( T(n) = 2T(n/2) + O(n) = O(n \log n) \)

---

## üìä 5. AN√ÅLISIS ASINT√ìTICO

### **Idea Principal**
El an√°lisis asint√≥tico ignora:
- Constantes de tiempo: `3n` y `n` son ambos `O(n)`.
- T√©rminos menores: `n¬≤ + n` ‚Üí `O(n¬≤)`.

Se centra en el **comportamiento cuando n es muy grande**.

### **Tipos**
- **Peor caso:** l√≠mite superior ‚Üí Big O.  
- **Mejor caso:** l√≠mite inferior ‚Üí Œ©.  
- **Promedio:** comportamiento esperado ‚Üí Œò.

---

## üî¢ 6. CONCEPTOS DE L√çMITE Y DERIVADA

Las derivadas permiten entender **crecimientos relativos** entre funciones.

### **Comparaci√≥n de crecimiento:**
Si  
\[
\lim_{n‚Üí‚àû} \frac{f(n)}{g(n)} = 0
\]
‚Üí *f* crece m√°s lento que *g*.

**Ejemplo:**
\[
\lim_{n‚Üí‚àû} \frac{\log n}{n} = 0
\]
‚Üí log n crece mucho m√°s lentamente que n.

### **Jerarqu√≠a de crecimiento:**
\[
1 < \log n < n < n \log n < n^2 < 2^n < n! 
\]

---

## üî¢ 7. ALGEBRA Y COMBINATORIA B√ÅSICA

Usadas para estimar operaciones o combinaciones posibles dentro de un algoritmo.

### **Combinatoria**
- **Permutaciones:** n√∫mero de formas de ordenar ‚Üí `n!`  
- **Combinaciones:** n√∫mero de subconjuntos ‚Üí `C(n, k) = n! / (k!(n‚àík)!)`  
- Uso: algoritmos de fuerza bruta, an√°lisis exponencial.

### **Binomio de Newton**
\[
(a + b)^n = \sum_{k=0}^{n} \binom{n}{k} a^{n-k} b^k
\]
‚Üí base para an√°lisis de ramas en √°rboles de recursi√≥n y probabilidades de casos.

---

## üìö 8. TEOR√çA DE CONJUNTOS Y FUNCIONES

Comprender **dominios, rangos** y **mapeos** ayuda a modelar estructuras como tablas hash o grafos.

- **Inyecci√≥n / Biyeci√≥n:** usada en an√°lisis de colisiones de hash.  
- **Relaciones y pares ordenados:** base para representar aristas `(u, v)` en grafos.  
- **Cardinalidad:** mide tama√±o de un conjunto; an√°logo al tama√±o de entrada *n*.

---

## üß† 9. PROBABILIDAD Y ESPERANZA MATEM√ÅTICA

Usada en an√°lisis **promedio o esperado** (randomized algorithms).

### **Conceptos Clave**
- **Valor esperado (E[X]):** coste promedio de una operaci√≥n.  
- **Varianza:** dispersi√≥n del tiempo de ejecuci√≥n.  
- **Linealidad de la esperanza:** permite sumar esperanzas de suboperaciones.

**Ejemplo:**  
En una tabla hash con buena funci√≥n dispersora, el tiempo promedio de b√∫squeda es **O(1)**, aunque el peor caso sea **O(n)**.

---

## ‚öôÔ∏è 10. RESUMEN DE CONCEPTOS IMPRESCINDIBLES

| √Årea | Conceptos Fundamentales | Aplicaci√≥n en Complejidad |
|------|--------------------------|----------------------------|
| **√Ålgebra y l√≠mites** | Crecimiento, dominancia, l√≠mites | Big O, comparaciones asint√≥ticas |
| **Logaritmos / Exponenciales** | log‚ÇÇ, cambio de base, potencias | B√∫squedas, √°rboles, recursi√≥n |
| **Series / Sumatorias** | aritm√©ticas y geom√©tricas | An√°lisis de bucles y divisiones |
| **Recurrencias** | Teorema maestro | An√°lisis recursivo |
| **Derivadas / L√≠mites** | Crecimiento relativo | Jerarqu√≠a de funciones |
| **Combinatoria** | Permutaciones, combinaciones | Algoritmos exponenciales |
| **Probabilidad** | Esperanza, dispersi√≥n | An√°lisis promedio |
| **Teor√≠a de conjuntos** | Cardinalidad, mapeos | Estructuras como hash o grafos |

---

üß© **En resumen:**  
Para dominar la **complejidad algor√≠tmica** es esencial entender c√≥mo crecen las funciones, c√≥mo se comportan los **logaritmos y sumatorias**, y c√≥mo se modelan las **relaciones recursivas**.  
Estas herramientas matem√°ticas forman el lenguaje con el que se mide la eficiencia y la escalabilidad de los algoritmos.

# Complejidad Logar√≠tmica y Notaciones ‚Äî Conceptos Avanzados
`$= dv.current().file.tags.join(" ")`

---

## üìà 1. Escalas de crecimiento y jerarqu√≠a de complejidad

Comprender la relaci√≥n entre diferentes √≥rdenes de crecimiento permite comparar algoritmos con precisi√≥n.

| Orden de crecimiento | Ejemplo | Comportamiento |
|----------------------|----------|----------------|
| `O(1)` | Acceso directo a array | Constante |
| `O(log n)` | B√∫squeda binaria | Crecimiento lento |
| `O(n)` | Recorrido lineal | Proporcional |
| `O(n log n)` | MergeSort, HeapSort | Casi lineal |
| `O(n¬≤)` | B√∫squeda doble | Cuadr√°tico |
| `O(2‚Åø)` | Fuerza bruta combinatoria | Exponencial |
| `O(n!)` | Permutaciones totales | Factorial |

El **crecimiento logar√≠tmico** aparece t√≠picamente cuando **cada paso divide el problema a la mitad**.

---

## üßÆ 2. Profundizaci√≥n matem√°tica: bases y equivalencias del logaritmo

- La **base del logaritmo** en la complejidad (`log‚ÇÇ`, `log‚ÇÅ‚ÇÄ`, `ln`) es **irrelevante** asint√≥ticamente:
	\[
	O(\log_a n) = O(\log_b n)
	\]
	porque las bases se relacionan por un factor constante:
	\[
	\log_a n = \frac{\log_b n}{\log_b a}
	\]

- En an√°lisis algor√≠tmico se usa casi siempre `log‚ÇÇ(n)`, por representar **divisiones binarias** (como en √°rboles o b√∫squeda binaria).

- Para estructuras jer√°rquicas que se dividen en *k* partes, la complejidad generaliza a:
	\[
	T(n) = O(\log_k n)
	\]
	donde k es el factor de divisi√≥n.

---

## üß† 3. Complejidades fraccionales y sublogar√≠tmicas

Existen algoritmos con **complejidades menores que logar√≠tmicas**, aunque muy raros:

- **O(Œ±(n))**, donde `Œ±` es la **funci√≥n inversa de Ackermann**, crece m√°s lentamente que `log n`.
	- Ejemplo: Union-Find / Disjoint Set con *path compression*.
- **O(log log n)** aparece en estructuras avanzadas como **van Emde Boas Trees** o **skip lists**.

Estas funciones representan **eficiencia extrema** en acceso o b√∫squeda.

---

## üß© 4. Notaciones complementarias a Big O

- **Œò (Theta):** acota *exactamente* la tasa de crecimiento.
	\[
	f(n) = Œò(g(n)) \Rightarrow f(n) \text{ crece igual que } g(n)
	\]

- **Œ© (Omega):** cota inferior (m√≠nimo tiempo posible).
	\[
	f(n) = Œ©(g(n)) \Rightarrow f(n) \text{ no crece m√°s lento que } g(n)
	\]

- **o (small-o):** crece *estrictamente m√°s lento* que la funci√≥n comparada.
	\[
	f(n) = o(g(n)) \Rightarrow \lim_{n‚Üí‚àû} \frac{f(n)}{g(n)} = 0
	\]

- **œâ (small-omega):** crece *estrictamente m√°s r√°pido*.
	\[
	f(n) = œâ(g(n)) \Rightarrow \lim_{n‚Üí‚àû} \frac{f(n)}{g(n)} = ‚àû
	\]

Estas notaciones permiten expresar relaciones asint√≥ticas **con mayor precisi√≥n** que `O(...)`.

---

## ‚öóÔ∏è 5. Dominio de recurrencias y el Teorema Maestro

Muchos algoritmos logar√≠tmicos o log-lineales surgen de **recurrencias** del tipo:
\[
T(n) = aT\left(\frac{n}{b}\right) + f(n)
\]

El **Teorema Maestro** ofrece una forma directa de obtener la complejidad:

1. Si `f(n) = O(n^{log_b a - Œµ})`, ‚Üí `T(n) = Œò(n^{log_b a})`
2. Si `f(n) = Œò(n^{log_b a})`, ‚Üí `T(n) = Œò(n^{log_b a} * log n)`
3. Si `f(n) = Œ©(n^{log_b a + Œµ})`, ‚Üí `T(n) = Œò(f(n))`

Ejemplo:  
- **MergeSort:** `T(n) = 2T(n/2) + O(n)` ‚Üí `O(n log n)`
- **Binary Search:** `T(n) = T(n/2) + O(1)` ‚Üí `O(log n)`

---

## üî¢ 6. Aproximaciones y crecimiento real

Para observar la diferencia pr√°ctica entre escalas de complejidad:

| n | log‚ÇÇ(n) | n | n log n | n¬≤ |
|---|----------|---|----------|----|
| 10 | 3.32 | 10 | 33.2 | 100 |
| 100 | 6.64 | 100 | 664 | 10,000 |
| 1,000 | 9.97 | 1,000 | 9,970 | 1,000,000 |

El crecimiento logar√≠tmico es **extremadamente eficiente**, incluso para tama√±os de entrada enormes.

---

## üß© 7. Conceptos matem√°ticos relacionados imprescindibles

- **Logaritmos y propiedades de cambio de base**
- **L√≠mites y tasas de crecimiento (an√°lisis asint√≥tico)**
- **Series y sumatorias**
- **Derivadas y funciones mon√≥tonas** (para comparar crecimientos)
- **Recurrencias y su resoluci√≥n**
- **Teor√≠a de conjuntos infinitos y cardinalidad** (para razonamientos sobre "orden de magnitud")
- **Combinatoria b√°sica y factoriales** (para contrastar crecimiento exponencial y factorial)
- **Teor√≠a de complejidad computacional** (P, NP, EXP)

---

## üî¨ 8. Interpretaci√≥n geom√©trica e intuitiva

- En una gr√°fica logar√≠tmica, `O(log n)` se **aplasta** lentamente con respecto a `O(n)`.  
- Es com√∫n en **procesos jer√°rquicos**, **divisiones recursivas**, o **estructuras balanceadas**.  
- Ejemplo intuitivo:  
	si duplicas el tama√±o del problema, un algoritmo `O(log n)` **solo suma una operaci√≥n adicional**.

---

## üìö 9. Aplicaciones donde aparece la complejidad logar√≠tmica

- B√∫squeda en √°rboles balanceados: AVL Trees, Red-Black Trees, B-Trees
- B√∫squeda binaria en arrays ordenados
- Acceso a estructuras jer√°rquicas o √≠ndices
- Heaps para operaciones de prioridad
- Union-Find / Disjoint Set optimizado
- Skip Lists y Segment Trees

---

## üß≠ 10. Complejidad compuesta y mixta

En la pr√°ctica, los algoritmos combinan diferentes √≥rdenes:

- `O(n log n)` ‚Üí mezcla lineal y logar√≠tmica (divisi√≥n + combinaci√≥n)
- `O(log¬≤ n)` ‚Üí logar√≠tmico anidado (doble divisi√≥n)
- `O(n log log n)` ‚Üí b√∫squeda en estructuras muy optimizadas

Estas complejidades **intermedias** aparecen en algoritmos de compresi√≥n, b√∫squeda avanzada y an√°lisis de datos.

---

## üîó 11. Lecturas recomendadas

- Teorema Maestro
- Recurrencias
- Teor√≠a de la Complejidad Computacional
- √Årboles de B√∫squeda Balanceados
- An√°lisis Asint√≥tico
- Logaritmos y Crecimiento de Funciones

---
