---
date: 2024-11-19 23:32
title: Deep Learning
tags:
  - IA
  - datascience
  - deep-learning
keywords:
source:
status: üåü
Parent: "[[Area-Prog]]"
cssclasses:
  - hide-embedded-header1
  - wide
categories:
  - Data Science
public_note: "true"
category: Data Science
---
# Deep Learning
``$= dv.current().file.tags.join(" ")``

- [Data Science](/uncategorized/data-science/)
- [Machine Learning](/data%20science/machine-learning/)
- [Regresi√≥n Lineal y M√≠nimos Cuadrados Ordinarios](/data%20science/regresi-n-lineal-y-m-nimos-cuadrados-ordinarios/)

## NLP (Natural Language Processing)
- Conceptos fundamentales: tokenizaci√≥n, embeddings, transformers
- Modelos populares: BERT, GPT, T5
- Aplicaciones: an√°lisis de sentimiento, traducci√≥n autom√°tica, chatbots
- Librer√≠as: Hugging Face Transformers, SpaCy, NLTK

## Computer Vision
- Tareas principales: clasificaci√≥n de im√°genes, detecci√≥n de objetos, segmentaci√≥n sem√°ntica
- Modelos populares: CNN, ResNet, YOLO, EfficientNet
- T√©cnicas: data augmentation, transfer learning, fine-tuning
- Librer√≠as: OpenCV, TensorFlow, PyTorch, Keras

## Plataformas y Herramientas
- Google Colab: entorno colaborativo en la nube para ejecutar notebooks de Python
- GitHub Codespaces: desarrollo remoto basado en contenedores con VS Code
- [Azure](/cloud/azure/): servicios en la nube para despliegue y entrenamiento de modelos de IA

## Proyectos y Aplicaciones
- Paridad humana: modelos para simular comportamiento humano o predecir resultados similares a los humanos
- Ejemplos: asistentes virtuales, sistemas de recomendaci√≥n, generaci√≥n de texto o imagen

## Bloques de C√≥digo

### Ejemplo de Regresi√≥n Lineal en Python
{% raw %}
```python
import numpy as np
from sklearn.linear_model import LinearRegression

# Datos de ejemplo
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

# Crear y entrenar modelo
model = LinearRegression()
model.fit(X, y)

# Predicci√≥n
pred = model.predict(np.array(6))
print(pred)
```
{% endraw %}`

### Ejemplo B√°sico de NLP con Hugging Face

{% raw %}
```python
from transformers import pipeline

# Crear pipeline de an√°lisis de sentimiento
classifier = pipeline("sentiment-analysis")

# Analizar texto
result = classifier("Me encanta aprender Deep Learning!")
print(result)
```
{% endraw %}

### Ejemplo de Computer Vision con PyTorch

{% raw %}
```python
import torch
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image

# Cargar modelo preentrenado
model = models.resnet18(pretrained=True)
model.eval()

# Preparar imagen
img = Image.open("imagen.jpg")
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor()
])
img_t = transform(img).unsqueeze(0)

# Predicci√≥n
out = model(img_t)
_, pred = torch.max(out, 1)
print(pred)
```
{% endraw %}

# Deep Learning Intro

## Procesamiento de Lenguaje Natural (PLN)
- Objetivo: que las m√°quinas comprendan, procesen y generen lenguaje humano.
- Aplicaciones: traducci√≥n autom√°tica, chatbots, an√°lisis de sentimiento, generaci√≥n de texto.

## Redes Neuronales Recurrentes (RNN)
- Dise√±adas para manejar secuencias de datos donde el orden importa.
- Capturan dependencias temporales entre palabras o elementos de la secuencia.
- Limitaciones:
	- Dificultad con secuencias muy largas (problema de desvanecimiento del gradiente).
	- Memoria limitada sobre estados anteriores.

## Transformers
- Manejan todas las palabras de la secuencia a la vez, eliminando dependencia estricta del orden temporal.
- Conceptos clave:
	- **Tokens**: divisiones de texto en unidades que el modelo procesa (palabras, subpalabras o caracteres).
	- **Embeddings**: vectores num√©ricos que representan tokens.
	- **Positional Encoding**: agrega informaci√≥n sobre la posici√≥n de cada token para mantener el orden dentro de la secuencia.
- Ventajas:
	- Mejor manejo de secuencias largas.
	- Paralelizaci√≥n m√°s eficiente durante el entrenamiento.
	- Facilita la captura de relaciones globales en el texto.

## Arquitectura Transformers
- Componentes principales:
	- Encoder: procesa la secuencia de entrada y genera representaciones contextuales.
	- Decoder: genera la secuencia de salida a partir de la informaci√≥n codificada.
- Funciona sumando embeddings de los tokens con su informaci√≥n posicional.
- Cambios de estado representados mediante vectores que capturan la atenci√≥n sobre toda la secuencia.
- Base de modelos modernos de PLN como GPT, BERT, T5.

## Tokens y API Keys
- **Tokens**: unidades m√≠nimas de texto que los transformers usan para procesar y generar lenguaje.
- **API Keys**: credenciales necesarias para interactuar con servicios de modelos de lenguaje en la nube.

# Qu√© es el Machine Learning y Deep Learning: Un mapa conceptual DotCSV
[¬øQu√© es el Machine Learning?¬øY Deep Learning? Un mapa conceptual | DotCSV](https://www.youtube.com/watch?v=KytW151dpqU)

## Tipos de Aprendizaje

### Supervisado
- Relaci√≥n entre variables de entrada y salida.
- Objetivo: aprender a predecir resultados a partir de muchos ejemplos.
- Ejemplos:
	- Regla matem√°tica: si la relaci√≥n existente es multiplicar por 2 ‚Üí la red aprende esa regla.
	- Clasificaci√≥n de correos electr√≥nicos como SPAM mediante patrones.
	- Predicci√≥n de depresi√≥n a partir de interacciones en Instagram.
- Concepto clave: aprender relaciones entre datos y generalizar a valores desconocidos.

### No supervisado
- No se proporciona informaci√≥n previa sobre la salida.
- Objetivo: reconocer patrones, estructuras o clusters en los datos.
- Ejemplos:
	- Agrupamiento de s√≠mbolos en lenguaje poligonal o lineal.
	- Identificaci√≥n de diferentes tipos de estructuras sin etiquetas.
- Problemas:
	- No hay respuesta conocida para evaluar el algoritmo.
	- Determinar cu√°ntas categor√≠as o lenguajes existen.
- Ventajas:
	- Datasets m√°s f√°ciles y baratos de obtener, ya que no requieren etiquetado manual.
- Conceptos:
	- Patrones de similitud en datos de entrada ‚Üí descubrimiento de estructura interna.
	- Reconocer caras, similitudes entre objetos, espacios latentes o estructuras conceptuales.
	- Permite analizar c√≥mo interact√∫a el sistema con la informaci√≥n de entrada y salida (Black Box).

### Reforzado
- Aprendizaje basado en recompensas y castigos seg√∫n las acciones del agente.
- Objetivo: maximizar la recompensa acumulada aprendiendo a interactuar con el entorno.
- Ejemplos: juegos, robots, optimizaci√≥n de procesos.

## Modelos de Machine Learning
- Representaci√≥n: mapas, ecuaciones, relaciones matem√°ticas o diagramas.
- Tipos de modelos:
	- Modelos probabil√≠sticos: describen incertidumbre y relaciones estad√≠sticas.
	- Modelos f√≠sicos o geom√©tricos: por ejemplo, modelos geoc√©ntricos para predecir posici√≥n de Marte.
- Componentes de un modelo:
	- Datos observacionales: puntos en un espacio multidimensional.
	- Par√°metros variables: ej. radios de √≥rbitas, pesos de una red neuronal.
	- Restricciones: ej. Tierra en el centro, √≥rbitas circulares.
- Objetivo del modelo:
	- Ajustar la representaci√≥n matem√°tica para minimizar el error entre predicciones y datos reales.
	- Construir modelos aproximados a la realidad.
	- Permitir predicciones futuras y comprender fen√≥menos.
- Manejo de dimensiones: los datos pueden ser multidimensionales, cada punto representa una instancia (persona, objeto, medici√≥n).

## Par√°metros y Funci√≥n de Error
- Par√°metros del modelo: valores que el algoritmo ajusta para mejorar su desempe√±o.
- Funci√≥n de error:
	- Mide la diferencia entre predicciones y valores reales.
	- Supervisado: se calcula a partir de los datos de salida suministrados.
	- No supervisado: se calcula a partir de la estructura de los datos de entrada.
- Se√±al de error:
	- Utilizada para reajustar par√°metros.
	- El proceso de optimizaci√≥n se denomina **entrenamiento** o **ajuste del modelo**.
- Consideraciones:
	- No siempre m√°s flexibilidad es mejor; puede causar sobreajuste.
	- El objetivo es encontrar algoritmos capaces de aprender los valores √≥ptimos de los par√°metros a partir de los datos.

## Conceptos clave
- Aprendizaje a partir de datos.
- Generalizaci√≥n a casos desconocidos.
- Estructuras internas y espacios latentes.
- Optimizaci√≥n mediante se√±al de error.
- Equilibrio entre flexibilidad del modelo y capacidad de generalizar.

# Deep Learning: Arquitectura y Fundamentos

- [Data Science](/uncategorized/data-science/)
- [Machine Learning](/data%20science/machine-learning/)
## Fundamentos de Deep Learning
- **Neuronas artificiales**: unidades b√°sicas que simulan el comportamiento de las neuronas biol√≥gicas.
- **Funciones de activaci√≥n**: introducen no linealidad en la red.
	- Ejemplos: ReLU, Sigmoid, Tanh, Softmax
- **Propagaci√≥n hacia adelante (Forward Propagation)**: c√°lculo de la salida de la red a partir de las entradas.
- **Funci√≥n de p√©rdida (Loss Function)**: mide el error entre la predicci√≥n y la salida real.
	- Ejemplos: MSE, Cross-Entropy, Huber Loss
- **Retropropagaci√≥n (Backpropagation)**: algoritmo para ajustar los pesos de la red usando gradientes.
- **Optimizaci√≥n**: m√©todos para minimizar la funci√≥n de p√©rdida.
	- Ejemplos: SGD, Adam, RMSProp
- **Regularizaci√≥n**: t√©cnicas para evitar sobreajuste.
	- Dropout, L1/L2, Data Augmentation
- **Batching**: procesar m√∫ltiples muestras a la vez para eficiencia computacional.
- **Epochs**: n√∫mero de pasadas completas por el dataset durante el entrenamiento.

## Arquitectura de Redes Neuronales
- **Redes Feedforward (FFNN)**:
	- Conexi√≥n unidireccional de la entrada a la salida.
	- Ideal para regresi√≥n y clasificaci√≥n b√°sica.
- **Redes Convolucionales (CNN)**:
	- Especializadas en procesamiento de im√°genes.
	- Capas: Convoluci√≥n, Pooling, Fully Connected.
- **Redes Recurrentes (RNN)**:
	- Dise√±adas para datos secuenciales (texto, series temporales).
	- Variantes: LSTM, GRU.
- **Transformers**:
	- Basadas en mecanismos de atenci√≥n.
	- Uso principal en NLP y cada vez m√°s en visi√≥n computacional.
	- Ejemplos: BERT, GPT, T5.
- **Redes Generativas (GANs)**:
	- Generan datos nuevos a partir de un conjunto de entrenamiento.
	- Componentes: Generador y Discriminador.
- **Redes Autoencoder**:
	- Aprendizaje no supervisado.
	- √ötiles para reducci√≥n de dimensionalidad y denoising.

## Consideraciones de Dise√±o
- N√∫mero de capas y neuronas por capa.
- Selecci√≥n de funci√≥n de activaci√≥n y p√©rdida.
- Elecci√≥n de optimizador y tasa de aprendizaje.
- Estrategias de regularizaci√≥n para mejorar generalizaci√≥n.
- Selecci√≥n de tama√±o de batch y n√∫mero de epochs.
- Preprocesamiento de datos y normalizaci√≥n.

## Plataformas y Herramientas
- Google Colab: ejecutar notebooks y entrenar modelos en la nube.
- GitHub Codespaces: desarrollo remoto de modelos.
- [Azure](/cloud/azure/): despliegue de modelos, entrenamiento a gran escala y servicios de IA.

## Ejemplo: Arquitectura Simple de Red Neuronal con Keras
{% raw %}
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Crear modelo
model = Sequential([
    Dense(64, activation='relu', input_shape=(100,)),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compilar modelo
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()
```
{% endraw %}`

## Ejemplo: Red Convolucional B√°sica

{% raw %}
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),
    MaxPooling2D((2,2)),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D((2,2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
```
{% endraw %}