---
date: 2024-02-19 03:15
title: GCP Big Data Developer y Apache Hadoop
tags:
status: üåü
Parent: "[[Area-IA]]"
keywords:
source:
cssclasses:
public_note: "true"
category: GCP
categories:
  - GCP
  - IA
  - hide-embedded-header1
  - Data Science
---
# GCP Big Data Developer y Apache Hadoop
`$= dv.current().file.tags.join(" ")`

- [apache](/backend/apache/)
- [GCP Google cloud](/data%20science/gcp-google-cloud/)
- [Data Science](/uncategorized/data-science/)
## Temario del curso
- Fundamentos de Big Data y Hadoop
- Ecosistema Apache Hadoop
- Apache Spark y procesamiento distribuido
- Ingesta, almacenamiento y formatos de datos
- Desarrollo de aplicaciones Big Data
- Orquestaci√≥n y flujos de trabajo
- Optimizaci√≥n, depuraci√≥n y buenas pr√°cticas
- Preparaci√≥n para certificaci√≥n Cloudera CCA
### Examen de certificaci√≥n
- **Cloudera Certified Associate Spark and Hadoop Developer (CCA)**
## Referencias y notas relacionadas
- Apache Spark vs. Hadoop Diferencias Clave y Cu√°ndo Usar Cada Uno

## Objetivo General
Preparar al alumno para analizar y resolver problemas del mundo real utilizando **Apache Hadoop** y las herramientas asociadas al centro de datos empresariales.

El curso cubre todo el ciclo de dise√±o y construcci√≥n de soluciones Big Data:
- Ingesta de datos desde m√∫ltiples fuentes
- Selecci√≥n del formato de archivo adecuado para almacenamiento
- Procesamiento eficiente de datos distribuidos
- Presentaci√≥n de resultados al usuario final de forma clara y accionable

Se va m√°s all√° de **MapReduce**, incorporando componentes avanzados del ecosistema Hadoop para desarrollar aplicaciones de **convergencia de datos**, altamente relevantes para el negocio y orientadas a la toma de decisiones.

## Competencias y conocimientos adquiridos
- Fundamentos del lenguaje **Java** aplicados al desarrollo Big Data
- Comprensi√≥n profunda de los componentes principales de Hadoop:
	- HDFS
	- MapReduce
	- YARN
- Desarrollo de aplicaciones utilizando la **API de Hadoop**
- Depuraci√≥n y optimizaci√≥n de programas MapReduce
- Introducci√≥n pr√°ctica a otros elementos clave del ecosistema Hadoop:
	- Hive
	- Pig
	- HBase
	- Flume
	- Oozie
- Uso de **Apache Spark** como motor de procesamiento distribuido en memoria
- Dise√±o de arquitecturas Big Data escalables y tolerantes a fallos

## Ecosistema Hadoop
### HDFS (Hadoop Distributed File System)
- Sistema de archivos distribuido y tolerante a fallos
- Optimizado para grandes vol√∫menes de datos
- Replicaci√≥n autom√°tica de bloques
- Acceso secuencial de alta eficiencia

### MapReduce
- Modelo de programaci√≥n distribuido
- Procesamiento por lotes
- Separaci√≥n en fases Map y Reduce
- Escalable horizontalmente

### YARN
- Gestor de recursos del cl√∫ster
- Permite ejecutar m√∫ltiples motores de procesamiento
- Base para la convivencia de MapReduce, Spark y otras herramientas

## Herramientas del ecosistema
### Apache Hive
- Lenguaje SQL-like (HiveQL)
- Ideal para an√°lisis y consultas sobre HDFS
- Uso frecuente en entornos de data warehousing

### Apache Pig
- Lenguaje de alto nivel (Pig Latin)
- Procesamiento de datos semiestructurados
- Enfoque procedural

### Apache HBase
- Base de datos NoSQL orientada a columnas
- Acceso aleatorio en tiempo casi real
- Integraci√≥n directa con HDFS

### Apache Flume
- Ingesta de datos en tiempo real
- Recolecci√≥n de logs y eventos
- Flujo de datos hacia HDFS o HBase

### Apache Oozie
- Orquestador de flujos de trabajo
- Programaci√≥n y coordinaci√≥n de jobs Hadoop
- Integraci√≥n con Hive, Pig y MapReduce

### Apache Spark
- Motor de procesamiento distribuido en memoria
- Mucho m√°s r√°pido que MapReduce para ciertos casos de uso
- Soporte para:
	- Batch processing
	- Streaming
	- Machine Learning
	- Graph processing
- APIs en Scala, Java y Python


## Estructura modular
### M√≥dulo 1: Fundamentos de Desarrollo de Aplicaciones Python
- Sintaxis b√°sica y avanzada de Python
- Estructuras de datos
- Programaci√≥n orientada a objetos
- Preparaci√≥n para el uso de Python en entornos Big Data

### M√≥dulo 2: Developer Training for Spark & Hadoop
- Arquitectura Hadoop
- Desarrollo con MapReduce
- Introducci√≥n y uso pr√°ctico de Apache Spark
- Integraci√≥n Spark + HDFS

### M√≥dulo 3: Designing and Building Big Data Applications
- Dise√±o de arquitecturas Big Data
- Selecci√≥n de herramientas adecuadas seg√∫n el caso de uso
- Buenas pr√°cticas de rendimiento y escalabilidad
- Casos reales de aplicaci√≥n empresarial

# modulos expandidos python y hadoop Big Data
## Preparaci√≥n para el uso de Python en entornos Big Data

### Rol de Python en Big Data
- Lenguaje principal para an√°lisis de datos, automatizaci√≥n y desarrollo r√°pido
- Amplio ecosistema de librer√≠as orientadas a datos y procesamiento distribuido
- Integraci√≥n directa con motores Big Data como Apache Spark y Hadoop
- Uso frecuente en pipelines de datos, prototipado y validaci√≥n de modelos

### Fundamentos de Python aplicados a datos
- Tipos de datos b√°sicos y avanzados
	- int, float, str, bool
	- list, tuple, set, dict
- Control de flujo
	- condicionales
	- bucles
	- comprensiones de listas y diccionarios
- Funciones
	- definici√≥n y reutilizaci√≥n
	- argumentos por defecto
	- *args y **kwargs
- Manejo de errores
	- try / except / finally
	- creaci√≥n de excepciones personalizadas

### Programaci√≥n orientada a objetos para Big Data
- Clases y objetos
- Encapsulaci√≥n y modularidad
- Herencia y composici√≥n
- Dise√±o de c√≥digo reutilizable para pipelines de datos
- Buenas pr√°cticas de organizaci√≥n de proyectos Python

### Manejo de archivos y formatos de datos
- Lectura y escritura de archivos
	- CSV
	- JSON
	- Parquet
	- Avro
- Serializaci√≥n y deserializaci√≥n de datos
- Procesamiento de grandes vol√∫menes sin cargar todo en memoria
- Uso de generadores e iteradores

### Librer√≠as clave de Python para Big Data
- NumPy
	- C√°lculo num√©rico
	- Operaciones vectorizadas
- Pandas
	- DataFrames
	- Limpieza y transformaci√≥n de datos
	- An√°lisis exploratorio
- PyArrow
	- Intercambio de datos columnar
	- Integraci√≥n con Parquet y sistemas distribuidos
- Dask
	- Paralelismo y escalado en Python
	- Procesamiento tipo Pandas distribuido

### Introducci√≥n a PySpark
- Qu√© es PySpark y por qu√© se utiliza
- Diferencias entre Spark con Scala y Spark con Python
- Arquitectura b√°sica de Spark
	- Driver
	- Executors
	- Cluster Manager
- Creaci√≥n de aplicaciones Spark con Python

### RDDs y DataFrames en PySpark
- RDDs
	- Transformations
	- Actions
- DataFrames
	- Esquemas
	- Operaciones SQL-like
- Comparaci√≥n RDD vs DataFrame
- Casos de uso recomendados para cada enfoque

### Integraci√≥n de Python con Hadoop
- Acceso a datos almacenados en HDFS
- Ejecuci√≥n de jobs Spark sobre Hadoop
- Uso de Python para preprocesamiento y postprocesamiento
- Comunicaci√≥n con Hive y HBase desde Python

### Buenas pr√°cticas de Python en entornos Big Data
- Escritura de c√≥digo eficiente
- Minimizar operaciones costosas
- Uso adecuado de memoria
- Logging y monitorizaci√≥n
- Testing de pipelines de datos
- Versionado de c√≥digo y entornos virtuales

### Preparaci√≥n para entornos productivos
- Estructuraci√≥n de proyectos Big Data en Python
- Configuraci√≥n de entornos virtuales
- Gesti√≥n de dependencias
- Ejecuci√≥n en cl√∫ster frente a ejecuci√≥n local
- Adaptaci√≥n del c√≥digo a entornos distribuidos

## Preparaci√≥n para el uso de Python en entornos Big Data ‚Äî Ejemplos de c√≥digo

### Manejo eficiente de grandes vol√∫menes de datos en Python
#### Lectura de archivos grandes usando iteradores
{% raw %}
```python
def read_large_file(path):
	with open(path, "r") as f:
		for line in f:
			yield line

for row in read_large_file("datos_grandes.csv"):
	process(row)
```
{% endraw %}`

### Uso de generadores para optimizar memoria

#### Generadores aplicados a pipelines de datos

{% raw %}
```python
def clean_data(rows):
	for r in rows:
		yield r.strip().lower()

data = clean_data(read_large_file("input.txt"))
for row in data:
	print(row)
```
{% endraw %}

### Programaci√≥n orientada a objetos aplicada a Big Data

#### Clase para un pipeline de procesamiento

{% raw %}
```python
class DataPipeline:
	def __init__(self, source):
		self.source = source

	def extract(self):
		for row in self.source:
			yield row

	def transform(self, rows):
		for r in rows:
			yield r.upper()

	def load(self, rows):
		for r in rows:
			print(r)

pipeline = DataPipeline(read_large_file("data.txt"))
pipeline.load(pipeline.transform(pipeline.extract()))
```
{% endraw %}

### Manejo de formatos comunes en Big Data

#### Lectura y escritura de CSV con Pandas

{% raw %}
```python
import pandas as pd

df = pd.read_csv("data.csv")
df["total"] = df["price"] * df["quantity"]
df.to_csv("output.csv", index=False)
```
{% endraw %}

#### Lectura y escritura de Parquet

{% raw %}
```python
df = pd.read_parquet("data.parquet")
df.to_parquet("output.parquet")
```
{% endraw %}

### Uso de NumPy para c√°lculo eficiente

#### Operaciones vectorizadas

{% raw %}
```python
import numpy as np

data = np.array([10, 20, 30, 40])
result = data * 1.21
print(result)
```
{% endraw %}

### Procesamiento distribuido con Dask

#### DataFrame distribuido tipo Pandas

{% raw %}
```python
import dask.dataframe as dd

df = dd.read_csv("large_*.csv")
df_filtered = df[df["sales"] > 1000]
df_filtered.compute()
```
{% endraw %}

### Introducci√≥n a PySpark

#### Creaci√≥n de una sesi√≥n Spark

{% raw %}
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
	.appName("BigDataApp") \
	.getOrCreate()
```
{% endraw %}

### RDDs en PySpark

#### Transformations y Actions

{% raw %}
```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
rdd_squared = rdd.map(lambda x: x * x)
print(rdd_squared.collect())
```
{% endraw %}

### DataFrames en PySpark

#### Creaci√≥n y operaciones b√°sicas

{% raw %}
```python
data = [("Ana", 34), ("Luis", 45), ("Eva", 29)]
df = spark.createDataFrame(data, ["name", "age"])
df.filter(df.age > 30).show()
```
{% endraw %}

### Operaciones SQL-like en Spark

#### Uso de Spark SQL

{% raw %}
```python
df.createOrReplaceTempView("people")
spark.sql("SELECT name FROM people WHERE age > 30").show()
```
{% endraw %}

### Lectura de datos desde HDFS con PySpark

#### Acceso a archivos distribuidos

{% raw %}
```python
df = spark.read.csv("hdfs:///data/input.csv", header=True)
df.show()
```
{% endraw %}

### Integraci√≥n de Python con Hive

#### Lectura de tablas Hive desde Spark

{% raw %}
```python
spark.sql("SELECT * FROM sales_db.transactions").show()
```
{% endraw %}

### Buenas pr√°cticas de logging

#### Logging en pipelines Big Data

{% raw %}
```python
import logging

logging.basicConfig(level=logging.INFO)
logging.info("Inicio del procesamiento de datos")
```
{% endraw %}

### Testing b√°sico de funciones de datos

#### Prueba unitaria simple

{% raw %}
```python
def transform(x):
	return x * 2

def test_transform():
	assert transform(3) == 6
```
{% endraw %}

### Estructuraci√≥n de proyectos Big Data en Python

#### Estructura t√≠pica

{% raw %}
```text
project/
‚îú‚îÄ‚îÄ src/
‚îÇ	‚îú‚îÄ‚îÄ extract.py
‚îÇ	‚îú‚îÄ‚îÄ transform.py
‚îÇ	‚îî‚îÄ‚îÄ load.py
‚îú‚îÄ‚îÄ tests/
‚îÇ	‚îî‚îÄ‚îÄ test_transform.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ main.py
```
{% endraw %}

### Ejecuci√≥n local vs cl√∫ster

#### Env√≠o de una aplicaci√≥n PySpark a cl√∫ster

{% raw %}
```bash
spark-submit --master yarn app.py
```
{% endraw %}

# Developer Training for Spark & Hadoop ‚Äî Arquitectura Hadoop

## Temario
- Visi√≥n general de Hadoop
- Principios de dise√±o
- Componentes principales
- Arquitectura HDFS
- Arquitectura YARN
- Arquitectura MapReduce
- Alta disponibilidad y tolerancia a fallos
- Escalabilidad y rendimiento
- Seguridad en Hadoop
- Despliegues y modos de operaci√≥n

## Visi√≥n general de Hadoop
- Framework de **procesamiento distribuido** orientado a Big Data
- Dise√±ado para:
	- Grandes vol√∫menes de datos
	- Hardware commodity
	- Tolerancia a fallos
- Separaci√≥n clara entre:
	- Almacenamiento
	- Gesti√≥n de recursos
	- Procesamiento

## Principios de dise√±o
- **Escalabilidad horizontal**
	- A√±adir nodos en lugar de aumentar potencia
- **Tolerancia a fallos**
	- Replicaci√≥n de datos
	- Reintentos autom√°ticos de tareas
- **Procesamiento cerca de los datos**
	- Minimiza tr√°fico de red
- **Modelo batch**
	- Optimizado para grandes vol√∫menes, no baja latencia

## Componentes principales de Hadoop
- HDFS
- YARN
- MapReduce
- Hadoop Common

## Hadoop Common
- Librer√≠as y utilidades compartidas
- Configuraci√≥n
- Gesti√≥n de E/S
- Base para el resto del ecosistema

## Arquitectura HDFS
### Componentes de HDFS
- NameNode
	- Gestiona metadatos
	- Mantiene el namespace del sistema de archivos
	- Controla permisos y localizaci√≥n de bloques
- DataNode
	- Almacena bloques de datos
	- Ejecuta operaciones de lectura y escritura
- Secondary NameNode
	- Checkpoints del estado del NameNode
	- No es un backup activo

### Bloques y replicaci√≥n
- Tama√±o de bloque grande (128 MB o superior)
- Replicaci√≥n por defecto: 3
- Distribuci√≥n autom√°tica entre DataNodes
- Re-replicaci√≥n ante fallos

### Flujo de lectura y escritura
- Cliente consulta al NameNode
- Acceso directo a DataNodes
- Escritura en pipeline
- Confirmaciones encadenadas

## Arquitectura YARN
### Rol de YARN
- Gestor de recursos del cl√∫ster
- Permite m√∫ltiples frameworks de procesamiento
- Base para Spark, MapReduce, Tez, Flink

### Componentes de YARN
- ResourceManager
	- Gesti√≥n global de recursos
	- Planificaci√≥n de aplicaciones
- NodeManager
	- Gesti√≥n de recursos por nodo
	- Monitorizaci√≥n de contenedores
- ApplicationMaster
	- Gesti√≥n del ciclo de vida de cada aplicaci√≥n
	- Negociaci√≥n de recursos con ResourceManager

### Contenedores
- Unidad b√°sica de asignaci√≥n de recursos
- Define:
	- Memoria
	- CPU
- Aislados por aplicaci√≥n

## Arquitectura MapReduce
### Modelo de ejecuci√≥n
- Job
	- Unidad completa de procesamiento
- Task
	- Map Task
	- Reduce Task

### Fases de MapReduce
- Input Split
- Map
- Shuffle & Sort
- Reduce
- Output

### Caracter√≠sticas clave
- Procesamiento batch
- Escritura intermedia a disco
- Alta fiabilidad
- Menor rendimiento frente a motores en memoria

## Alta disponibilidad y tolerancia a fallos
### HDFS HA
- NameNode activo y en standby
- JournalNodes para sincronizaci√≥n
- Failover autom√°tico

### YARN HA
- ResourceManager activo/standby
- Recuperaci√≥n de aplicaciones en ejecuci√≥n

### Manejo de fallos
- Reejecuci√≥n autom√°tica de tareas
- Detecci√≥n de nodos ca√≠dos
- Redistribuci√≥n de bloques

## Escalabilidad y rendimiento
- Escalado a√±adiendo nodos
- Balanceo autom√°tico de datos
- Optimizaci√≥n mediante:
	- Tama√±o de bloque
	- Compresi√≥n
	- Formatos columnar (Parquet, ORC)
	- Ajuste de memoria y CPU en YARN

## Seguridad en Hadoop
- Autenticaci√≥n mediante Kerberos
- Autorizaci√≥n:
	- Permisos HDFS
	- ACLs
- Cifrado:
	- Datos en reposo
	- Datos en tr√°nsito
- Auditor√≠a y logs

## Modos de despliegue
- Local Mode
	- Desarrollo y pruebas
- Pseudo-distributed Mode
	- Todos los servicios en una sola m√°quina
- Fully Distributed Mode
	- Cl√∫ster real
	- Producci√≥n


# Developer Training for Spark & Hadoop ‚Äî Desarrollo con MapReduce [java](/software%20engineering/java/)

## Temario
- Introducci√≥n a MapReduce
- Modelo de programaci√≥n MapReduce
- Flujo de ejecuci√≥n de un Job
- Componentes clave
- Desarrollo de MapReduce con Java
- Tipos de datos y formatos de entrada/salida
- Shuffle y Sort
- Partitioner y Combiner
- Optimizaci√≥n de Jobs MapReduce
- Casos de uso comunes
- Limitaciones de MapReduce

## Introducci√≥n a MapReduce
- Modelo de programaci√≥n distribuido incluido en Hadoop
- Orientado a **procesamiento batch**
- Dise√±ado para grandes vol√∫menes de datos
- Basado en dos fases principales:
	- Map
	- Reduce
- Alta tolerancia a fallos y escalabilidad

## Modelo de programaci√≥n MapReduce
### Concepto clave
- Procesamiento de datos como pares **clave‚Äìvalor**
- Cada fase transforma pares clave‚Äìvalor en otros nuevos

### Funciones principales
- Mapper
	- Procesa datos de entrada
	- Genera pares intermedios
- Reducer
	- Agrega y procesa datos intermedios
	- Produce el resultado final

## Flujo de ejecuci√≥n de un Job MapReduce
- Input Data
- InputFormat
- Input Split
- Mapper
- Shuffle & Sort
- Reducer
- OutputFormat
- Output Data

## Componentes clave
### Job
- Representa una ejecuci√≥n completa de MapReduce
- Define:
	- Clases Mapper y Reducer
	- Tipos de clave y valor
	- Formatos de entrada y salida

### Task
- Unidad m√≠nima de ejecuci√≥n
- Tipos:
	- Map Task
	- Reduce Task

## Desarrollo de MapReduce con Java
### Estructura b√°sica de un Job
- Driver
	- Configuraci√≥n y lanzamiento del Job
- Mapper
	- L√≥gica de transformaci√≥n
- Reducer
	- L√≥gica de agregaci√≥n

### Mapper
{% raw %}
```java
public class WordCountMapper
	extends Mapper<LongWritable, Text, Text, IntWritable> {

	private final static IntWritable one = new IntWritable(1);
	private Text word = new Text();

	@Override
	protected void map(LongWritable key, Text value, Context context)
		throws IOException, InterruptedException {

		String[] tokens = value.toString().split(" ");
		for (String token : tokens) {
			word.set(token);
			context.write(word, one);
		}
	}
}
```
{% endraw %}`

### Reducer

{% raw %}
```java
public class WordCountReducer
	extends Reducer<Text, IntWritable, Text, IntWritable> {

	@Override
	protected void reduce(Text key, Iterable<IntWritable> values, Context context)
		throws IOException, InterruptedException {

		int sum = 0;
		for (IntWritable val : values) {
			sum += val.get();
		}
		context.write(key, new IntWritable(sum));
	}
}
```
{% endraw %}

### Driver

{% raw %}
```java
public class WordCountDriver {

	public static void main(String[] args) throws Exception {

		Configuration conf = new Configuration();
		Job job = Job.getInstance(conf, "word count");

		job.setJarByClass(WordCountDriver.class);
		job.setMapperClass(WordCountMapper.class);
		job.setReducerClass(WordCountReducer.class);

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);

		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));

		System.exit(job.waitForCompletion(true) ? 0 : 1);
	}
}
```
{% endraw %}

## Tipos de datos en MapReduce

- Writable
    - IntWritable
    - LongWritable
    - Text
    - DoubleWritable
- Serializaci√≥n eficiente para entorno distribuido

## Formatos de entrada y salida

### InputFormat

- TextInputFormat
- KeyValueTextInputFormat
- SequenceFileInputFormat

### OutputFormat
- TextOutputFormat
- SequenceFileOutputFormat

## Shuffle y Sort

- Fase intermedia autom√°tica
- Redistribuci√≥n de datos entre nodos
- Agrupaci√≥n por clave
- Ordenaci√≥n previa a Reduce
- Impacto cr√≠tico en el rendimiento
    

## Partitioner

### Funci√≥n
- Determina a qu√© Reducer va cada clave
- Por defecto: HashPartitioner

### Uso

- Controlar distribuci√≥n de carga
- Evitar skew de datos

## Combiner

### Prop√≥sito

- Reducer local
- Reduce volumen de datos transferidos
- Optimizaci√≥n opcional

### Requisitos

- Operaci√≥n asociativa y conmutativa

## Optimizaci√≥n de Jobs MapReduce

- Uso de Combiner
- Ajuste del n√∫mero de Reducers
- Compresi√≥n de datos intermedios
- Uso de formatos eficientes (SequenceFile, Avro)
- Minimizar datos en Shuffle
- Evitar l√≥gica pesada en Reducer

## Casos de uso comunes

- Conteo y agregaciones masivas
- Procesamiento de logs
- ETL batch
- An√°lisis hist√≥rico de datos
- Procesos nocturnos de gran volumen

## Limitaciones de MapReduce

- Alta latencia
- Escritura frecuente a disco
- Poco eficiente para:
    - Procesos iterativos
    - An√°lisis interactivo
    - Streaming
- Motivo principal de la adopci√≥n de Apache Spark

# Developer Training for Spark & Hadoop ‚Äî Desarrollo de MapReduce con Python, PySpark

## Temario
- MapReduce con Python en Hadoop
- Hadoop Streaming
- Arquitectura de Hadoop Streaming
- Desarrollo de Mapper en Python
- Desarrollo de Reducer en Python
- Tipos de entrada y salida
- Ejecuci√≥n de Jobs MapReduce con Python
- Debugging y pruebas locales
- Buenas pr√°cticas y limitaciones

## MapReduce con Python en Hadoop
- Hadoop permite usar lenguajes distintos a Java mediante **Hadoop Streaming**
- Hadoop Streaming:
	- Ejecuta programas externos como Mapper y Reducer
	- Comunicaci√≥n v√≠a **STDIN / STDOUT**
- Python es ideal para:
	- Prototipado r√°pido
	- Procesamiento de texto
	- ETL batch sencillo

## Hadoop Streaming
### Qu√© es
- Utilidad incluida en Hadoop
- Permite usar cualquier lenguaje que lea de entrada est√°ndar y escriba a salida est√°ndar
- Hadoop se encarga de:
	- Distribuci√≥n
	- Shuffle & Sort
	- Gesti√≥n de fallos

### Flujo de ejecuci√≥n
- Input desde HDFS
- Hadoop lanza el Mapper Python
- Salida del Mapper ‚Üí Shuffle & Sort
- Entrada ordenada al Reducer Python
- Salida final a HDFS

## Arquitectura de Hadoop Streaming
- Mapper
	- Script Python
	- Procesa l√≠nea a l√≠nea
- Reducer
	- Script Python
	- Recibe datos ordenados por clave
- Driver
	- Comando `hadoop jar`

## Desarrollo de Mapper en Python
### Mapper b√°sico (WordCount)
{% raw %}
```python
#!/usr/bin/env python3
import sys

for line in sys.stdin:
	line = line.strip()
	words = line.split()
	for word in words:
		print(f"{word}\t1")
```
{% endraw %}`

## Desarrollo de Reducer en Python

### Reducer b√°sico (WordCount)

{% raw %}
```python
#!/usr/bin/env python3
import sys

current_word = None
current_count = 0

for line in sys.stdin:
	word, count = line.strip().split("\t")
	count = int(count)

	if current_word == word:
		current_count += count
	else:
		if current_word:
			print(f"{current_word}\t{current_count}")
		current_word = word
		current_count = count

if current_word:
	print(f"{current_word}\t{current_count}")
```
{% endraw %}

## Formato de entrada y salida

* Entrada:

  * Texto plano
  * Una l√≠nea por registro
* Salida:

  * Formato clave‚Äìvalor
  * Separador por defecto: tabulaci√≥n (`\t`)
* Hadoop se encarga de ordenar por clave antes del Reducer

## Ejecuci√≥n de un Job MapReduce con Python

### Subida de datos a HDFS

{% raw %}
```bash
hdfs dfs -put input.txt /user/hadoop/input
```
{% endraw %}

### Ejecuci√≥n del Job

{% raw %}
```bash
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
	-input /user/hadoop/input \
	-output /user/hadoop/output \
	-mapper mapper.py \
	-reducer reducer.py \
	-file mapper.py \
	-file reducer.py
```
{% endraw %}

## Visualizaci√≥n del resultado

### Lectura de resultados desde HDFS

{% raw %}
```bash
hdfs dfs -cat /user/hadoop/output/part-00000
```
{% endraw %}

## Debugging y pruebas locales

### Prueba local del Mapper

{% raw %}
```bash
cat input.txt | python3 mapper.py
```
{% endraw %}

### Prueba local del flujo completo

{% raw %}
```bash
cat input.txt | python3 mapper.py | sort | python3 reducer.py
```
{% endraw %}

## Casos de uso comunes

* Conteo de palabras
* Procesamiento de logs
* Agregaciones simples
* Limpieza y normalizaci√≥n de texto
* ETL batch sencillo

## Buenas pr√°cticas en MapReduce con Python

* Mantener l√≥gica simple
* Evitar operaciones costosas en Reducer
* Usar Combiner cuando sea posible
* Validar entrada y salida
* Manejar errores de parsing
* Usar `#!/usr/bin/env python3` y permisos de ejecuci√≥n

## Limitaciones de MapReduce con Python

* Menor rendimiento que Java
* Overhead de procesos externos
* Dif√≠cil manejo de tipos complejos
* No recomendado para:

  * Procesos iterativos
  * Machine Learning
  * An√°lisis interactivo

## Comparaci√≥n con alternativas

* MapReduce con Python:
	* Simple
	* Batch
	* Bajo rendimiento
* PySpark:
	* Procesamiento en memoria
	* APIs de alto nivel
	* Mejor opci√≥n para la mayor√≠a de casos modernos

# Developer Training for Spark & Hadoop ‚Äî Introducci√≥n y uso pr√°ctico de Apache Spark

## Temario
- Qu√© es Apache Spark
- Motivaci√≥n y comparaci√≥n con MapReduce
- Arquitectura de Spark
- Componentes principales
- Modos de despliegue
- APIs y lenguajes soportados
- Conceptos fundamentales
- Uso pr√°ctico con ejemplos
- Integraci√≥n con Hadoop
- Casos de uso comunes
- Buenas pr√°cticas

## Qu√© es Apache Spark
- Motor de procesamiento distribuido
- Dise√±ado para:
	- Alto rendimiento
	- Procesamiento en memoria
	- Baja latencia
- Parte del ecosistema Big Data moderno
- Compatible con Hadoop

## Motivaci√≥n y comparaci√≥n con MapReduce
- Evita escritura frecuente a disco
- Procesamiento en memoria
- Ideal para:
	- Procesos iterativos
	- An√°lisis interactivo
	- Machine Learning
- MapReduce:
	- Batch
	- M√°s lento
- Spark:
	- Batch + Streaming + ML + Graph

## Arquitectura de Spark
### Componentes principales
- Driver
	- Punto de entrada de la aplicaci√≥n
	- Coordina ejecuci√≥n
- Cluster Manager
	- YARN
	- Standalone
	- Mesos
	- Kubernetes
- Executors
	- Ejecutan tareas
	- Mantienen datos en memoria

### Flujo de ejecuci√≥n
- Driver crea el DAG
- Planificaci√≥n de tareas
- Ejecuci√≥n distribuida
- Recolecci√≥n de resultados

## Componentes del ecosistema Spark
- Spark Core
- Spark SQL
- Spark Streaming
- MLlib
- GraphX

## Modos de despliegue
- Local Mode
- Standalone Mode
- YARN Mode
- Kubernetes Mode

## APIs y lenguajes soportados
- Scala
- Java
- Python (PySpark)
- R

## Conceptos fundamentales
### RDD
- Resilient Distributed Dataset
- Inmutable
- Distribuido
- Tolerante a fallos

### DataFrame
- Estructura tabular distribuida
- Esquema definido
- Optimizaci√≥n autom√°tica (Catalyst)

### Dataset
- Tipado fuerte (Scala/Java)
- Combina RDD y DataFrame

### Transformations y Actions
- Transformations
	- map
	- filter
	- flatMap
- Actions
	- collect
	- count
	- save

### Lazy Evaluation
- Las transformaciones no se ejecutan hasta una acci√≥n

## Uso pr√°ctico de Apache Spark
### Creaci√≥n de una SparkSession
{% raw %}
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
	.appName("SparkIntro") \
	.getOrCreate()
```
{% endraw %}`

### Lectura de datos desde HDFS

{% raw %}
```python
df = spark.read.csv("hdfs:///data/input.csv", header=True)
df.show()
```
{% endraw %}

### Transformaciones b√°sicas

{% raw %}
```python
df_filtered = df.filter(df["age"] > 30)
df_filtered.select("name", "age").show()
```
{% endraw %}

### Agregaciones

{% raw %}
```python
df.groupBy("country").count().show()
```
{% endraw %}

### Uso de Spark SQL

{% raw %}
```python
df.createOrReplaceTempView("people")
spark.sql("SELECT country, COUNT(*) FROM people GROUP BY country").show()
```
{% endraw %}

### Escritura de resultados

{% raw %}
```python
df_filtered.write.parquet("hdfs:///data/output")
```
{% endraw %}

## Integraci√≥n con Hadoop

* Lectura y escritura directa sobre HDFS
* Ejecuci√≥n sobre YARN
* Convivencia con MapReduce
* Uso de Hive Metastore

## Casos de uso comunes

* ETL distribuido
* Procesamiento de logs
* An√°lisis interactivo
* Machine Learning
* Streaming de datos

## Buenas pr√°cticas

* Preferir DataFrames frente a RDDs
* Persistir solo cuando sea necesario
* Evitar collect() en grandes vol√∫menes
* Ajustar recursos de ejecuci√≥n
* Usar formatos columnar
* Monitorear con Spark UI

# Developer Training for Spark & Hadoop ‚Äî Integraci√≥n Spark + HDFS

## Temario
- Relaci√≥n entre Spark y HDFS
- Arquitectura de integraci√≥n
- Acceso a datos en HDFS desde Spark
- Formatos de archivo soportados
- Lectura y escritura distribuida
- Persistencia y cacheo
- Ejecuci√≥n de Spark sobre YARN
- Gesti√≥n de permisos y seguridad
- Rendimiento y optimizaci√≥n
- Casos de uso habituales

## Relaci√≥n entre Spark y HDFS
- HDFS act√∫a como:
	- Sistema de almacenamiento distribuido
	- Fuente y destino de datos
- Spark act√∫a como:
	- Motor de procesamiento distribuido
	- No sustituye a HDFS
- Separaci√≥n clara:
	- HDFS ‚Üí almacenamiento
	- Spark ‚Üí c√≥mputo

## Arquitectura de integraci√≥n
- Spark se ejecuta:
	- Sobre YARN
	- En modo Standalone
	- En Kubernetes
- Acceso a HDFS mediante:
	- Hadoop FileSystem API
	- Configuraci√≥n heredada de Hadoop
- Data locality:
	- Spark intenta ejecutar tareas cerca de los bloques HDFS

## Acceso a datos en HDFS desde Spark
### Requisitos
- Hadoop configurado correctamente
- Variables de entorno:
	- HADOOP_CONF_DIR
	- YARN_CONF_DIR
- Permisos adecuados en HDFS

## Lectura de datos desde HDFS
### Lectura de CSV
{% raw %}
```python
df = spark.read \
	.option("header", "true") \
	.csv("hdfs:///data/input.csv")

df.show()
```
{% endraw %}`

### Lectura de Parquet

{% raw %}
```python
df = spark.read.parquet("hdfs:///data/input_parquet")
df.printSchema()
```
{% endraw %}

### Lectura de JSON

{% raw %}
```python
df = spark.read.json("hdfs:///data/input.json")
df.show()
```
{% endraw %}

## Escritura de datos en HDFS

### Escritura en CSV

{% raw %}
```python
df.write \
	.mode("overwrite") \
	.option("header", "true") \
	.csv("hdfs:///data/output_csv")
```
{% endraw %}

### Escritura en Parquet

{% raw %}
```python
df.write \
	.mode("overwrite") \
	.parquet("hdfs:///data/output_parquet")
```
{% endraw %}

## Formatos de archivo soportados

* Texto plano
* CSV
* JSON
* Parquet
* ORC
* Avro
* SequenceFile

### Recomendaciones

* Usar formatos columnares:
	* Parquet
	* ORC
* Mejor compresi√≥n
* Mejor rendimiento en consultas

## Persistencia y cacheo

### Cacheo de DataFrames

{% raw %}
```python
df.cache()
df.count()
```
{% endraw %}

### Persistencia con nivel de almacenamiento

{% raw %}
```python
from pyspark import StorageLevel

df.persist(StorageLevel.MEMORY_AND_DISK)
```
{% endraw %}

## Ejecuci√≥n de Spark sobre YARN

### Env√≠o de aplicaci√≥n Spark

{% raw %}
```bash
spark-submit \
	--master yarn \
	--deploy-mode cluster \
	app.py
```
{% endraw %}

### Beneficios

* Uso compartido de recursos
* Integraci√≥n con ecosistema Hadoop
* Escalabilidad y alta disponibilidad

## Gesti√≥n de permisos y seguridad

* Permisos HDFS:
	* Lectura
	* Escritura
	* Ejecuci√≥n
* Integraci√≥n con Kerberos
* Spark hereda identidad del usuario
* Control de acceso a datos sensibles

## Rendimiento y optimizaci√≥n

* Ajustar:
	* N√∫mero de particiones
	* Tama√±o de bloques HDFS
* Evitar:
	* Archivos peque√±os
	* collect() en grandes vol√∫menes
* Usar:
	* Parquet con compresi√≥n
	* Predicate pushdown
	* Partition pruning

## Casos de uso habituales
* ETL distribuido sobre HDFS
* Reemplazo de jobs MapReduce
* An√°lisis interactivo de datos hist√≥ricos
* Preparaci√≥n de datos para Machine Learning
* Procesamiento batch a gran escala

# Designing and Building Big Data Applications ‚Äî Dise√±o de arquitecturas Big Data

## Temario
- Introducci√≥n al dise√±o de arquitecturas Big Data
- Principios de arquitectura Big Data
- Tipos de arquitecturas Big Data
- Capas de una arquitectura Big Data
- Arquitecturas batch
- Arquitecturas streaming
- Arquitecturas h√≠bridas
- Selecci√≥n de herramientas
- Escalabilidad y tolerancia a fallos
- Seguridad y gobierno del dato
- Buenas pr√°cticas de dise√±o

## Introducci√≥n al dise√±o de arquitecturas Big Data
- El dise√±o de arquitecturas Big Data busca:
	- Procesar grandes vol√∫menes de datos
	- Gestionar variedad y velocidad
	- Garantizar fiabilidad y escalabilidad
- No existe una arquitectura √∫nica
- El dise√±o depende del:
	- Caso de uso
	- Tipo de datos
	- Requisitos de negocio

## Principios de arquitectura Big Data
- Escalabilidad horizontal
- Tolerancia a fallos
- Separaci√≥n de responsabilidades
- Procesamiento distribuido
- Observabilidad y monitorizaci√≥n
- Automatizaci√≥n

## Tipos de arquitecturas Big Data
### Arquitectura Lambda
- Capa batch
- Capa speed (streaming)
- Capa serving
- Alta complejidad
- Consistencia eventual

### Arquitectura Kappa
- Streaming como fuente principal
- Simplificaci√≥n respecto a Lambda
- Reprocesamiento mediante replay
- Menor complejidad operativa

### Arquitectura Batch tradicional
- Procesamiento peri√≥dico
- Orientada a hist√≥ricos
- Alta latencia

### Arquitectura Streaming
- Procesamiento en tiempo real
- Baja latencia
- Enfoque en eventos

## Capas de una arquitectura Big Data
### Capa de ingesta
- Captura de datos desde m√∫ltiples fuentes
- Batch o streaming
- Herramientas comunes:
	- Kafka
	- Flume
	- Sqoop
	- APIs

### Capa de almacenamiento
- Almacenamiento distribuido
- Persistencia a largo plazo
- Herramientas comunes:
	- HDFS
	- Object Storage
	- HBase

### Capa de procesamiento
- Transformaci√≥n y an√°lisis de datos
- Batch y/o streaming
- Herramientas comunes:
	- Spark
	- MapReduce
	- Flink

### Capa de serving
- Exposici√≥n de datos procesados
- Consultas y consumo
- Herramientas comunes:
	- Hive
	- Presto
	- Bases de datos anal√≠ticas

### Capa de visualizaci√≥n
- Dashboards y reporting
- Herramientas BI
- Consumo por usuario final

## Arquitecturas batch
- Procesamiento peri√≥dico
- Jobs programados
- Alta fiabilidad
- Ejemplos:
	- ETL nocturno
	- An√°lisis hist√≥rico

## Arquitecturas streaming
- Procesamiento continuo
- Eventos en tiempo real
- Baja latencia
- Ejemplos:
	- Monitorizaci√≥n
	- Detecci√≥n de fraude
	- IoT

## Arquitecturas h√≠bridas
- Combinan batch y streaming
- Casos de uso complejos
- Balance entre latencia y consistencia

## Selecci√≥n de herramientas
- Volumen de datos
- Latencia requerida
- Complejidad operativa
- Coste
- Ecosistema existente

## Escalabilidad y tolerancia a fallos
- Replicaci√≥n de datos
- Reintentos autom√°ticos
- Balanceo de carga
- Dise√±o sin single points of failure

## Seguridad y gobierno del dato
- Autenticaci√≥n
- Autorizaci√≥n
- Cifrado
- Auditor√≠a
- Calidad del dato
- Cat√°logo de datos

## Buenas pr√°cticas de dise√±o
- Dise√±ar para el fallo
- Automatizar despliegues
- Usar formatos eficientes
- Documentar arquitectura
- Monitorizar desde el inicio
- Revisar peri√≥dicamente la arquitectura

# Designing and Building Big Data Applications ‚Äî Selecci√≥n de herramientas adecuadas seg√∫n el caso de uso

## Temario
- Importancia de la selecci√≥n de herramientas
- Factores clave de decisi√≥n
- Herramientas por capa de arquitectura
- Casos de uso batch
- Casos de uso streaming
- Casos de uso anal√≠ticos
- Casos de uso operacionales
- Comparativas habituales
- Anti-patrones comunes
- Buenas pr√°cticas de selecci√≥n

## Importancia de la selecci√≥n de herramientas
- Una mala elecci√≥n impacta en:
	- Rendimiento
	- Coste
	- Mantenibilidad
- No existe una herramienta universal
- La herramienta debe adaptarse al caso de uso
- Priorizar simplicidad y alineaci√≥n con negocio

## Factores clave de decisi√≥n
- Volumen de datos
- Velocidad de ingesti√≥n
- Latencia requerida
- Tipo de datos
	- Estructurados
	- Semiestructurados
	- No estructurados
- Frecuencia de procesamiento
- Complejidad operativa
- Escalabilidad
- Ecosistema existente
- Coste

## Herramientas por capa de arquitectura
### Ingesta
- Batch
	- Sqoop
	- Scripts Python
- Streaming
	- Kafka
	- Flume
	- APIs

### Almacenamiento
- HDFS
- Object Storage
- HBase
- Data Lake

### Procesamiento
- Batch
	- Spark
	- MapReduce
- Streaming
	- Spark Structured Streaming
	- Flink

### Consulta y serving
- Hive
- Presto / Trino
- Impala
- Bases anal√≠ticas

### Orquestaci√≥n
- Oozie
- Airflow

## Casos de uso batch
### Caracter√≠sticas
- Alta latencia aceptable
- Grandes vol√∫menes
- Procesamiento peri√≥dico

### Herramientas recomendadas
- HDFS
- Spark
- Hive
- Oozie

### Ejemplos
- ETL nocturno
- An√°lisis hist√≥rico
- Reprocesamiento completo de datos

## Casos de uso streaming
### Caracter√≠sticas
- Baja latencia
- Eventos continuos
- Procesamiento en tiempo real

### Herramientas recomendadas
- Kafka
- Spark Structured Streaming
- Flink
- HBase

### Ejemplos
- Detecci√≥n de fraude
- Monitorizaci√≥n en tiempo real
- IoT

## Casos de uso anal√≠ticos
### Caracter√≠sticas
- Consultas interactivas
- Grandes datasets
- Agregaciones complejas

### Herramientas recomendadas
- Parquet / ORC
- Hive
- Presto / Trino
- Spark SQL

### Ejemplos
- BI
- Reporting
- Data exploration

## Casos de uso operacionales
### Caracter√≠sticas
- Acceso casi en tiempo real
- Baja latencia de lectura
- Consultas por clave

### Herramientas recomendadas
- HBase
- Cassandra
- Redis

### Ejemplos
- Perfiles de usuario
- Recomendaciones
- Sistemas de scoring

## Comparativas habituales
### Spark vs MapReduce
- Spark
	- Procesamiento en memoria
	- R√°pido
- MapReduce
	- Batch
	- Alta fiabilidad

### Hive vs Presto
- Hive
	- Batch
	- Integraci√≥n Hadoop
- Presto
	- Interactivo
	- Baja latencia

### HDFS vs HBase
- HDFS
	- Almacenamiento masivo
- HBase
	- Acceso aleatorio

## Anti-patrones comunes
- Usar streaming para procesos batch
- Usar HBase como data lake
- Procesar peque√±os vol√∫menes con herramientas complejas
- Ignorar costes operativos

## Buenas pr√°cticas de selecci√≥n
- Empezar simple
- Validar con pruebas de concepto
- Medir rendimiento
- Escalar progresivamente
- Documentar decisiones t√©cnicas

# Designing and Building Big Data Applications ‚Äî Buenas pr√°cticas de rendimiento y escalabilidad

## Temario
- Principios generales de rendimiento
- Escalabilidad horizontal vs vertical
- Buenas pr√°cticas en almacenamiento
- Buenas pr√°cticas en procesamiento
- Gesti√≥n de particiones
- Uso eficiente de memoria
- Optimizaci√≥n de Spark
- Gesti√≥n de archivos peque√±os
- Monitorizaci√≥n y tuning
- Anti-patrones comunes

## Principios generales de rendimiento
- Dise√±ar pensando en el crecimiento
- Minimizar movimiento de datos
- Procesar cerca del almacenamiento
- Evitar cuellos de botella centralizados
- Medir antes de optimizar

## Escalabilidad horizontal vs vertical
### Escalabilidad horizontal
- A√±adir nodos al cl√∫ster
- Modelo preferido en Big Data
- Mayor tolerancia a fallos
- Coste predecible

### Escalabilidad vertical
- Aumentar recursos de un nodo
- Limitada f√≠sicamente
- Menor tolerancia a fallos

## Buenas pr√°cticas en almacenamiento
- Usar sistemas distribuidos
	- HDFS
	- Object Storage
- Elegir formatos eficientes
	- Parquet
	- ORC
- Activar compresi√≥n
- Evitar archivos muy peque√±os
- Definir tama√±os de bloque adecuados

## Buenas pr√°cticas en procesamiento
- Preferir motores en memoria
	- Spark
- Evitar reprocesamientos innecesarios
- Reutilizar resultados intermedios
- Separar l√≥gica batch y streaming
- Paralelizar tareas de forma equilibrada

## Gesti√≥n de particiones
### Importancia de las particiones
- Determinan paralelismo
- Impactan directamente en rendimiento
- Afectan uso de red y disco

### Buenas pr√°cticas
- Particionar por columnas de uso frecuente
- Evitar:
	- Particiones demasiado grandes
	- Particiones demasiado peque√±as
- Ajustar n√∫mero de particiones seg√∫n cl√∫ster

## Uso eficiente de memoria
- No cargar datasets completos innecesariamente
- Liberar recursos tras su uso
- Persistir solo cuando aporta beneficio
- Elegir nivel de persistencia adecuado

### Persistencia en Spark
{% raw %}
```python
from pyspark import StorageLevel

df.persist(StorageLevel.MEMORY_AND_DISK)
```
{% endraw %}`

## Optimizaci√≥n de Spark

### Buenas pr√°cticas generales

* Usar DataFrames en lugar de RDDs
* Evitar `collect()` en grandes vol√∫menes
* Usar `filter` y `select` lo antes posible
* Aprovechar Catalyst Optimizer

### Ajuste de recursos

* Memoria del driver
* Memoria de executors
* N√∫mero de cores por executor

### Ejemplo de env√≠o optimizado

{% raw %}
```bash
spark-submit \
	--executor-memory 4G \
	--executor-cores 2 \
	--num-executors 10 \
	app.py
```
{% endraw %}

## Gesti√≥n de archivos peque√±os

### Problema

* Sobrecargan NameNode
* Reducen eficiencia del procesamiento

### Soluciones

* Compactar archivos
* Usar formatos columnar
* Reparticionar antes de escribir

### Reparticionado en Spark

{% raw %}
```python
df.repartition(10).write.parquet("hdfs:///data/output")
```
{% endraw %}

## Monitorizaci√≥n y tuning

* Usar Spark UI
* Analizar:
	* DAG
	* Stages
	* Tasks
* Identificar skew de datos
* Ajustar configuraci√≥n progresivamente

## Anti-patrones comunes

* Sobreparticionar datos
* Usar un √∫nico reducer
* Ignorar el tama√±o de los archivos
* Persistir todo indiscriminadamente
* Optimizar sin m√©tricas

## Buenas pr√°cticas finales

* Dise√±ar para el fallo
* Escalar de forma incremental
* Automatizar despliegues
* Documentar configuraciones
* Revisar peri√≥dicamente el rendimiento


# Designing and Building Big Data Applications ‚Äî Casos reales de aplicaci√≥n empresarial

## Temario
- Introducci√≥n
- Casos en retail y e-commerce
- Casos en banca y finanzas
- Casos en telecomunicaciones
- Casos en salud
- Casos en IoT y Smart Cities
- Casos en medios y entretenimiento
- Beneficios obtenidos
- Lecciones aprendidas
- Buenas pr√°cticas

## Introducci√≥n
- Las aplicaciones Big Data buscan resolver problemas reales de negocio:
	- Mejora de decisiones
	- Optimizaci√≥n de procesos
	- Personalizaci√≥n de servicios
- Integran ingesta, almacenamiento, procesamiento y visualizaci√≥n
- Casos variados muestran impacto en distintos sectores

## Casos en retail y e-commerce
- An√°lisis de comportamiento de clientes
	- Segmentaci√≥n y recomendaciones personalizadas
	- Herramientas: Spark, Hive, Kafka
- Optimizaci√≥n de inventario y log√≠stica
	- Predicci√≥n de demanda
	- Alertas de stock en tiempo real
- Ejemplo: Amazon utiliza Big Data para recomendaciones de productos y optimizaci√≥n log√≠stica

## Casos en banca y finanzas
- Detecci√≥n de fraude en transacciones
	- Procesamiento en tiempo real
	- Spark Streaming + Kafka + HBase
- An√°lisis de riesgos y scoring crediticio
	- Modelos predictivos en batch
	- Hadoop + Spark MLlib
- Regulaci√≥n y cumplimiento normativo
	- Auditor√≠a y trazabilidad de transacciones

## Casos en telecomunicaciones
- Monitorizaci√≥n de redes
	- Detecci√≥n de anomal√≠as
	- Streaming de logs y eventos
- Optimizaci√≥n de planes y ofertas
	- Segmentaci√≥n de clientes
	- Personalizaci√≥n de servicios
- Ejemplo: an√°lisis de tr√°fico m√≥vil para mejorar cobertura y calidad de servicio

## Casos en salud
- An√°lisis de historiales m√©dicos
	- Spark + HDFS para procesamiento masivo
	- Identificaci√≥n de patrones de enfermedad
- Investigaci√≥n gen√©tica y bioinform√°tica
	- Procesamiento de secuencias gen√≥micas
	- Big Data para descubrimiento de f√°rmacos
- Monitorizaci√≥n de pacientes en tiempo real
	- IoT y streaming de dispositivos m√©dicos

## Casos en IoT y Smart Cities
- Monitorizaci√≥n de sensores urbanos
	- Tr√°fico, calidad del aire, energ√≠a
	- Procesamiento en tiempo real con Spark Streaming
- Mantenimiento predictivo de infraestructuras
	- An√°lisis de datos de sensores industriales
- Ejemplo: Smart meters para consumo el√©ctrico eficiente

## Casos en medios y entretenimiento
- An√°lisis de comportamiento de usuarios
	- Recomendaciones de contenido
	- Optimizaci√≥n de campa√±as publicitarias
- Procesamiento de logs y m√©tricas
	- Streaming y batch combinados
- Ejemplo: Netflix usa Spark y Hadoop para recomendaciones personalizadas y an√°lisis de streaming

## Beneficios obtenidos
- Reducci√≥n de costes operativos
- Mejora en la experiencia del cliente
- Aceleraci√≥n en la toma de decisiones
- Predicci√≥n y prevenci√≥n de problemas
- Mayor eficiencia en procesos internos

## Lecciones aprendidas
- Elegir la herramienta adecuada seg√∫n el caso de uso
- Combinar procesamiento batch y streaming seg√∫n necesidad
- Dise√±ar arquitecturas escalables y tolerantes a fallos
- Importancia de la calidad y gobernanza de datos
- Monitorizaci√≥n y optimizaci√≥n continua

## Buenas pr√°cticas
- Empezar con pruebas de concepto
- Iterar y escalar progresivamente
- Integrar monitoreo desde el inicio
- Documentar arquitectura y decisiones
- Mantener seguridad y cumplimiento regulatorio
