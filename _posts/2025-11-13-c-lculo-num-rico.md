---
date: 2025-11-13 10:54
title: C√°lculo num√©rico
keywords:
source:
status: üåü
Parent: "[[Area-IA]]"
public_note: "true"
category: mates
---
# üßÆ C√°lculo num√©rico
`$= dv.current().file.tags.join(" ")`
- C√°lculo num√©rico
	- Desbordamiento y desbordamiento
	- Mal estado
	- M√©todo de optimizaci√≥n basado en gradiente
	- Optimizaci√≥n de restricciones
	- Ejemplo: m√≠nimos cuadrados lineales

El **c√°lculo num√©rico** estudia los m√©todos y algoritmos que permiten obtener soluciones aproximadas a problemas matem√°ticos, especialmente cuando no existen soluciones anal√≠ticas exactas o su c√°lculo es impracticable.  
Su objetivo es minimizar los errores de aproximaci√≥n y controlar la estabilidad de los m√©todos.

### üîπ Fuentes de error en c√°lculo num√©rico
1. **Error de redondeo:** ocurre al representar n√∫meros reales con precisi√≥n finita en el ordenador.  
2. **Error de truncamiento:** proviene de aproximar un proceso infinito (como una serie o derivada) por una expresi√≥n finita.  
3. **Error de propagaci√≥n:** se acumula a lo largo de los pasos de un algoritmo debido a errores previos.

### üîπ Precisi√≥n y estabilidad
- Un **m√©todo preciso** produce resultados cercanos al valor real.  
- Un **m√©todo estable** no amplifica los errores durante el proceso num√©rico.  
- Un **algoritmo mal condicionado** o **mal estado** amplifica los errores iniciales en los datos de entrada.

---

## ‚ö†Ô∏è Desbordamiento y subdesbordamiento

En los c√°lculos computacionales, los n√∫meros se representan con un n√∫mero limitado de bits, lo que provoca limitaciones:

- **Desbordamiento (Overflow):** ocurre cuando el resultado de una operaci√≥n excede el m√°ximo representable.  
  Ejemplo:  
  $$ x = 10^{308} \Rightarrow x^2 = 10^{616} \text{ (no representable en IEEE 754 double)} $$

- **Subdesbordamiento (Underflow):** sucede cuando el resultado es tan peque√±o que se aproxima a cero dentro de la precisi√≥n de m√°quina.  
  Ejemplo:  
  $$ x = 10^{-308} \Rightarrow x^2 = 10^{-616} \approx 0 $$

El **manejo del desbordamiento** implica escalar los datos, usar logaritmos o representar magnitudes en formato normalizado.

---

## üß© Mal estado (condicionamiento de un problema)

El **estado** o **condicionamiento** de un problema mide c√≥mo las peque√±as variaciones en los datos de entrada afectan al resultado.

Sea una funci√≥n $f(x)$. Si un peque√±o cambio $\Delta x$ produce un gran cambio en $f(x)$, el problema est√° **mal condicionado**.

### üîπ N√∫mero de condici√≥n
Se define como:

$$
\kappa(f, x) = \left| \frac{x}{f(x)} \frac{df(x)}{dx} \right|
$$

- Si $\kappa$ es grande, el problema es **mal condicionado**.  
- Si $\kappa$ es peque√±o, el problema es **bien condicionado**.

En el caso de sistemas lineales $A\mathbf{x} = \mathbf{b}$:

$$
\kappa(A) = \|A\| \cdot \|A^{-1}\|
$$

Cuanto mayor sea $\kappa(A)$, m√°s sensible ser√° la soluci√≥n a errores en $\mathbf{b}$ o en $A$.

---

## üî∫ M√©todo de optimizaci√≥n basado en gradiente

Los **m√©todos de optimizaci√≥n por gradiente** buscan minimizar una funci√≥n $f(\mathbf{x})$ ajustando iterativamente $\mathbf{x}$ en la direcci√≥n opuesta a su gradiente.

### üîπ Principio b√°sico
$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha_k \nabla f(\mathbf{x}_k)
$$

donde:
- $\nabla f(\mathbf{x}_k)$ es el gradiente en el punto actual,  
- $\alpha_k$ es la **tasa de aprendizaje** o **paso de actualizaci√≥n**.

### üîπ Propiedades
- Si $\alpha_k$ es demasiado grande ‚Üí el algoritmo puede divergir.  
- Si $\alpha_k$ es demasiado peque√±o ‚Üí converge muy lentamente.  
- El gradiente indica la **direcci√≥n de mayor aumento**, por lo que se desciende en la direcci√≥n opuesta.

### üîπ Ejemplo (C√≥digo)
{% raw %}
```python
# M√©todo de descenso del gradiente
import numpy as np

def gradiente(f_grad, x0, alpha=0.01, tol=1e-6, max_iter=1000):
	for _ in range(max_iter):
		grad = f_grad(x0)
		if np.linalg.norm(grad) < tol:
			break
		x0 = x0 - alpha * grad
	return x0
```
{% endraw %}`

---

## üß± Optimizaci√≥n de restricciones

En muchos problemas se desea minimizar una funci√≥n ( f(\mathbf{x}) ) sujeta a **restricciones**.

### üîπ Tipos de restricciones

* **Igualdad:**
  $$ g_i(\mathbf{x}) = 0 $$
* **Desigualdad:**
  $$ h_j(\mathbf{x}) \le 0 $$

### üîπ M√©todos principales

1. **Multiplicadores de Lagrange:**
   Se introduce un conjunto de variables ( \lambda_i ) (multiplicadores) y se define:
   $$
   \mathcal{L}(\mathbf{x}, \lambda) = f(\mathbf{x}) + \sum_i \lambda_i g_i(\mathbf{x})
   $$
   El √≥ptimo cumple:
   $$
   \nabla_{\mathbf{x&#125;&#125; \mathcal{L} = 0, \quad g_i(\mathbf{x}) = 0
   $$

2. **M√©todos de penalizaci√≥n:**
   Transforman el problema restringido en uno no restringido a√±adiendo penalizaciones:
   $$
   F(\mathbf{x}) = f(\mathbf{x}) + \mu \sum_i g_i(\mathbf{x})^2
   $$
   donde ( \mu > 0 ) controla el peso de la penalizaci√≥n.

3. **M√©todo de barrera interior:**
   Para restricciones de desigualdad:
   $$
   F(\mathbf{x}) = f(\mathbf{x}) - \frac{1}{t} \sum_j \ln(-h_j(\mathbf{x}))
   $$
   con ( t ) que crece progresivamente para aproximarse al l√≠mite factible.

---

## üìâ Ejemplo: m√≠nimos cuadrados lineales

El problema de **m√≠nimos cuadrados** busca una soluci√≥n ( \mathbf{x} ) que minimice el error cuadr√°tico entre un modelo lineal ( A\mathbf{x} ) y los datos observados ( \mathbf{b} ):

$$
\min_{\mathbf{x&#125;&#125; |A\mathbf{x} - \mathbf{b}|^2
$$

### üîπ Soluci√≥n anal√≠tica

Si ( A ) tiene rango completo:

$$
A^T A \mathbf{x} = A^T \mathbf{b}
$$

y por tanto:

$$
\mathbf{x} = (A^T A)^{-1} A^T \mathbf{b}
$$

### üîπ Soluci√≥n mediante Descomposici√≥n SVD

Usando la descomposici√≥n en valores singulares ( A = U \Sigma V^T ):

$$
A^+ = V \Sigma^+ U^T
$$

y por tanto:

$$
\mathbf{x} = A^+ \mathbf{b}
$$

donde ( A^+ ) es la **pseudoinversa de Moore‚ÄìPenrose**, que cumple:

$$
AA^+A = A, \quad A^+AA^+ = A^+, \quad (AA^+)^T = AA^+, \quad (A^+A)^T = A^+A
$$

### üîπ Interpretaci√≥n geom√©trica

El vector ( A\mathbf{x} ) es la **proyecci√≥n ortogonal** de ( \mathbf{b} ) sobre el subespacio generado por las columnas de ( A ).
La soluci√≥n ( \mathbf{x} ) minimiza la distancia entre ( A\mathbf{x} ) y ( \mathbf{b} ).

# üßÆ Fundamentos del c√°lculo num√©rico

El **c√°lculo num√©rico** es una disciplina que estudia los m√©todos y algoritmos para obtener soluciones aproximadas a problemas matem√°ticos, especialmente cuando las soluciones exactas son imposibles o dif√≠ciles de calcular.  
Su prop√≥sito principal es **encontrar soluciones estables, precisas y eficientes** en presencia de limitaciones computacionales y errores inevitables.

---

## üîπ Naturaleza del c√°lculo num√©rico

1. **Aproximaci√≥n:**  
	El c√°lculo num√©rico no busca el valor exacto, sino una **aproximaci√≥n controlada** al valor real, cuyo error puede ser medido o acotado.

2. **Error y estabilidad:**  
	El an√°lisis del error permite determinar si un m√©todo es confiable.  
	Un algoritmo **estable** no amplifica los errores de redondeo o truncamiento durante el proceso.

3. **Algoritmos y eficiencia:**  
	La resoluci√≥n de problemas en computadora requiere m√©todos **r√°pidos y precisos**, dise√±ados para operar dentro de los l√≠mites de la aritm√©tica de m√°quina.

---

## ‚öôÔ∏è Conceptos fundamentales

### Representaci√≥n num√©rica y aritm√©tica de punto flotante

Los ordenadores representan n√∫meros reales mediante un **formato de punto flotante**, definido por una base $\beta$, una mantisa $m$ y un exponente $e$:

$$
x = \pm m \times \beta^e
$$

El n√∫mero de d√≠gitos en la mantisa limita la **precisi√≥n**, y el rango de exponentes define los **l√≠mites de representaci√≥n**.

---

### Tipos de error

1. **Error absoluto:**  
	$$ E_a = |x - \tilde{x}| $$
	donde $x$ es el valor exacto y $\tilde{x}$ el valor aproximado.

2. **Error relativo:**  
	$$ E_r = \frac{|x - \tilde{x}|}{|x|} $$

3. **Error de redondeo:**  
	Proviene de la imposibilidad de representar ciertos n√∫meros en la m√°quina.

4. **Error de truncamiento:**  
	Surge al reemplazar procesos infinitos (como derivadas, integrales o series) por aproximaciones finitas.

5. **Propagaci√≥n del error:**  
	En c√°lculos iterativos o secuenciales, los errores se acumulan y pueden amplificarse si el m√©todo no es estable.

---

### Condicionamiento de un problema

El **condicionamiento** mide la sensibilidad del resultado ante peque√±as perturbaciones en los datos de entrada.  
Sea una funci√≥n $f(x)$:

$$
\kappa(f, x) = \left| \frac{x}{f(x)} \frac{df(x)}{dx} \right|
$$

- Si $\kappa$ es peque√±o ‚Üí el problema est√° **bien condicionado**.  
- Si $\kappa$ es grande ‚Üí el problema est√° **mal condicionado**.

Un problema mal condicionado puede producir resultados poco confiables incluso con m√©todos num√©ricamente estables.

---

### Estabilidad num√©rica

Un **m√©todo estable** controla la propagaci√≥n de los errores durante el proceso.  
Incluso si el problema est√° bien condicionado, un m√©todo **inestable** puede producir resultados err√≥neos.

Ejemplo de inestabilidad:
- Restar dos n√∫meros casi iguales puede provocar **p√©rdida de significancia**.
- C√°lculos iterativos mal dise√±ados pueden amplificar los errores de redondeo.

---

### Desbordamiento y subdesbordamiento

En aritm√©tica de punto flotante:
- **Desbordamiento (overflow):** el n√∫mero excede el m√°ximo representable ‚Üí resultado infinito.  
- **Subdesbordamiento (underflow):** el n√∫mero es demasiado peque√±o ‚Üí se aproxima a cero.

Estrategias para evitarlo:
- Escalar los datos o usar logaritmos.  
- Normalizar magnitudes durante los c√°lculos.  
- Utilizar tipos de datos con mayor rango (por ejemplo, `float64`).

---

### Convergencia

En m√©todos iterativos, como los de optimizaci√≥n o resoluci√≥n de ecuaciones, la **convergencia** indica que la secuencia de aproximaciones $x_k$ se acerca al valor verdadero $x^*$:

$$
\lim_{k \to \infty} x_k = x^*
$$

Tipos de convergencia:
1. **Lineal:** $|x_{k+1} - x^*| \approx C |x_k - x^*|$
2. **Cuadr√°tica:** $|x_{k+1} - x^*| \approx C |x_k - x^*|^2$
3. **Superlineal:** velocidad intermedia entre lineal y cuadr√°tica.

---

### Evaluaci√≥n de m√©todos num√©ricos

Un buen m√©todo num√©rico debe cumplir:

1. **Consistencia:** el error de truncamiento tiende a cero al refinar el m√©todo.  
2. **Estabilidad:** los errores no se amplifican durante el c√°lculo.  
3. **Convergencia:** las aproximaciones tienden al valor exacto.  
4. **Eficiencia:** bajo costo computacional y buena precisi√≥n.  

---

## üß© Ejemplo cl√°sico: m√≠nimos cuadrados

Problema:
$$
\min_{\mathbf{x&#125;&#125; \|A\mathbf{x} - \mathbf{b}\|^2
$$

Soluci√≥n:
$$
A^T A \mathbf{x} = A^T \mathbf{b}
\Rightarrow
\mathbf{x} = (A^T A)^{-1} A^T \mathbf{b}
$$

Si $A$ no es cuadrada o no tiene rango completo, se usa la **pseudoinversa** $A^+$:

$$
A^+ = V \Sigma^+ U^T
\quad \Rightarrow \quad
\mathbf{x} = A^+ \mathbf{b}
$$

---

## üß† Conclusi√≥n conceptual

El c√°lculo num√©rico combina el rigor matem√°tico con la realidad computacional.  
Sus fundamentos ‚Äîrepresentaci√≥n, error, estabilidad, condicionamiento y convergencia‚Äî son esenciales para dise√±ar algoritmos robustos capaces de resolver problemas reales con precisi√≥n controlada.


# üß© Lenguaje matem√°tico del c√°lculo num√©rico

El **lenguaje matem√°tico del c√°lculo num√©rico** proporciona la notaci√≥n, s√≠mbolos y convenciones que permiten expresar los algoritmos, errores y m√©todos de manera rigurosa y comprensible.  
Este lenguaje une el pensamiento matem√°tico abstracto con la implementaci√≥n computacional precisa.

---

## üîπ N√∫meros y notaci√≥n

1. **N√∫meros reales y aproximados:**  
	Un n√∫mero real $x$ se representa de forma aproximada por $\tilde{x}$, donde:
	$$
	\tilde{x} = x + e, \quad e = \text{error de aproximaci√≥n}
	$$

2. **Error absoluto y relativo:**  
	$$
	E_a = |x - \tilde{x}|, \quad E_r = \frac{|x - \tilde{x}|}{|x|}
	$$

3. **Notaci√≥n cient√≠fica (punto flotante):**  
	$$
	x = \pm m \times \beta^e
	$$
	donde $\beta$ es la base (normalmente 2 o 10), $m$ la mantisa, y $e$ el exponente.

---

## üîπ Expresiones y operadores comunes

1. **Normas (magnitud de un vector o matriz):**
	- Vector:
	  $$
	  \|\mathbf{x}\|_2 = \sqrt{\sum_i x_i^2}
	  $$
	- Matriz:
	  $$
	  \|A\|_2 = \max_{\mathbf{x} \ne 0} \frac{\|A\mathbf{x}\|_2}{\|\mathbf{x}\|_2}
	  $$

2. **Producto interno:**
	$$
	\langle \mathbf{x}, \mathbf{y} \rangle = \sum_i x_i y_i
	$$

3. **Producto matricial:**
	$$
	C = AB, \quad c_{ij} = \sum_k a_{ik} b_{kj}
	$$

4. **Transpuesta y simetr√≠a:**
	$$
	A^T = (a_{ji}), \quad \text{si } A^T = A \text{ entonces } A \text{ es sim√©trica.}
	$$

5. **Derivada y gradiente:**
	- Derivada (una variable): $f'(x) = \frac{df}{dx}$
	- Gradiente (varias variables):  
	  $$
	  \nabla f(\mathbf{x}) =
	  \begin{bmatrix}
	  \frac{\partial f}{\partial x_1} \\[4pt]
	  \frac{\partial f}{\partial x_2} \\[4pt]
	  \vdots \\[4pt]
	  \frac{\partial f}{\partial x_n}
	  \end{bmatrix}
	  $$

6. **Jacobiano (matriz de derivadas parciales):**
	$$
	J_{ij} = \frac{\partial f_i}{\partial x_j}
	$$

7. **Norma del error (en vectores o funciones):**
	$$
	\| \mathbf{e} \| = \| \mathbf{x} - \tilde{\mathbf{x&#125;&#125; \|
	$$

---

## üîπ Operaciones num√©ricas y s√≠mbolos √∫tiles

| S√≠mbolo | Significado | Ejemplo |
|----------|--------------|----------|
| $\approx$ | Aproximadamente igual | $\pi \approx 3.1416$ |
| $\sim$ | Comportamiento asint√≥tico | $f(x) \sim x^2$ |
| $\simeq$ | Igualdad num√©rica aproximada | $0.333... \simeq \frac{1}{3}$ |
| $\Rightarrow$ | Implica o conduce a | $A\mathbf{x}=\mathbf{b} \Rightarrow \mathbf{x}=A^{-1}\mathbf{b}$ |
| $\Longleftrightarrow$ | Equivalencia | $a=b \Longleftrightarrow a-b=0$ |
| $\propto$ | Proporcionalidad | $f(x) \propto x^2$ |
| $\mathbb{R}^n$ | Espacio eucl√≠deo n-dimensional | $\mathbf{x} \in \mathbb{R}^n$ |

---

## üîπ Relaciones entre funciones y errores

1. **Desarrollo de Taylor (aproximaci√≥n local):**
	$$
	f(x+h) = f(x) + h f'(x) + \frac{h^2}{2} f''(x) + \mathcal{O}(h^3)
	$$
	Se utiliza para analizar errores de truncamiento y dise√±ar m√©todos num√©ricos.

2. **Orden de error:**
	Un m√©todo tiene **orden $p$** si:
	$$
	E(h) = C h^p + \mathcal{O}(h^{p+1})
	$$
	Cuanto mayor es $p$, m√°s r√°pido decrece el error al refinar el paso $h$.

3. **An√°lisis de estabilidad:**
	En un m√©todo iterativo:
	$$
	\mathbf{x}_{k+1} = G \mathbf{x}_k + \mathbf{c}
	$$
	El m√©todo es estable si el **radio espectral** $\rho(G) < 1$.

---

## üîπ Expresiones matriciales clave en c√°lculo num√©rico

1. **Sistema lineal:**
	$$
	A\mathbf{x} = \mathbf{b}
	$$

2. **Descomposiciones num√©ricas:**
	- LU: $A = LU$
	- QR: $A = QR$
	- SVD: $A = U \Sigma V^T$

3. **Pseudoinversa de Moore‚ÄìPenrose:**
	$$
	A^+ = V \Sigma^+ U^T
	$$

4. **N√∫mero de condici√≥n:**
	$$
	\kappa(A) = \|A\| \cdot \|A^{-1}\|
	$$
	Mide la sensibilidad del resultado respecto a los errores en los datos.

---

## üîπ Notaci√≥n algor√≠tmica y computacional

- **Iteraci√≥n:**  
	$$
	x_{k+1} = x_k - \alpha \nabla f(x_k)
	$$
	indica un proceso repetitivo, t√≠pico de m√©todos de optimizaci√≥n o b√∫squeda de ra√≠ces.

- **Criterio de parada:**  
	$$
	\|x_{k+1} - x_k\| < \varepsilon
	$$
	para una tolerancia $\varepsilon$ peque√±a.

- **Complejidad algor√≠tmica:**  
	$\mathcal{O}(n^3)$ indica el crecimiento del coste computacional con el tama√±o del problema.

---

## üß† Interpretaci√≥n conceptual

El lenguaje del c√°lculo num√©rico traduce la abstracci√≥n matem√°tica al contexto computacional.  
Los s√≠mbolos expresan relaciones exactas, mientras que las aproximaciones reflejan limitaciones pr√°cticas.  
Comprender este lenguaje permite:
- Analizar errores y estabilidad,  
- Formular algoritmos precisos,  
- Comunicar resultados con rigor y claridad matem√°tica.
