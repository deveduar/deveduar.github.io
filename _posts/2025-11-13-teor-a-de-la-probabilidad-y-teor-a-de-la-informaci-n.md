---
date: 2025-11-13 10:56
title: Teor√≠a de la probabilidad y teor√≠a de la informaci√≥n
tags:
keywords:
source:
status: üåü
Parent: "[[Area-IA]]"
cssclasses:
  - hide-embedded-header1
categories:
  - mates
public_note: "true"
category: mates
---
# üßÆ Teor√≠a de la probabilidad y teor√≠a de la informaci√≥n
`$= dv.current().file.tags.join(" ")`

- Teor√≠a de la probabilidad y teor√≠a de la informaci√≥n.
	- Variables aleatorias
	- Distribuci√≥n de probabilidad
	- Probabilidad marginal
	- Probabilidad condicional
	- Independencia e independencia condicional
	- Expectativa, varianza y covarianza
	- Distribuci√≥n de probabilidad com√∫n
	- Reglas bayesianas
	- Variable continua
	- Teor√≠a de la informaci√≥n
	- Modelo estad√≠stico estructurado

La **teor√≠a de la probabilidad** proporciona un marco matem√°tico para modelar la incertidumbre y cuantificar el azar.  
La **teor√≠a de la informaci√≥n**, por otro lado, estudia la cuantificaci√≥n, almacenamiento y transmisi√≥n de la informaci√≥n.  
Ambas teor√≠as se conectan profundamente en campos como el Aprendizaje autom√°tico, Estad√≠stica bayesiana o la Codificaci√≥n de datos.

---

## üé≤ Variables aleatorias

Una **variable aleatoria** (VA) es una funci√≥n que asigna un n√∫mero real a cada posible resultado de un experimento aleatorio.

- **Variable aleatoria discreta:** toma un n√∫mero finito o numerable de valores (ej. lanzamiento de un dado).
- **Variable aleatoria continua:** puede tomar cualquier valor dentro de un intervalo real (ej. temperatura, tiempo).

En notaci√≥n formal:

$$X : \Omega \rightarrow \mathbb{R}$$

donde $\Omega$ es el espacio muestral.

---

## üìä Distribuci√≥n de probabilidad

Describe c√≥mo se distribuyen los valores posibles de una variable aleatoria.

- Para una **variable discreta**, se define por la **funci√≥n de masa de probabilidad (pmf)**:
  
  $$P(X = x_i) = p_i, \quad \text{con} \quad \sum_i p_i = 1$$

- Para una **variable continua**, se define mediante la **funci√≥n de densidad de probabilidad (pdf)** $f(x)$:
  
  $$P(a \le X \le b) = \int_a^b f(x)\,dx, \quad \text{con} \quad \int_{-\infty}^{\infty} f(x)\,dx = 1$$

---

## üß© Probabilidad marginal

La **probabilidad marginal** es la probabilidad de un evento considerando todas las posibles ocurrencias de otras variables.

$$P(X) = \sum_Y P(X, Y)$$
(en el caso discreto)  
o  
$$P(X) = \int P(X, Y)\,dY$$
(en el caso continuo).

Se obtiene integrando o sumando sobre las dem√°s variables.

---

## üîó Probabilidad condicional

Es la probabilidad de que ocurra un evento $A$ dado que otro evento $B$ ha ocurrido:

$$P(A|B) = \frac{P(A \cap B)}{P(B)}, \quad P(B) > 0$$

Permite actualizar el conocimiento ante nueva informaci√≥n.

---

## ‚öñÔ∏è Independencia e independencia condicional

- **Independencia:**  
  Dos variables son independientes si conocer una no cambia la probabilidad de la otra:

  $$P(A \cap B) = P(A) P(B)$$

- **Independencia condicional:**  
  Dos variables $A$ y $B$ son independientes **dado** $C$ si:

  $$P(A, B | C) = P(A | C) P(B | C)$$

Este concepto es fundamental en los Modelos gr√°ficos probabil√≠sticos.

---

## üìà Expectativa, varianza y covarianza

- **Esperanza matem√°tica (media):**

  $$\mathbb{E}[X] = 
  \begin{cases} 
  \sum_x x P(X = x) & \text{(discreta)} \\[5pt]
  \int_{-\infty}^{\infty} x f(x)\,dx & \text{(continua)}
  \end{cases}$$

- **Varianza:**

  $$\operatorname{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]$$

  Mide la dispersi√≥n de la variable respecto a su media.

- **Covarianza:**

  $$\operatorname{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]$$

  Indica c√≥mo var√≠an conjuntamente dos variables.

---

## üìö Distribuciones de probabilidad comunes

Algunas distribuciones fundamentales:

- **Bernoulli:** $P(X=1)=p, \, P(X=0)=1-p$
- **Binomial:** $P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}$
- **Poisson:** $P(X=k)=\frac{\lambda^k e^{-\lambda}}{k!}$
- **Uniforme continua:** $f(x)=\frac{1}{b-a}$ para $a \le x \le b$
- **Normal (Gaussiana):**
  $$f(x)=\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$
- **Exponencial:** $f(x)=\lambda e^{-\lambda x}, \, x \ge 0$

---

## üßÆ Reglas bayesianas

Basadas en el **teorema de Bayes**:

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

Permite actualizar creencias ante nueva evidencia, base del Aprendizaje bayesiano.

- **Probabilidad a priori:** $P(A)$
- **Verosimilitud:** $P(B|A)$
- **Evidencia:** $P(B) = \sum_A P(B|A)P(A)$
- **Probabilidad a posteriori:** $P(A|B)$

---

## üåà Variable continua

Para una variable continua $X$, la probabilidad de tomar un valor exacto es cero:

$$P(X = x) = 0$$

Las probabilidades se calculan sobre intervalos.  
Las funciones de densidad deben cumplir:

$$f(x) \ge 0, \quad \int_{-\infty}^{\infty} f(x)\,dx = 1$$

La **funci√≥n de distribuci√≥n acumulada (CDF)**:

$$F(x) = P(X \le x) = \int_{-\infty}^x f(t)\,dt$$

---

## üí° Teor√≠a de la informaci√≥n

La **informaci√≥n** se mide como reducci√≥n de incertidumbre.  
El concepto central es la **entrop√≠a de Shannon**:

$$H(X) = - \sum_i P(x_i) \log_2 P(x_i)$$

Cuantifica la incertidumbre promedio de una variable aleatoria.  
Otras medidas derivadas:

- **Informaci√≥n mutua:**
  $$I(X;Y) = \sum_{x,y} P(x,y) \log \frac{P(x,y)}{P(x)P(y)}$$
  mide la dependencia entre dos variables.

- **Entrop√≠a condicional:**
  $$H(X|Y) = -\sum_{x,y} P(x,y) \log P(x|y)$$

- **Entrop√≠a cruzada y divergencia KL:**
  $$D_{KL}(P||Q) = \sum_x P(x)\log\frac{P(x)}{Q(x)}$$
  utilizada en Aprendizaje profundo para medir la diferencia entre distribuciones.

---

## üß† Modelo estad√≠stico estructurado

Un **modelo estad√≠stico estructurado** combina relaciones probabil√≠sticas entre variables, a menudo expresadas mediante grafos o ecuaciones latentes.

Ejemplos:

- Modelos gr√°ficos bayesianos
- Redes neuronales probabil√≠sticas
- Modelos de Markov ocultos (HMM)
- Modelos lineales generalizados (GLM)

Estos modelos permiten representar dependencias, inferir variables ocultas y realizar predicciones basadas en la estructura de la incertidumbre.

# üß† Problemas y soluciones en teor√≠a de la probabilidad e informaci√≥n

Esta gu√≠a recopila **problemas frecuentes**, **m√©todos de resoluci√≥n** y **estrategias pr√°cticas** para aplicar los conceptos de [Teor√≠a de la probabilidad y teor√≠a de la informaci√≥n](/mates/teor-a-de-la-probabilidad-y-teor-a-de-la-informaci-n/).

---

## üéØ Gu√≠a general de resoluci√≥n

1. **Identificar el tipo de variable:**
	- Si toma valores contables ‚Üí discreta.
	- Si toma valores en un intervalo ‚Üí continua.

2. **Determinar la distribuci√≥n:**
	- Bernoulli / Binomial ‚Üí experimentos con √©xito-fracaso.
	- Poisson ‚Üí n√∫mero de eventos en un intervalo.
	- Normal ‚Üí fen√≥menos continuos con simetr√≠a.
	- Exponencial ‚Üí tiempos entre eventos.

3. **Aplicar las f√≥rmulas clave:**
	- Esperanza, varianza, covarianza.
	- Probabilidades marginales o condicionales.
	- Teorema de Bayes para actualizaci√≥n de creencias.

4. **Visualizar la estructura:**
	- Representar dependencias mediante grafos.
	- Identificar independencia o independencia condicional.

5. **Verificar unidades y sentido:**
	- Toda probabilidad debe estar entre 0 y 1.
	- Las densidades pueden ser mayores que 1, pero integran a 1.

---

## üß© Problema 1: Probabilidad condicional

**Enunciado:**  
En un test m√©dico, el 1% de la poblaci√≥n tiene una enfermedad. La prueba detecta correctamente la enfermedad el 99% de las veces, pero tiene un 2% de falsos positivos.  
Si una persona da positivo, ¬øcu√°l es la probabilidad de que realmente est√© enferma?

**Soluci√≥n (Teorema de Bayes):**

$$
P(E|+) = \frac{P(+|E) P(E)}{P(+|E) P(E) + P(+|\neg E) P(\neg E)}
$$

Sustituyendo:

$$
P(E|+) = \frac{0.99 \cdot 0.01}{0.99 \cdot 0.01 + 0.02 \cdot 0.99} \approx 0.33
$$

**Interpretaci√≥n:**  
A pesar del test fiable, la baja prevalencia reduce dr√°sticamente la probabilidad real de estar enfermo.

**Conceptos usados:**  
Probabilidad condicional, Teorema de Bayes, Reglas bayesianas

---

## üìä Problema 2: Probabilidad marginal

**Enunciado:**  
Una f√°brica produce el 40% de sus piezas en la m√°quina A y el 60% en la m√°quina B. La probabilidad de defecto es 1% en A y 3% en B.  
¬øCu√°l es la probabilidad de que una pieza elegida al azar sea defectuosa?

**Soluci√≥n:**

$$
P(D) = P(D|A) P(A) + P(D|B) P(B) = 0.01 \cdot 0.4 + 0.03 \cdot 0.6 = 0.022
$$

**Interpretaci√≥n:**  
La probabilidad total se obtiene combinando las contribuciones de cada fuente.

**Conceptos usados:**  
Probabilidad marginal, Regla de la probabilidad total

---

## üßÆ Problema 3: Esperanza y varianza

**Enunciado:**  
Sea una variable aleatoria discreta $X$ con la siguiente distribuci√≥n:

| x | 1 | 2 | 3 |
|---|---|---|---|
| P(X=x) | 0.2 | 0.5 | 0.3 |

Calcular $\mathbb{E}[X]$ y $\operatorname{Var}(X)$.

**Soluci√≥n:**

$$
\mathbb{E}[X] = 1 \cdot 0.2 + 2 \cdot 0.5 + 3 \cdot 0.3 = 2.1
$$

$$
\operatorname{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2 = (1^2 \cdot 0.2 + 2^2 \cdot 0.5 + 3^2 \cdot 0.3) - (2.1)^2 = 0.49
$$

**Interpretaci√≥n:**  
La varianza mide la dispersi√≥n respecto a la media esperada.

**Conceptos usados:**  
Expectativa, varianza y covarianza

---

## üîó Problema 4: Independencia condicional

**Enunciado:**  
En un sistema de diagn√≥stico, la variable $D$ indica enfermedad, $S$ s√≠ntomas y $T$ el resultado del test.  
Se sabe que $T$ depende solo de $D$, no directamente de $S$.  
Demuestra que $T$ y $S$ son independientes dado $D$.

**Soluci√≥n:**

$$
P(T|S, D) = P(T|D)
$$

Esto cumple la definici√≥n de independencia condicional:

$$
T \perp S \mid D
$$

**Interpretaci√≥n:**  
El conocimiento del s√≠ntoma no cambia la probabilidad del test una vez conocida la enfermedad.

**Conceptos usados:**  
Independencia e independencia condicional, Modelos gr√°ficos probabil√≠sticos

---

## üßæ Problema 5: Entrop√≠a de Shannon

**Enunciado:**  
Una fuente emite s√≠mbolos con probabilidades:  
$P(A)=0.5, P(B)=0.25, P(C)=0.25$.  
Calcular la entrop√≠a $H(X)$.

**Soluci√≥n:**

$$
H(X) = -\sum_i P(x_i) \log_2 P(x_i) = -[0.5 \log_2 0.5 + 0.25 \log_2 0.25 + 0.25 \log_2 0.25] = 1.5 \text{ bits}
$$

**Interpretaci√≥n:**  
En promedio, cada s√≠mbolo transporta 1.5 bits de informaci√≥n.

**Conceptos usados:**  
Teor√≠a de la informaci√≥n, Entrop√≠a de Shannon

---

## üìà Problema 6: Informaci√≥n mutua

**Enunciado:**  
Dos variables binarias $X, Y$ tienen la siguiente distribuci√≥n conjunta:

| X | Y | P(X,Y) |
|---|---|---------|
| 0 | 0 | 0.25 |
| 0 | 1 | 0.25 |
| 1 | 0 | 0.25 |
| 1 | 1 | 0.25 |

Calcular la informaci√≥n mutua $I(X;Y)$.

**Soluci√≥n:**

$$
I(X;Y) = \sum_{x,y} P(x,y) \log_2 \frac{P(x,y)}{P(x)P(y)}
$$

Dado que $X$ y $Y$ son independientes, $P(x,y) = P(x)P(y) = 0.25$.  
Por tanto:

$$
I(X;Y) = 0
$$

**Interpretaci√≥n:**  
No hay relaci√≥n entre $X$ y $Y$: conocer una no reduce la incertidumbre de la otra.

**Conceptos usados:**  
Informaci√≥n mutua, Independencia

---

## üß† Estrategias de estudio y aplicaci√≥n

1. **Practica derivaciones:** usa ejemplos con diferentes distribuciones.
2. **Aplica la regla de Bayes en contextos reales:** diagn√≥stico, predicci√≥n, clasificaci√≥n.
3. **Analiza dependencias con grafos probabil√≠sticos:** ayuda a visualizar independencia.
4. **Usa simulaciones:** genera datos con Python o R para comprobar resultados emp√≠ricos.
5. **Conecta con la teor√≠a de la informaci√≥n:** mide incertidumbre y dependencia entre variables.

---

## üíª Ejemplo de c√≥digo (Python)

{% raw %}
```python
import numpy as np

# Simulaci√≥n de una variable binomial
n, p = 10, 0.5
x = np.random.binomial(n, p, 10000)

# C√°lculo emp√≠rico de esperanza y varianza
E = np.mean(x)
Var = np.var(x)

print(f"Esperanza: {E:.2f}, Varianza: {Var:.2f}")
```
{% endraw %}`

**Resultado esperado:**

$$  
\mathbb{E}[X] \approx np = 5, \quad \operatorname{Var}(X) \approx np(1-p) = 2.5  
$$

---

## üìò Enlaces relacionados

- [Teor√≠a de la probabilidad y teor√≠a de la informaci√≥n](/mates/teor-a-de-la-probabilidad-y-teor-a-de-la-informaci-n/)
    
- Reglas bayesianas
    
- Modelos gr√°ficos probabil√≠sticos
    
- Expectativa, varianza y covarianza
    
- Teor√≠a de la informaci√≥n





# üßÆ Lenguaje matem√°tico en probabilidad y teor√≠a de la informaci√≥n

El **lenguaje matem√°tico** es esencial para expresar conceptos de probabilidad, estad√≠sticas y teor√≠a de la informaci√≥n de manera precisa.  
A continuaci√≥n se presentan los s√≠mbolos, notaciones y estructuras m√°s importantes usados en este campo, con sintaxis compatible con **LaTeX para Obsidian**.

---

## üé≤ Variables aleatorias

- **Variable aleatoria discreta:**  
  Toma valores contables $x_1, x_2, \dots, x_n$.  
  Funci√≥n de masa de probabilidad (pmf):

$$
P(X = x_i) = p_i, \quad \sum_{i} p_i = 1
$$

- **Variable aleatoria continua:**  
  Toma valores en un intervalo real.  
  Funci√≥n de densidad de probabilidad (pdf):

$$
f_X(x) \ge 0, \quad \int_{-\infty}^{\infty} f_X(x) \, dx = 1
$$

Funci√≥n de distribuci√≥n acumulada (CDF):

$$
F_X(x) = P(X \le x) = \int_{-\infty}^{x} f_X(t)\, dt
$$

---

## üìä Probabilidades

- **Probabilidad de un evento:**  

$$
P(A) \in [0,1], \quad P(\Omega) = 1
$$

- **Probabilidad condicional:**  

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}, \quad P(B) > 0
$$

- **Probabilidad conjunta:**  

$$
P(A \cap B) = P(A|B) P(B) = P(B|A) P(A)
$$

- **Probabilidad marginal:**  

$$
P(X) = \sum_Y P(X,Y) \quad \text{(discreta)}, \quad P(X) = \int P(X,Y)\, dY \quad \text{(continua)}
$$

---

## ‚öñÔ∏è Independencia

- **Independencia de eventos:**

$$
A \perp B \quad \Leftrightarrow \quad P(A \cap B) = P(A) P(B)
$$

- **Independencia condicional:**

$$
A \perp B \mid C \quad \Leftrightarrow \quad P(A \cap B \mid C) = P(A \mid C) P(B \mid C)
$$

---

## üßÆ Estad√≠sticos fundamentales

- **Esperanza matem√°tica (media):**

$$
\mathbb{E}[X] = 
\begin{cases} 
\sum_i x_i P(X = x_i) & \text{discreta} \\[2mm]
\int_{-\infty}^{\infty} x f_X(x) \, dx & \text{continua} 
\end{cases}
$$

- **Varianza:**

$$
\operatorname{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
$$

- **Covarianza:**

$$
\operatorname{Cov}(X,Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]
$$

- **Correlaci√≥n:**

$$
\rho_{X,Y} = \frac{\operatorname{Cov}(X,Y)}{\sqrt{\operatorname{Var}(X)\operatorname{Var}(Y)}}
$$

---

## üìö Distribuciones comunes

- **Bernoulli:**  

$$
P(X=1)=p, \quad P(X=0)=1-p
$$

- **Binomial:**  

$$
P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}, \quad k=0,\dots,n
$$

- **Poisson:**  

$$
P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k=0,1,2,\dots
$$

- **Normal (Gaussiana):**  

$$
f_X(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\Big(-\frac{(x-\mu)^2}{2\sigma^2}\Big)
$$

---

## üí° Teor√≠a de la informaci√≥n

- **Entrop√≠a de Shannon:**  

$$
H(X) = -\sum_i P(x_i) \log_2 P(x_i)
$$

- **Entrop√≠a condicional:**  

$$
H(X|Y) = - \sum_{x,y} P(x,y) \log_2 P(x|y)
$$

- **Informaci√≥n mutua:**  

$$
I(X;Y) = \sum_{x,y} P(x,y) \log_2 \frac{P(x,y)}{P(x)P(y)}
$$

- **Divergencia KL:**  

$$
D_{KL}(P\|Q) = \sum_x P(x) \log_2 \frac{P(x)}{Q(x)}
$$

---

## üîó Notaci√≥n recomendada en LaTeX para Obsidian

- Para f√≥rmulas en l√≠nea: `$ ... $`  
- Para bloques de ecuaciones: `$$ ... $$`  
- Funciones: `\mathbb{E}`, `\operatorname{Var}`, `\operatorname{Cov}`, `\log`, `\sum`, `\int`  
- Relaciones: `\perp`, `\cap`, `\cup`, `\subset`, `\Rightarrow`, `\Leftrightarrow`  

---

## üìò Enlaces relacionados

- Variables aleatorias  
- Distribuci√≥n de probabilidad  
- Probabilidad condicional  
- Expectativa, varianza y covarianza  
- Teor√≠a de la informaci√≥n

