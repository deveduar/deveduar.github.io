---
date: 2024-02-18 16:14
title: algebra lineal
Hecho: false
tags:
  - Mates
keywords:
source:
status: üåü
Parent: "[[Area-IA]]"
cssclasses:
  - hide-embedded-header1
categories:
  - mates
public_note: "true"
category: mates
---
# √Ålgebra lineal
`$= dv.current().file.tags.join(" ")`

- [matematicas](/uncategorized/mates/)
- [Intro Algebra lineal y ML](/mates/intro-algebra-lineal-y-ml/)
- [C√°lculo num√©rico](/mates/c-lculo-num-rico/)
- [Teor√≠a de la probabilidad y teor√≠a de la informaci√≥n](/mates/teor-a-de-la-probabilidad-y-teor-a-de-la-informaci-n/)
- 
- Matrices especiales
- Descomposici√≥n propia
- Descomposici√≥n del valor singular
- Pseudoinverso de Moore-Penrose
- Operador de seguimiento
- Determinantes
- Ejemplo: an√°lisis de componentes principales
## Matrices especiales

Las **matrices especiales** son aquellas con propiedades estructurales o algebraicas particulares que facilitan su an√°lisis y c√°lculo. Algunas de las m√°s relevantes son:

- **Matriz diagonal**: solo tiene elementos distintos de cero en su diagonal principal. Simplifica operaciones como la multiplicaci√≥n o la potenciaci√≥n.  
	- Ejemplo: $D = \text{diag}(d_1, d_2, ..., d_n)$
- **Matriz identidad**: diagonal con todos los elementos diagonales iguales a 1. Act√∫a como elemento neutro en la multiplicaci√≥n: $AI = IA = A$.
- **Matriz sim√©trica**: $A = A^T$. Representa transformaciones donde los ejes principales permanecen ortogonales, muy usada en an√°lisis de componentes principales (PCA).
- **Matriz ortogonal**: $Q^TQ = QQ^T = I$. Sus columnas son vectores unitarios y ortogonales entre s√≠. Preserva la norma y los √°ngulos.
- **Matriz herm√≠tica**: versi√≥n compleja de la sim√©trica, donde $A = A^*$ (transpuesta conjugada).
- **Matriz unitaria**: generalizaci√≥n compleja de la ortogonal. Cumple $U^*U = I$.
- **Matriz triangular**: superior o inferior, seg√∫n la ubicaci√≥n de los ceros. Fundamental en m√©todos de factorizaci√≥n como LU o QR.
- **Matriz de permutaci√≥n**: representa un reordenamiento de filas o columnas.

Estas matrices suelen usarse en descomposiciones y transformaciones fundamentales del √°lgebra lineal y la computaci√≥n cient√≠fica.

---

## Descomposici√≥n propia (Autovalores y autovectores)

La **descomposici√≥n propia** o **descomposici√≥n espectral** se aplica a matrices cuadradas, especialmente sim√©tricas. Permite expresar una matriz $A$ como:

$$A = PDP^{-1}$$

donde:
- $D$ es una matriz diagonal con los **autovalores** $\lambda_i$,
- $P$ contiene los **autovectores** asociados en sus columnas.

Si $A$ es **sim√©trica**, entonces $P$ es ortogonal, y la descomposici√≥n se simplifica a:

$$A = Q \Lambda Q^T$$

Esta forma muestra que $A$ puede interpretarse como una combinaci√≥n ponderada de direcciones invariantes.

### Propiedades clave
- Los autovalores indican la ‚Äúintensidad‚Äù de la transformaci√≥n en cada direcci√≥n propia.
- Los autovectores indican las direcciones invariantes bajo la transformaci√≥n.
- En geometr√≠a y PCA, representan los ejes principales de variaci√≥n.

---

## Descomposici√≥n del valor singular (SVD)

La **descomposici√≥n en valores singulares (SVD)** generaliza la descomposici√≥n propia a matrices no cuadradas. Toda matriz $A_{m \times n}$ puede escribirse como:

$$A = U \Sigma V^T$$

donde:
- $U$ es ortogonal ($U^TU = I_m$),
- $V$ es ortogonal ($V^TV = I_n$),
- $\Sigma$ es diagonal (o rectangular) con los **valores singulares** $\sigma_i \ge 0$.

### Interpretaci√≥n
- Los valores singulares representan la magnitud de la transformaci√≥n lineal en direcciones espec√≠ficas.
- Las columnas de $U$ y $V$ son los vectores propios de $AA^T$ y $A^TA$, respectivamente.
- Es √∫til en reducci√≥n de dimensiones, PCA, filtrado de ruido y compresi√≥n de datos.

### C√≥digo (Ejemplo en Python)
{% raw %}
```python
import numpy as np

A = np.array([[3, 1], [1, 3]])
U, S, Vt = np.linalg.svd(A)

print("U =", U)
print("Valores singulares =", S)
print("Vt =", Vt)
```
{% endraw %}`

---

## Pseudoinverso de Moore-Penrose

El **pseudoinverso de Moore-Penrose** ( A^+ ) extiende el concepto de matriz inversa a matrices no cuadradas o singulares. Se define como la √∫nica matriz que cumple:

$$
AA^{+}A = A, \qquad A^{+}AA^{+} = A^{+}, \qquad (AA^{+})^{T} = AA^{+}, \qquad (A^{+}A)^{T} = A^{+}A
$$

C√°lculo mediante la descomposici√≥n en valores singulares (SVD). Si
$A = U \Sigma V^{T}$, entonces

$$
A^{+} = V \,\Sigma^{+}\, U^{T}
$$

donde $\Sigma^{+}$  se obtiene invirtiendo los valores singulares distintos de cero.

### Aplicaciones

* Resolver sistemas sobredeterminados o subdeterminados.
* Minimizar errores cuadr√°ticos (( |Ax - b|^2 )).
* Fundamental en regresi√≥n lineal y aprendizaje autom√°tico.

### C√≥digo

{% raw %}
```python
import numpy as np

A = np.array([[1, 2], [3, 4], [5, 6]])
A_pinv = np.linalg.pinv(A)
print(A_pinv)
```
{% endraw %}

---

## Operador de seguimiento (Trace)

El **operador de seguimiento** o **traza** de una matriz cuadrada ( A ) se define como la suma de sus elementos diagonales:

$$
\text{tr}(A) = \sum_i a_{ii}
$$

### Propiedades

* Linealidad: ( \text{tr}(A + B) = \text{tr}(A) + \text{tr}(B) ).
* Invarianza por ciclo: ( \text{tr}(AB) = \text{tr}(BA) ).
* Igual a la suma de los autovalores de ( A ).
* En geometr√≠a, puede interpretarse como el **rastro de una transformaci√≥n lineal** (cu√°nto ‚Äúescala‚Äù el espacio).

---

## Determinantes

El **determinante** de una matriz cuadrada ( A ) mide el **factor de escala** de la transformaci√≥n lineal que representa. Si ( \det(A) = 0 ), la matriz no es invertible.

### Propiedades

* ( \det(AB) = \det(A)\det(B) )
* ( \det(A^T) = \det(A) )
* Si ( A ) es triangular, su determinante es el producto de los elementos diagonales.
* ( \det(A^{-1}) = 1/\det(A) )

### Interpretaci√≥n geom√©trica

* Representa el cambio de volumen (o √°rea en 2D) bajo la transformaci√≥n.
* El signo del determinante indica si hay una **inversi√≥n de orientaci√≥n** (reflexi√≥n).

### C√≥digo

{% raw %}
```python
import numpy as np

A = np.array([[2, 3], [1, 4]])
detA = np.linalg.det(A)
print(detA)
```
{% endraw %}

---

## Ejemplo: An√°lisis de Componentes Principales (PCA)

El **PCA** es una aplicaci√≥n directa de la SVD y la descomposici√≥n propia. Su objetivo es **reducir la dimensionalidad** manteniendo la mayor varianza posible.

### Proceso

1. Estandarizar los datos.
2. Calcular la matriz de covarianza ( C = \frac{1}{n-1} X^TX ).
3. Obtener autovalores y autovectores de ( C ).
4. Ordenar los autovectores por los autovalores m√°s grandes.
5. Proyectar los datos sobre las nuevas componentes principales.

### C√≥digo

{% raw %}
```python
import numpy as np

X = np.array([[1, 2], [3, 4], [5, 6]])
X_centered = X - np.mean(X, axis=0)
cov = np.cov(X_centered, rowvar=False)
eigvals, eigvecs = np.linalg.eig(cov)
X_pca = X_centered.dot(eigvecs[:, :1])

print("Autovalores:", eigvals)
print("Autovectores:", eigvecs)
print("Datos proyectados:", X_pca)
```
{% endraw %}

### Interpretaci√≥n

* Los autovectores indican las **direcciones principales** del espacio de datos.
* Los autovalores cuantifican la **varianza explicada** por cada componente.
* PCA es base para t√©cnicas de reducci√≥n de dimensionalidad, machine learning y an√°lisis estad√≠stico.


# Fundamentos de √Ålgebra Lineal

El **√°lgebra lineal** es la rama de las matem√°ticas que estudia los **vectores**, las **matrices** y las **transformaciones lineales** entre espacios vectoriales. Constituye la base de gran parte de la matem√°tica aplicada, la f√≠sica, la estad√≠stica y la computaci√≥n cient√≠fica.

---

## Vectores

Un **vector** es un elemento de un espacio vectorial que puede representarse como una lista ordenada de n√∫meros, llamados **componentes**.

$$\vec{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}$$

### Interpretaci√≥n
- Geom√©tricamente, representa una **direcci√≥n** y una **magnitud**.
- Algebraicamente, se interpreta como un **punto** o **tupla de coordenadas**.

### Operaciones b√°sicas
- **Suma de vectores:**  
	$$\vec{u} + \vec{v} = (u_1 + v_1, u_2 + v_2, ..., u_n + v_n)$$
- **Multiplicaci√≥n por un escalar:**  
	$$c\vec{v} = (cv_1, cv_2, ..., cv_n)$$
- **Producto escalar (o punto):**  
	$$\vec{u} \cdot \vec{v} = \sum_i u_i v_i$$
- **Norma (longitud):**  
	$$\|\vec{v}\| = \sqrt{\vec{v} \cdot \vec{v}}$$

Estas operaciones permiten construir conceptos m√°s avanzados como la **proyecci√≥n**, la **ortogonalidad** y las **transformaciones lineales**.

---

## Espacios vectoriales

Un **espacio vectorial** es un conjunto de vectores junto con dos operaciones:
1. **Suma de vectores**
2. **Multiplicaci√≥n por escalares**

Estas operaciones deben satisfacer axiomas como asociatividad, conmutatividad, existencia de un vector nulo y de inversos aditivos.

Ejemplo:  
El conjunto $\mathbb{R}^n$ con las operaciones habituales de suma y multiplicaci√≥n por escalar es un espacio vectorial.

### Subespacios
Un **subespacio** es un subconjunto de un espacio vectorial que tambi√©n cumple los axiomas del espacio vectorial.  
Ejemplo: el conjunto de vectores de $\mathbb{R}^3$ que yacen en un plano que pasa por el origen.

---

## Combinaciones lineales

Una **combinaci√≥n lineal** de vectores $\vec{v}_1, \vec{v}_2, ..., \vec{v}_k$ es cualquier vector que se pueda expresar como:

$$\vec{w} = c_1 \vec{v}_1 + c_2 \vec{v}_2 + ... + c_k \vec{v}_k$$

donde $c_i$ son escalares.

El **conjunto de todas las combinaciones lineales** posibles se llama el **subespacio generado** o **espacio generado** por esos vectores, denotado como:

$$\text{span}\{\vec{v}_1, ..., \vec{v}_k\}$$

---

## Independencia lineal

Un conjunto de vectores es **linealmente independiente** si **ninguno de ellos puede escribirse como combinaci√≥n lineal de los dem√°s**.  

Formalmente, los vectores $\vec{v}_1, ..., \vec{v}_k$ son independientes si:

$$c_1 \vec{v}_1 + ... + c_k \vec{v}_k = 0 \implies c_1 = c_2 = ... = c_k = 0$$

Si esta condici√≥n no se cumple, el conjunto es **dependiente**.

---

## Base y dimensi√≥n

Una **base** de un espacio vectorial es un conjunto de vectores **linealmente independientes** que **generan todo el espacio**.

- Toda combinaci√≥n lineal de los vectores de la base puede representar **√∫nicamente** un vector del espacio.
- La **dimensi√≥n** del espacio es el **n√∫mero de vectores** de cualquier base.

Ejemplos:
- En $\mathbb{R}^2$, una base est√°ndar es $\{(1,0), (0,1)\}$.
- En $\mathbb{R}^3$, la base est√°ndar es $\{(1,0,0), (0,1,0), (0,0,1)\}$.

---

## Transformaciones lineales

Una **transformaci√≥n lineal** es una funci√≥n $T: V \to W$ entre espacios vectoriales que cumple:

$$T(\vec{u} + \vec{v}) = T(\vec{u}) + T(\vec{v}), \quad T(c\vec{v}) = cT(\vec{v})$$

Toda transformaci√≥n lineal puede representarse mediante una **matriz** que act√∫a sobre vectores por medio de la multiplicaci√≥n matricial:

$$T(\vec{v}) = A\vec{v}$$

### Propiedades
- Conservan el origen y las combinaciones lineales.
- Pueden **rotar**, **escalar**, **reflejar** o **proyectar** espacios.
- Sus efectos geom√©tricos pueden analizarse a trav√©s de los **autovalores y autovectores** de la matriz asociada.

### C√≥digo (Ejemplo de transformaci√≥n lineal)
{% raw %}
```python
import numpy as np

A = np.array([[2, 0], [0, 3]])  # Escala el eje x por 2 y el eje y por 3
v = np.array([1, 2])
T_v = A @ v

print("Vector transformado:", T_v)
```
{% endraw %}`

---

## Aplicaciones b√°sicas

* Representaci√≥n de sistemas de ecuaciones lineales.
* An√°lisis geom√©trico de rotaciones y proyecciones.
* C√°lculo de m√≠nimos cuadrados.
* Base te√≥rica para vectores, matrices y descomposici√≥n del valor singular.

---

## Conceptos relacionados

* Vectores y operaciones vectoriales
* Matrices y operaciones matriciales
* Espacios vectoriales y subespacios
* Transformaciones lineales y matrices asociadas
* Descomposici√≥n propia
* PCA y reducci√≥n de dimensionalidad




# Lenguaje matem√°tico del √Ålgebra Lineal

Comprender el **lenguaje simb√≥lico y formal** del √°lgebra lineal es esencial para poder interpretar correctamente sus definiciones, teoremas y demostraciones.  
Esta nota introduce la **notaci√≥n**, **s√≠mbolos**, y **convenciones** m√°s comunes que aparecen en libros, art√≠culos o clases de √°lgebra lineal.

---

## S√≠mbolos y notaciones fundamentales

### Conjuntos num√©ricos
- $\mathbb{N}$: n√∫meros naturales $(0, 1, 2, 3, ‚Ä¶)$
- $\mathbb{Z}$: n√∫meros enteros $(..., -2, -1, 0, 1, 2, ...)$
- $\mathbb{Q}$: n√∫meros racionales (fracciones)
- $\mathbb{R}$: n√∫meros reales (l√≠nea continua)
- $\mathbb{C}$: n√∫meros complejos $(a + bi)$

En √°lgebra lineal trabajamos principalmente sobre $\mathbb{R}$ o $\mathbb{C}$, llamados **campos base**.

---

## Notaci√≥n de vectores y matrices

- **Vector columna**:  
	$$\vec{v} = 
	\begin{bmatrix} 
	v_1 \\ v_2 \\ \vdots \\ v_n 
	\end{bmatrix}$$
  Se representa con flecha ($\vec{v}$), letra min√∫scula negrita (**v**) o letra en min√∫scula (seg√∫n el contexto).

- **Matriz**:  
	Una colecci√≥n rectangular de n√∫meros:
	$$A = 
	\begin{bmatrix}
	a_{11} & a_{12} & \cdots & a_{1n} \\
	a_{21} & a_{22} & \cdots & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn}
	\end{bmatrix}$$

- **Dimensiones**:
	- Si $A$ tiene $m$ filas y $n$ columnas, se dice que $A \in \mathbb{R}^{m \times n}$.
	- Un vector columna $\vec{v} \in \mathbb{R}^n$ puede verse como una matriz $n \times 1$.

- **√çndices**:
	- El elemento $a_{ij}$ est√° en la **i-√©sima fila** y **j-√©sima columna**.
	- El sub√≠ndice suele empezar en 1: $a_{11}, a_{12}, ...$

---

## Operaciones y s√≠mbolos comunes

| S√≠mbolo                                      | Nombre             | Significado                                       |
| -------------------------------------------- | ------------------ | ------------------------------------------------- |
| $+$                                          | Suma               | Adici√≥n de vectores o matrices                    |
| $-$                                          | Resta              | Diferencia de vectores o matrices                 |
| $\cdot$ o $\langle \vec{u}, \vec{v} \rangle$ | Producto escalar   | Multiplicaci√≥n componente a componente y suma     |
| $\times$                                     | Producto vectorial | Solo en $\mathbb{R}^3$                            |
| $AB$                                         | Producto matricial | Multiplicaci√≥n entre matrices compatibles         |
| $A^T$                                        | Transpuesta        | Intercambia filas y columnas                      |
| $A^{-1}$                                     | Inversa            | Matriz que satisface $AA^{-1} = I$                |
| $\det(A)$                                    | Determinante       | Escalar que mide el cambio de volumen             |
| $\text{tr}(A)$                               | Trazar / Trace     | Suma de los elementos diagonales                  |
| $\| \vec{v} \|$                              | Norma              | Longitud o magnitud del vector                    |
| $\lambda$                                    | Autovalor          | Escala un autovector en una transformaci√≥n lineal |
| $\vec{v}$                                    | Autovector         | Vector propio asociado a $\lambda$                |

---

## Notaci√≥n funcional

Las **transformaciones lineales** se suelen escribir como funciones:

$$T: V \to W$$
$$T(\vec{v}) = A\vec{v}$$

donde $T$ es la transformaci√≥n, $V$ y $W$ son espacios vectoriales, y $A$ es la matriz asociada.

### Ejemplo
$$T:
\begin{bmatrix}
x \\ y
\end{bmatrix}
\mapsto
\begin{bmatrix}
2x + y \\
x - y
\end{bmatrix}$$

Esto puede representarse como una multiplicaci√≥n matricial:

$$T(\vec{v}) = 
\begin{bmatrix}
2 & 1 \\
1 & -1
\end{bmatrix}
\begin{bmatrix}
x \\ y
\end{bmatrix}$$

---

## Expresiones y cuantificadores

El √°lgebra lineal usa notaci√≥n l√≥gica para definir propiedades y teoremas:

- $\forall$: ‚Äúpara todo‚Äù  
	$\forall \vec{v} \in V, \, T(c\vec{v}) = cT(\vec{v})$
- $\exists$: ‚Äúexiste‚Äù  
	$\exists \lambda \in \mathbb{R} \, / \, A\vec{v} = \lambda\vec{v}$
- $\implies$: ‚Äúimplica que‚Äù  
	$\text{A invertible} \implies \det(A) \ne 0$
- $\iff$: ‚Äúsi y solo si‚Äù  
	$A$ es invertible $\iff$ sus columnas son linealmente independientes.

---

## Notaci√≥n vectorial y geom√©trica

### √Ångulos y ortogonalidad
Dos vectores son **ortogonales** si su producto escalar es cero:
$$\vec{u} \cdot \vec{v} = 0$$
y forman un √°ngulo de $90^\circ$.

### Proyecciones
La **proyecci√≥n** de $\vec{u}$ sobre $\vec{v}$ se denota:
$$\text{proj}_{\vec{v}}(\vec{u}) = \frac{\vec{u} \cdot \vec{v}}{\|\vec{v}\|^2} \vec{v}$$

---

## Convenciones tipogr√°ficas

- Letras **may√∫sculas** ‚Üí matrices ($A, B, C, ...$)
- Letras **min√∫sculas con flecha o negrita** ‚Üí vectores ($\vec{v}, \vec{u}, \mathbf{x}$)
- Letras **griegas** ‚Üí escalares especiales o autovalores ($\lambda, \mu, \sigma$)
- $I$ ‚Üí matriz identidad
- $0$ ‚Üí vector o matriz nula, seg√∫n el contexto

---

## Estructura t√≠pica de una expresi√≥n

Ejemplo de expresi√≥n simb√≥lica con significado completo:

$$A\vec{x} = \vec{b}$$

Donde:
- $A \in \mathbb{R}^{m \times n}$: matriz de coeficientes  
- $\vec{x} \in \mathbb{R}^n$: vector de inc√≥gnitas  
- $\vec{b} \in \mathbb{R}^m$: vector de resultados  

Esta ecuaci√≥n representa un **sistema de ecuaciones lineales**.

---

## Recomendaciones para leer expresiones matem√°ticas

1. **Identificar el tipo de objeto**: escalar, vector o matriz.  
2. **Analizar las dimensiones**: comprobar que las operaciones sean v√°lidas.  
3. **Distinguir entre s√≠mbolos abstractos y concretos**:  
	- $A$ puede representar una matriz gen√©rica o una matriz espec√≠fica.  
4. **Buscar patrones**: los teoremas y propiedades suelen repetirse con diferentes nombres o s√≠mbolos.  
5. **Relacionar con su interpretaci√≥n geom√©trica**: visualizar ayuda a entender los significados algebraicos.

---

## Conceptos relacionados

- Fundamentos de √Ålgebra Lineal  
- Vectores y operaciones vectoriales  
- Matrices y operaciones matriciales  
- Transformaciones lineales y matrices asociadas  
- Espacios vectoriales  
- Notaci√≥n matem√°tica general

---

# Problemas y soluciones de √Ålgebra Lineal

Esta nota re√∫ne **problemas b√°sicos con soluciones paso a paso**, √∫tiles para practicar y reforzar la comprensi√≥n de los conceptos del Fundamentos de √Ålgebra Lineal y Lenguaje matem√°tico del √Ålgebra Lineal.

Los diagramas **Mermaid** se usan para representar visualmente flujos de operaciones o relaciones entre vectores, matrices y transformaciones.

---

## üß© Problema 1: Suma y multiplicaci√≥n escalar de vectores

Sean los vectores:
$$\vec{u} = (2, -1, 3), \quad \vec{v} = (1, 4, -2)$$

**Calcular:**
1. $\vec{u} + \vec{v}$
2. $2\vec{u} - 3\vec{v}$

### Soluci√≥n paso a paso

1. **Suma de vectores**
   $$\vec{u} + \vec{v} = (2+1, -1+4, 3+(-2)) = (3, 3, 1)$$

2. **Combinaci√≥n lineal**
   $$2\vec{u} - 3\vec{v} = 2(2, -1, 3) - 3(1, 4, -2)$$
   $$= (4, -2, 6) - (3, 12, -6) = (1, -14, 12)$$

### Representaci√≥n Mermaid
{% raw %}
```mermaid
graph LR
	A["2u = (4, -2, 6)"]
	B["3v = (3, 12, -6)"]
	C["2u - 3v = (1, -14, 12)"]
	A --> C
	B --> C
```
{% endraw %}`

---

## üßÆ Problema 2: Producto escalar y √°ngulo entre vectores

Dados:  
$$
\vec{a} = (2, 1), \quad \vec{b} = (3, 4)
$$

**Calcular:**

1. $\vec{a} \cdot \vec{b}$  
2. El √°ngulo entre ellos.

---

### üß© Soluci√≥n

1. **Producto escalar:**

$$
\vec{a} \cdot \vec{b} = 2\cdot3 + 1\cdot4 = 10
$$

2. **Normas de los vectores:**

$$
|\vec{a}| = \sqrt{2^2 + 1^2} = \sqrt{5}, \quad |\vec{b}| = \sqrt{3^2 + 4^2} = 5
$$

3. **√Ångulo entre los vectores:**

$$
\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{|\vec{a}||\vec{b}|} = \frac{10}{5\sqrt{5}} = \frac{2}{\sqrt{5}}
$$

$$
\Rightarrow \theta = \cos^{-1}\left(\frac{2}{\sqrt{5}}\right) \approx 26.6^\circ
$$

---

### üí° Interpretaci√≥n geom√©trica

El √°ngulo peque√±o indica que los vectores $\vec{a}$ y $\vec{b}$ apuntan en direcciones similares, lo que se traduce en una alta **proyecci√≥n mutua** (gran componente de uno sobre el otro).

Puedes visualizar la relaci√≥n entre ellos con el siguiente diagrama (opcional, si usas Mermaid):

{% raw %}
```mermaid
graph TD
	A["(0,0)"] --> B["(2,1)  ‚Üí  ùíÇ"]
	A --> C["(3,4)  ‚Üí  ùíÉ"]
```
{% endraw %}`


### Representaci√≥n Mermaid

{% raw %}
```mermaid
graph TD
	A["a = (2,1)"]
	B["b = (3,4)"]
	C["a¬∑b = 10"]
	D["Œ∏ ‚âà 26.6¬∞"]
	A --> C
	B --> C
	C --> D
```
{% endraw %}

---


## üß© Problema 3: Sistema de ecuaciones lineales

Resolver:  
$$
\begin{cases}
x + 2y = 5 \\
3x - y = 4
\end{cases}
$$

---

### üí° Soluci√≥n matricial

$$
A =
\begin{bmatrix}
1 & 2 \\
3 & -1
\end{bmatrix}, \quad
\vec{x} =
\begin{bmatrix}
x \\ y
\end{bmatrix}, \quad
\vec{b} =
\begin{bmatrix}
5 \\ 4
\end{bmatrix}
$$

Entonces:  
$$
A\vec{x} = \vec{b}
$$

**Determinante:**
$$
\det(A) = (1)(-1) - (3)(2) = -7 \neq 0
$$

Por tanto, $A$ es **invertible**:

$$
A^{-1} = \frac{1}{-7}
\begin{bmatrix}
-1 & -2 \\
-3 & 1
\end{bmatrix}
$$

**Soluci√≥n:**
$$
\vec{x} = A^{-1}\vec{b} =
\frac{1}{-7}
\begin{bmatrix}
-1 & -2 \\
-3 & 1
\end{bmatrix}
\begin{bmatrix}
5 \\ 4
\end{bmatrix}
=
\frac{1}{-7}
\begin{bmatrix}
-13 \\ -11
\end{bmatrix}
=
\begin{bmatrix}
13/7 \\ 11/7
\end{bmatrix}
$$

---

### üìä Representaci√≥n Mermaid

{% raw %}
```mermaid
graph TD
	A["Sistema lineal A¬∑x = b"]
	B["A invertible ‚Üí A‚Åª¬π existe"]
	C["x = A‚Åª¬π¬∑b"]
	A --> B --> C
```
{% endraw %}`

---

## üî¢ Problema 4: Transformaci√≥n lineal y su efecto geom√©trico

Sea la transformaci√≥n ( T: \mathbb{R}^2 \to \mathbb{R}^2 ) dada por:  
$$  
T(x, y) = (2x - y, , x + y)  
$$

**Determinar:**

1. La matriz asociada.
    
2. El efecto geom√©trico.
    

---

### üí° Soluci√≥n

1. Aplicamos (T) a los vectores base:
    

$$  
T(1,0) = (2,1), \quad T(0,1) = (-1,1)  
$$

Por tanto:

$$  
A =  
\begin{bmatrix}  
2 & -1 \  
1 & 1  
\end{bmatrix}  
$$

2. **Efecto geom√©trico:**
    
    - Escala el eje (x) por 2 y lo combina con (y).
        
    - Rota y estira el plano en direcciones oblicuas.
        

---

### üß≠ Representaci√≥n Mermaid

{% raw %}
```mermaid
graph LR
	X["(x, y)"]
	Y["T(x,y) = (2x - y, x + y)"]
	Z["Transformaci√≥n: escala + rotaci√≥n"]
	X --> Y --> Z
```
{% endraw %}

---

## üìò Problema 5: Autovalores y autovectores

Sea:  
$$  
A =  
\begin{bmatrix}  
4 & 1 \  
2 & 3  
\end{bmatrix}  
$$

**Encontrar los autovalores y autovectores.**

---

### üí° Soluci√≥n

1. **Ecuaci√≥n caracter√≠stica:**
    

$$  
\det(A - \lambda I) =  
\begin{vmatrix}  
4 - \lambda & 1 \  
2 & 3 - \lambda  
\end{vmatrix}  
= (4-\lambda)(3-\lambda) - 2 = \lambda^2 - 7\lambda + 10 = 0  
$$

$$  
\Rightarrow \lambda_1 = 5, \quad \lambda_2 = 2  
$$

2. **Autovectores:**
    

Para (\lambda_1 = 5):  
$$  
(A - 5I)\vec{v} = 0 \Rightarrow  
\begin{bmatrix}  
-1 & 1 \  
2 & -2  
\end{bmatrix}  
\Rightarrow \vec{v}_1 = (1, 1)  
$$

Para (\lambda_2 = 2):  
$$  
(A - 2I)\vec{v} = 0 \Rightarrow  
\begin{bmatrix}  
2 & 1 \  
2 & 1  
\end{bmatrix}  
\Rightarrow \vec{v}_2 = (1, -2)  
$$

---

### ‚úÖ Resultado final

$$  
\lambda_1 = 5, \quad \vec{v}_1 = (1,1)  
$$

$$  
\lambda_2 = 2, \quad \vec{v}_2 = (1,-2)  
$$
### Representaci√≥n Mermaid

{% raw %}
```mermaid
graph TD
	A["A"]
	B["Ecuaci√≥n caracter√≠stica ‚Üí det(A - ŒªI) = 0"]
	C["Œª‚ÇÅ = 5, Œª‚ÇÇ = 2"]
	D["Vectores propios v‚ÇÅ = (1,1), v‚ÇÇ = (1,-2)"]
	A --> B --> C --> D
```
{% endraw %}

---

## üß† Recomendaciones de pr√°ctica

* Cambia los n√∫meros y verifica que el proceso se mantiene.
* Intenta **representar gr√°ficamente** los vectores antes y despu√©s de transformarlos.
* Usa Fundamentos de √Ålgebra Lineal y Lenguaje matem√°tico del √Ålgebra Lineal como referencia te√≥rica para comprender cada paso.

---

## Conceptos relacionados

* Vectores y operaciones vectoriales
* Matrices y operaciones matriciales
* Transformaciones lineales y matrices asociadas
* Descomposici√≥n propia
* PCA y espacios vectoriales

---


